{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install deepchem\n!pip install pysmiles\n!pip install openpyxl\n# !pip install rdkit\n\n# !pip install MolGraphConvFeaturizer\n!pip install PubChemPy\n!pip install PyDrive\n!pip install xlrd==2.0.1\n!pip install Pandas==1.3.5\n# !pip install Pandas==1.1.5\n!pip install  gdown\nimport deepchem as dc\nfrom rdkit import Chem\nprint(\"hjjbjh\")\nfrom pysmiles import read_smiles\nimport networkx as nx\nfrom deepchem.feat import MolGraphConvFeaturizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n#   gauth = GoogleAuth()\n#   gauth.credentials = GoogleCredentials.get_application_default()\n#   drivea = GoogleDrive(gauth)\n#   drive.mount('/content/drive')\ndef normalize1(X, means1=None, std1=None, means2=None, std2=None, feat_filt=None, norm='tanh_norm'):\n    if std1 is None:\n        std1 = np.nanstd(X, axis=0)\n    if feat_filt is None:\n        feat_filt = std1!=0\n    X = X[:,feat_filt]\n    X = np.ascontiguousarray(X)\n    if means1 is None:\n        means1 = np.mean(X, axis=0)\n    X = (X-means1)/std1[feat_filt]\n    if norm == 'norm':\n        return(X, means1, std1, feat_filt)\n    elif norm == 'tanh':\n        return(np.tanh(X), means1, std1, feat_filt)\n    elif norm == 'tanh_norm':\n        X = np.tanh(X)\n        if means2 is None:\n            means2 = np.mean(X, axis=0)\n        if std2 is None:\n            std2 = np.std(X, axis=0)\n        X = (X-means2)/std2\n        X[:,std2==0]=0\n        return(X, means1, std1, means2, std2, feat_filt) \n    \n    \ndef get_data():\n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  smiles=pd.read_excel('pubchem.xls', header=None)\n  smiles=np.array(smiles)\n\n#   !gdown https://drive.google.com/uc?id=15bNKK7tacCJIFzvt5y4WfU6uKbPdYtA6\n  !gdown --id 1z1sWG4E9BqipP0iczLrbR4qxSa4KG95_\n  data_to_repeat=pd.read_excel('pcbi.1006752.s004.xls', header=None)\n  data_to_repeat=np.array(data_to_repeat)\n  \n#   !gdown --id 1TThHsLyORlcHuEBgOad20SQUYH88O9mm\n#   data_to_repeat=pd.read_excel('labels1.xlsx', header=None)\n#   data_to_repeat=np.array(data_to_repeat)\n    \n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  unique_drugs=pd.read_excel('pubchem.xls', header=None)\n  unique_drugs=np.array(unique_drugs)         \n\n#   !gdown https://drive.google.com/uc?id=1IMr5zMLRAXC5iE2MAHJbKmHf2-0DCZNd\n# #   !gdown --id 1bBJUFBA4Tm9YdE5OxA1mbUfBI8B7wqcr\n#   feature_cell=pd.read_excel('final_feature_cell.xlsx', header=None)\n#   feature_cell=np.array(feature_cell)\n\n  !gdown --id 1NoUKCRZ0CkNVFP1-578x_L0otPUMkz6T\n  feature_cell=pd.read_excel('unique934_cell.xlsx',header=None)\n  feature_cell=np.array(feature_cell)\n  a=np.zeros((1,934))\n  feature_cell[22,1:]=a\n  feature_cell[36,1:]=a\n#   !gdown --id 109nyFVOO_P9DdyrhWzbNg0FBB_Y8gD58\n#   feature_cell=pd.read_excel('final_deep_cell.xlsx',header=None)\n#   feature_cell=np.array(feature_cell)\n\n#   !gdown https://drive.google.com/uc?id=1YTe0v5PzwjlgPqo3OTU4FMghOFAHgeeC\n  !gdown --id 1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\n  deleted_index=pd.read_excel('deleted_index.xls', header=None)\n  deleted_index=np.array(deleted_index)  \n\n  return smiles[1:,1],data_to_repeat[1:,:],unique_drugs[1:,0],feature_cell,deleted_index\n\n\ndef atom_features(atom):\n    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n                                          ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As',\n                                           'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se',\n                                           'Ti', 'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n                                           'Pt', 'Hg', 'Pb', 'Unknown']) +\n                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    [atom.GetIsAromatic()])\n\n\ndef one_of_k_encoding(x, allowable_set):\n    if x not in allowable_set:\n        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))\n\n\ndef one_of_k_encoding_unk(x, allowable_set):\n    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\n\n\ndef smile_to_graph(smile):\n    mol = Chem.MolFromSmiles(smile)\n\n    c_size = mol.GetNumAtoms()\n\n    features = []\n    for atom in mol.GetAtoms():\n        feature = atom_features(atom)\n        features.append(feature / sum(feature))\n\n    edges = []\n    for bond in mol.GetBonds():\n        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n    g = nx.Graph(edges).to_directed()\n    edge_index = []\n    for e1, e2 in g.edges:\n        edge_index.append([e1, e2])\n\n    return c_size, features, edge_index\n\n\ndef graph_node(smiles):\n  node=[]\n  adj=[]\n  max=0\n  final_feature=[]\n  final_adjacency=[]\n  for i  in range(len(smiles)):\n     m=smiles[i]\n    #  aa=m#.item((0))\n     featurizer = MolGraphConvFeaturizer(use_edges=True,use_chirality=True)\n     out = featurizer.featurize(m)\n     feature=out[0].node_features\n        \n#      featurizer = DMPNNFeaturizer()\n#      out = featurizer.featurize(m)\n#      feature=out[0].node_features\n    \n     ma=feature.shape[0]\n     n=feature.shape[1]\n     if(ma>max):\n       max=ma\n    # out[0].edge_featuwres.shape\n     molecules=read_smiles(m)\n     adjacency1=nx.to_numpy_array(molecules)\n     node.append(feature)\n     adj.append(adjacency1)\n#   f_n=np.zeros((max,n))\n#   f_a=np.zeros((max,max))\n  feature=[]\n  adjacency=[]\n  for i  in range(len(smiles)):\n    n1=node[i]\n    a=adj[i]\n    f_n=np.zeros((max,n))\n    f_a=np.zeros((max,max))\n    f_n[0:n1.shape[0],]=n1\n    f_a[0:n1.shape[0],0:n1.shape[0]]=a\n    final_feature.append(f_n)\n    final_adjacency.append(f_a)\n\n\n  return final_feature,final_adjacency \n\n\ndef graph_node_edge(smiles):\n  node=[]\n  adj=[]\n  max=0\n  max1=0\n  final_feature=[]\n  final_adjacency=[]\n  for i  in range(len(smiles)):\n     m=smiles[i]\n    #  aa=m#.item((0))\n    #  featurizer = MolGraphConvFeaturizer(use_edges=True,use_chirality=True)\n    #  out = featurizer.featurize(m)\n    #  feature=out[0].node_features\n     num_atom,feature,edge_index=smile_to_graph(m)  \n#      featurizer = DMPNNFeaturizer()\n#      out = featurizer.featurize(m)\n#      feature=out[0].node_features\n     feature=np.stack( feature, axis=0)\n     edge_index=np.stack( edge_index, axis=0)\n    \n     ma=num_atom\n     n=feature.shape[1]\n     if(ma>max):\n       max=ma\n    # out[0].edge_featuwres.shape\n     node.append(feature)\n\n     \n     ne=edge_index.shape[0]\n     if(ne>max1):\n       max1=ne\n    # out[0].edge_featuwres.shape\n     adj.append(edge_index)\n#   f_n=np.zeros((max,n))\n#   f_a=np.zeros((max,max))\n  feature=[]\n  adjacency=[]\n  for i  in range(len(smiles)):\n    n1=node[i]\n    a=adj[i]\n    f_n=np.zeros((max,n))\n    f_n[0:n1.shape[0],]=n1\n    f_a=np.zeros((max1,2))\n    f_a[0:a.shape[0],]=a\n    final_feature.append(f_n)\n    final_adjacency.append(f_a)\n\n  return final_feature,final_adjacency \n\n\ndef repeat_smiles(data_to_repeat,unique_drugs,feature,adjacency,deleted_index,synergy,senstivity):\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  synergy1=[]\n  senstivity1=[]\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    if(i not in deleted_index):\n      f_drug1.append(feature[k1[0]])\n      f_drug2.append(feature[k2[0]])\n      a_drug1.append(adjacency[k1[0]])\n      a_drug2.append(adjacency[k2[0]])\n      synergy1.append(synergy[i,])\n      senstivity1.append(senstivity[i,])\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,synergy1,senstivity1\n\ndef repeat_smiles1(data_to_repeat,unique_drugs,feature,adjacency,unique_cell):\n  unique_feature=unique_cell[:,1:]\n  unique_name=unique_cell[:,0]\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  c=data_to_repeat[:,2]\n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  feature_cell=[]\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    cc=c[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    cc1=[m for m, v in enumerate(unique_name) if cc in v]\n    f_drug1.append(feature[k1[0]])\n    f_drug2.append(feature[k2[0]])\n    a_drug1.append(adjacency[k1[0]])\n    a_drug2.append(adjacency[k2[0]])\n    feature_cell.append(unique_feature[cc1[0]])\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,feature_cell \n\ndef repeat_smiles_deep(data_to_repeat,unique_drugs,feature,adjacency,unique_cell):\n  unique_feature=unique_cell[:,1:]\n  unique_name=unique_cell[:,0]\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  c=data_to_repeat[:,2]\n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  feature_cell=[]\n  f=np.zeros((feature[0].shape[0],feature[0].shape[1]))\n  a=np.zeros((adjacency[0].shape[0],adjacency[0].shape[1]))\n  cell=np.zeros((unique_cell[0].shape[0]))\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    cc=c[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    cc1=[m for m, v in enumerate(unique_name) if cc in v]\n    if(cc1):\n        f_drug1.append(feature[k1[0]])\n        f_drug2.append(feature[k2[0]])\n        a_drug1.append(adjacency[k1[0]])\n        a_drug2.append(adjacency[k2[0]])\n        feature_cell.append(unique_feature[cc1[0]])\n    else:\n        f_drug1.append(f)\n        f_drug2.append(f)\n        a_drug1.append(a)\n        a_drug2.append(a)\n        feature_cell.append(cell)\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,feature_cell\n\ndef split_data(feature_cell,synergy,senstivity):\n  row=feature_cell.shape[0]\n  col=feature_cell.shape[1]\n  input1=np.zeros((row,col+1))\n  # a=np.array('i')\n  for i in range(len(feature_cell)):\n    input1[i,0:col]=feature_cell[i,:]\n    input1[i,col]=i\n\n\n  index_train=[]\n  index_test=[]\n  output = np.c_[synergy,senstivity ]\n#   dataset1=np.c_[input11,output1]\n#   dataset=np.random.shuffle(dataset1)\n#   input1=dataset[:,0:len(dataset[0])-2]\n#   output=dataset[:,-2:]\n  x_train, x_test, y_train, y_test = train_test_split(input1, output,test_size=.20)\n  train_synergy=y_train[:,0]\n  train_senstivity=y_train[:,1]\n  test_synergy=y_test[:,0]\n  test_senstivity=y_test[:,1]\n  for i in range(len(x_train)):#.shape[0]):\n    for j in range(len(input1)):#.shape[0]):\n      if(all(x_train[i]==input1[j])):\n        index_train.append(j)\n        \n\n  for ii in range(len(x_test)):#.shape[0]):\n    for jj in range(len(input1)):#.shape[0]):\n      if(all(x_test[ii]==input1[jj])):\n        index_test.append(jj)\n        \n\n  return  train_synergy, train_senstivity,test_synergy, test_senstivity,index_train,index_test\n\n\ndef split_data1(feature_cell,synergy,senstivity):\n  row=feature_cell.shape[0]\n  col=feature_cell.shape[1]\n  input1=np.zeros((row,col+1))\n  # a=np.array('i')\n  k=0\n  for i in range(len(feature_cell)):\n    if((senstivity[i])):\n        input1[k,0:col]=feature_cell[i,:]\n        input1[k,col]=i\n        k=k+1\n\n\n  index_train=[]\n  index_test=[]\n  output=[]\n  for i in range(len(feature_cell)):\n    if((senstivity[i])):\n      output.append (np.c_[synergy[i],senstivity[i] ])\n#   dataset1=np.c_[input11,output1]\n#   dataset=np.random.shuffle(dataset1)\n#   input1=dataset[:,0:len(dataset[0])-2]\n#   output=dataset[:,-2:]\n  output= np.array(output)\n  x_train, x_test, y_train, y_test = train_test_split(input1, output,test_size=.20)\n  train_synergy=y_train[:,0]\n  train_senstivity=y_train[:,1]\n  test_synergy=y_test[:,0]\n  test_senstivity=y_test[:,1]\n  for i in range(len(x_train)):#.shape[0]):\n    for j in range(len(input1)):#.shape[0]):\n      if(all(x_train[i]==input1[j])):\n        index_train.append(j)\n        \n\n  for ii in range(len(x_test)):#.shape[0]):\n    for jj in range(len(input1)):#.shape[0]):\n      if(all(x_test[ii]==input1[jj])):\n        index_test.append(jj)\n        \n\n  return  index_train,index_test\n\n\ndef index(m):\n    if m==1:\n      !gdown --id 1b82ry7sSPqPIcJSr-i4rmC9XYp7Z1CpH\n      test_ind=pd.read_excel('ind1_test.xls', header=None)\n      test_ind=np.array(test_ind)\n    \n      !gdown --id 1aAaYK1EG5AVlWtwHlsajVJo4Qx-u2ig3\n      train_ind=pd.read_excel('ind1_train.xls', header=None)\n      train_ind=np.array(train_ind)\n\n    \n    if m==2:\n        !gdown --id 1EXKurPZ5ScjiZM0OG-bSv7IPXyKKW6B5\n        test_ind=pd.read_excel('ind2_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1-rayr-tZdwX4DDtSHChkoSdgEK1P3jWN\n        train_ind=pd.read_excel('ind2_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    if m==3:\n        !gdown --id 19O4yKBUCAyPrvdUne2IhwOY73A4y7pzT\n        test_ind=pd.read_excel('ind3_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1YNZBGdL--Ww9ZSGWUKmeC26DdJDywrR4\n        train_ind=pd.read_excel('ind3_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==4:\n        !gdown --id 1sJ3ksFbMOQoBTqNw5Ddv7bGgD9YESRGn\n        test_ind=pd.read_excel('ind4_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1-P14BxLrpnYS-9TbsvyATKtFTFG7QsLO\n        train_ind=pd.read_excel('ind4_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==5:\n        !gdown --id 1iZo5wJgiUOGBAVRy0Hw6vXngFHe9ciFH\n        test_ind=pd.read_excel('ind5_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1xlFp8g1gDGPa_Sf0zgrPWm3hnRCSkAxp\n        train_ind=pd.read_excel('ind5_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    return test_ind,train_ind\n\n\ndef index1(m):\n    if m==1:\n      !gdown --id 10a4cV1tOqomBo7RzWqnbs6j5-RGcBNjS\n      test_ind=pd.read_excel('another_ind1_test.xls', header=None)\n      test_ind=np.array(test_ind)\n    \n      !gdown --id 1Kn8Y2iGCH7DLwq72s_QmouA0Db1WBfVR\n      train_ind=pd.read_excel('another_ind1_train.xls', header=None)\n      train_ind=np.array(train_ind)\n\n    \n    if m==2:\n        !gdown --id 1WR4kDyNO6Xo-P-z3O2q6M_tHr6Soj6NZ\n        test_ind=pd.read_excel('another_ind2_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1aegtCzmVg9znXPTUvpzf2zLkmcjEeQ22\n        train_ind=pd.read_excel('another_ind2_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    if m==3:\n        !gdown --id 1W29vAF6mOoKmacbibxzTNQCfp42VVYYN\n        test_ind=pd.read_excel('another_ind3_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1B8h0ewSo4XJCvSWBdS5oiQiwtAL3H72T\n        train_ind=pd.read_excel('another_ind3_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==4:\n        !gdown --id 1tUXsGGT_-JdJTpT5sYEBB3zNFnh60tJR\n        test_ind=pd.read_excel('another_ind4_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1T9H0O6bzr2d01cPnSyE-vYk_9doKKBHZ\n        train_ind=pd.read_excel('another_ind4_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==5:\n        !gdown --id 1p1kOBJQRhsavnOwgeb_3z-gBqLSS08sE\n        test_ind=pd.read_excel('another_ind5_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1hhw-ITuxUnKwTlLveS1MJER_KM580SXa\n        train_ind=pd.read_excel('another_ind5_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    return test_ind,train_ind\n\n\ndef train_test_input(f_drug1,a_drug1,f_drug2,a_drug2,cell_line,index_train,index_test,synery,senstivity):\n  train_f_drug1=[]\n  train_a_drug1=[]\n  train_f_drug2=[]\n  train_a_drug2=[]\n  train_cell_line=[]\n  train_synergy=[]\n  train_senstivity=[]\n  test_f_drug1=[]\n  test_a_drug1=[]\n  test_f_drug2=[]\n  test_a_drug2=[]\n  test_cell_line=[]\n  test_synergy=[]\n  test_senstivity=[]\n  for i in range(len(index_train)):\n      train_a_drug1.append(a_drug1[index_train[i]])\n      train_a_drug2.append(a_drug2[index_train[i]])\n      train_f_drug1.append(f_drug1[index_train[i]])\n      train_f_drug2.append(f_drug2[index_train[i]])\n      train_cell_line.append(cell_line[index_train[i]])\n      train_synergy.append(synergy[index_train[i]])\n      train_senstivity.append(senstivity[index_train[i]])\n  for ii in range(len(index_test)):\n      test_a_drug1.append(a_drug1[index_test[ii]])\n      test_a_drug2.append(a_drug2[index_test[ii]])\n      test_f_drug1.append(f_drug1[index_test[ii]])\n      test_f_drug2.append(f_drug2[index_test[ii]])\n      test_cell_line.append(cell_line[index_test[ii]])\n      test_synergy.append(synergy[index_test[ii]])\n      test_senstivity.append(senstivity[index_test[ii]])\n\n  return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity\n\ndef train_test_input1(f_drug1,a_drug1,f_drug2,a_drug2,cell_line,index_test,synergy,senstivity):\n  train_f_drug1=[]\n  train_a_drug1=[]\n  train_f_drug2=[]\n  train_a_drug2=[]\n  train_cell_line=[]\n  train_synergy=[]\n  train_senstivity=[]\n  test_f_drug1=[]\n  test_a_drug1=[]\n  test_f_drug2=[]\n  test_a_drug2=[]\n  test_cell_line=[]\n  test_synergy=[]\n  test_senstivity=[]\n    \n  for i in range(len(f_drug1)):\n     ind=i+1\n     if(ind not in index_test):\n          train_a_drug1.append(a_drug1[i])\n          train_a_drug2.append(a_drug2[i])\n          train_f_drug1.append(f_drug1[i])\n          train_f_drug2.append(f_drug2[i])\n          train_cell_line.append(cell_line[i])\n          train_synergy.append(synergy[i])\n          train_senstivity.append(senstivity[i])\n  for ii in range(len(index_test)):\n#       ind1=ii-1\n      x=index_test[ii]-1\n      n=x[0]\n      test_a_drug1.append(a_drug1[n])\n      test_a_drug2.append(a_drug2[n])\n      test_f_drug1.append(f_drug1[n])\n      test_f_drug2.append(f_drug2[n])\n      test_cell_line.append(cell_line[n])\n      test_synergy.append(synergy[n])\n      test_senstivity.append(senstivity[n])\n    \n  return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity\n\n\ndef preprocess(index_train,index_test):\n    index_train1=[]\n    index_test1=[]\n    for i in range(len(index_train)):\n        index_train1.append((index_train[i])[0])\n        \n    for ii in range(len(index_test)):\n        index_test1.append((index_test[ii])[0])\n        \n    return index_train1,index_test1\n\n\n\ndef train_test_input2(f_drug1,a_drug1,f_drug2,a_drug2,cell_line,index_test,synergy,senstivity):\n  \n \n  train_a_drug1=a_drug1[3581:]\n  train_a_drug2=a_drug2[3581:]\n  train_f_drug1=f_drug1[3581:]\n  train_f_drug2=f_drug2[3581:]\n  train_cell_line=cell_line[3581:]\n  train_synergy=synergy[3581:]\n  train_senstivity=senstivity[3581:]\n  test_a_drug1=a_drug1[0:3581]\n  test_a_drug2=a_drug2[0:3581]\n  test_f_drug1=f_drug1[0:3581]\n  test_f_drug2=f_drug2[0:3581]\n  test_cell_line=cell_line[0:3581]\n  test_synergy=synergy[0:3581]\n  test_senstivity=senstivity[0:3581]\n    \n#   train_synergy=np.array(train_synergy)\n#   train_senstivity=np.array(train_senstivity)\n#   test_synergy=np.array(test_synergy)\n#   test_senstivity=np.array(test_senstivity)\n\n  return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity\n\n\ndef get_data_me1(s):\n    \n\n    !gdown --id 1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\n    dele=pd.read_excel('deleted_index.xls',header=None)\n    dele=np.array(dele)\n    dele=dele-1\n    \n    \n#     !gdown http://www.bioinf.jku.at/software/DeepSynergy/data_test_fold0_tanh.p.gz\n#     file = gzip.open(data_file, 'rb')\n#     X_tr, X_val, X_train, X_test, y_tr, y_val, y_train, y_test = pickle.load(file)\n#     file.close()\n    \n\n    !gdown 1SvLGjU---dvwsuCTqRMehtV-Gmf2Hk52\n    labels = pd.read_csv('pcbi.1006752.s004.csv', index_col=0) \n    \n    h=len(np.array(labels))\n    #labels are duplicated for the two different ways of ordering in the data\n#     labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['Fold']!=test_fold)\n    \n#     h=len(idx_train[0])\n    xx=np.where(idx_train == dele)[1]\n    idx_train=np.delete(idx_train,xx)\n    xx1=np.where(idx_train == (dele+h))[1]\n    idx_train=np.delete(idx_train,xx1)\n#     idx_train=idx_train[!dele]\n    #indices of test data for model testing: fold 0\n    idx_test = np.where(labels['Fold']==test_fold)\n#     h1=len(idx_test[0])\n    yy=np.where(idx_test == dele)[1]\n    idx_test=np.delete(idx_test,yy)\n    yy1=np.where(idx_test == (dele+h))[1]\n    idx_test=np.delete(idx_test,yy1)\n    \n#     X_train = X[idx_train]\n#     X_test = X[idx_test]\n    idx_train1=idx_train + (idx_train.shape)\n    idx_test1=idx_test + (idx_test.shape)\n    \n    idx_train1=idx_train + (h)\n    idx_test1=idx_test + (h)\n    idx_train1=np.r_[idx_train,idx_train1]\n    idx_test1=np.r_[idx_test,idx_test1]\n    #choose idx_train1 and idx_test1 if you want to duplicate data\n    return idx_train1,idx_test1\n\ndef get_data_me(s):\n   \n    !gdown --id 1R_E1txnkHrwMQlBKpG7a4BHcZM4kRSkj\n    dele=pd.read_excel('deleted_deep.xlsx',header=None)\n\n    dele=np.array(dele)\n    dele=dele-1\n \n#     !gdown http://www.bioinf.jku.at/software/DeepSynergy/data_test_fold0_tanh.p.gz\n#     file = gzip.open(data_file, 'rb')\n#     X_tr, X_val, X_train, X_test, y_tr, y_val, y_train, y_test = pickle.load(file)\n#     file.close()\n    \n    !gdown 1HNEch5czfqjThnZpFpP-Dcy0Qi-DQdc1\n    labels = pd.read_csv('labels1.csv', index_col=0)\n\n\n    h=len(np.array(labels))\n    #labels are duplicated for the two different ways of ordering in the data\n#     labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['fold']!=test_fold)\n    \n#     h=len(idx_train[0])\n    xx=np.where(idx_train == dele)[1]\n    idx_train=np.delete(idx_train,xx)\n    xx1=np.where(idx_train == (dele+h))[1]\n    idx_train=np.delete(idx_train,xx1)\n#     idx_train=idx_train[!dele]\n    #indices of test data for model testing: fold 0\n    idx_test = np.where(labels['fold']==test_fold)\n#     h1=len(idx_test[0])\n    yy=np.where(idx_test == dele)[1]\n    idx_test=np.delete(idx_test,yy)\n    yy1=np.where(idx_test == (dele+h))[1]\n    idx_test=np.delete(idx_test,yy1)\n    \n#     X_train = X[idx_train]\n#     X_test = X[idx_test]\n\n    y_train = labels.iloc[idx_train]['synergy'].values\n    y_test = labels.iloc[idx_test]['synergy'].values\n    y1_train = labels.iloc[idx_train]['senstivity'].values\n    y1_test = labels.iloc[idx_test]['senstivity'].values\n \n    return y_train,y_test,idx_train,idx_test,y1_train,y1_test\n\n\n\nsmiles,data_to_repeat,unique_drugs,unique_cell,deleted_index=get_data()\n# smiles,data_to_repeat,unique_drugs,unique_cell,deleted_index=get_data()\ndeleted_index=deleted_index-1\n\nfeature,adjacency=graph_node(smiles)\n# feature,edge_index=graph_node_edge(smiles)\ndata_to_repeat=np.r_[data_to_repeat,data_to_repeat]\nl=int((data_to_repeat.shape[0])/2)\ndata_to_repeat[l:,0]=data_to_repeat[0:l,1]\ndata_to_repeat[l:,1]=data_to_repeat[0:l,0]\n# np.random.shuffle(data_to_repeat)\nsynergy=data_to_repeat[:,13]\nsenstivity=data_to_repeat[:,5]\n# synergy=data_to_repeat[:,3]\n# senstivity=data_to_repeat[:,5]\n\n# f_drug1,f_drug2,a_drug1,a_drug2,synergy,senstivity=repeat_smiles(data_to_repeat,unique_drugs,feature,adjacency,deleted_index,synergy,senstivity)\n\nf_drug1,f_drug2,a_drug1,a_drug2,feature_cell=repeat_smiles1(data_to_repeat,unique_drugs,feature,adjacency,unique_cell)\n# f_drug1,f_drug2,a_drug1,a_drug2,feature_cell=repeat_smiles_deep(data_to_repeat,unique_drugs,feature,adjacency,unique_cell)\n# y_train,y_test,index_train,index_test,y1_train,y1_test=get_data_me(3)\nindex_train,index_test=get_data_me1(2)\n# train_synergy1, train_senstivity1,test_synergy1, test_senstivity1,index_train,index_test=split_data(feature_cell,synergy,senstivity)\n# feature_cell=np.array(feature_cell).astype(float)\n# index_train,index_test=split_data1(feature_cell,synergy,senstivity)\n# index_test,index_train=index1(1)\n# index_train,index_test=preprocess(index_train,index_test)\n# train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy, train_senstivity,test_synergy, test_senstivity=train_test_input1(f_drug1,a_drug1,f_drug2,a_drug2,feature_cell,index_test,synergy,senstivity)\ntrain_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity=train_test_input(f_drug1,a_drug1,f_drug2,a_drug2,feature_cell,index_train,index_test,synergy,senstivity)\ntrain_cell_line=np.array(train_cell_line).astype(float)\ntest_cell_line=np.array(test_cell_line).astype(float)  \nnorm = \"tanh_norm\"\nif norm == \"tanh_norm\":\n    train_cell_line, mean, std, mean2, std2, feat_filt = normalize1(train_cell_line, norm=norm)\n    test_cell_line, mean, std, mean2, std2, feat_filt = normalize1(test_cell_line, mean, std, mean2, std2, \n                                                          feat_filt=feat_filt, norm=norm)\nelse:\n    train_cell_line, mean, std, feat_filt = normalize1(train_cell_line, norm=norm)\n    test_cell_line, mean, std, feat_filt = normalize1(test_cell_line, mean, std, feat_filt=feat_filt, norm=norm)\n  \nprint(synergy.shape)\n# print(np.random.shuffle(dataset1))\n# cc\n# cc1=[m for m, v in enumerate(unique_name) if cc in v]\n# # int((l))ik2\n# unique_cell[0].shape[1]\n# unique_name=unique_cell[:,0]\nprint(train_cell_line.shape)  \nprint(test_cell_line.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T15:49:12.184215Z","iopub.execute_input":"2022-12-12T15:49:12.184642Z","iopub.status.idle":"2022-12-12T15:51:23.638421Z","shell.execute_reply.started":"2022-12-12T15:49:12.184545Z","shell.execute_reply":"2022-12-12T15:51:23.636894Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting deepchem\n  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.3.5)\nRequirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.21.6)\nRequirement already satisfied: scipy<1.9 in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.7.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.0.2)\nCollecting rdkit\n  Downloading rdkit-2022.9.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/29.1 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.0.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->deepchem) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->deepchem) (2022.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from rdkit->deepchem) (9.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->deepchem) (3.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\nInstalling collected packages: rdkit, deepchem\nSuccessfully installed deepchem-2.7.1 rdkit-2022.9.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pysmiles\n  Downloading pysmiles-1.0.2-py2.py3-none-any.whl (22 kB)\nRequirement already satisfied: networkx~=2.0 in /opt/conda/lib/python3.7/site-packages (from pysmiles) (2.5)\nRequirement already satisfied: pbr in /opt/conda/lib/python3.7/site-packages (from pysmiles) (5.10.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx~=2.0->pysmiles) (5.1.1)\nInstalling collected packages: pysmiles\nSuccessfully installed pysmiles-1.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: openpyxl in /opt/conda/lib/python3.7/site-packages (3.0.10)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.7/site-packages (from openpyxl) (1.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting PubChemPy\n  Downloading PubChemPy-1.0.4.tar.gz (29 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: PubChemPy\n  Building wheel for PubChemPy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PubChemPy: filename=PubChemPy-1.0.4-py3-none-any.whl size=13834 sha256=c4a139c29ccc54384ef71cc06db7c6914cee0685ddf66a1d128acaebc7edc5fc\n  Stored in directory: /root/.cache/pip/wheels/7c/3d/8c/8192697412e9899dc55bbbb08bbc1197bef333caaa2a71c448\nSuccessfully built PubChemPy\nInstalling collected packages: PubChemPy\nSuccessfully installed PubChemPy-1.0.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting PyDrive\n  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (1.12.11)\nRequirement already satisfied: oauth2client>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (4.1.3)\nRequirement already satisfied: PyYAML>=3.0 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (6.0)\nRequirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (0.1.0)\nRequirement already satisfied: google-api-core<3dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.33.2)\nRequirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (0.20.4)\nRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\nRequirement already satisfied: google-auth<3dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.35.0)\nRequirement already satisfied: six<2dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.15.0)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (0.2.7)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (4.8)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.28.1)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.56.3)\nCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5\n  Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (59.8.0)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.2.4)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.2->PyDrive) (3.0.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.1.0)\nBuilding wheels for collected packages: PyDrive\n  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PyDrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27453 sha256=17bd77c2d5d881dee0a50f7ee965f69094f452aa67a011b678c6ace86035d863\n  Stored in directory: /root/.cache/pip/wheels/57/cc/07/6aac75f5395a224650905accd38c868c2276782a56f1046b7b\nSuccessfully built PyDrive\nInstalling collected packages: protobuf, PyDrive\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Uninstalling protobuf-3.19.4:\n      Successfully uninstalled protobuf-3.19.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\ntensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\ntensorflow 2.6.4 requires tensorboard<2.7,>=2.6.0, but you have tensorboard 2.10.1 which is incompatible.\ntensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\ntensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\ntensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.12.0 requires protobuf<=3.20.1,>=3.12.2, but you have protobuf 3.20.3 which is incompatible.\nnnabla 1.31.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.47.0 requires grpcio>=1.47.0, but you have grpcio 1.43.0 which is incompatible.\ngcsfs 2022.5.0 requires fsspec==2022.5.0, but you have fsspec 2022.8.2 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed PyDrive-1.3.1 protobuf-3.20.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting xlrd==2.0.1\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: xlrd\nSuccessfully installed xlrd-2.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: Pandas==1.3.5 in /opt/conda/lib/python3.7/site-packages (1.3.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from Pandas==1.3.5) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from Pandas==1.3.5) (2022.1)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from Pandas==1.3.5) (1.21.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->Pandas==1.3.5) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting gdown\n  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.28.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.7.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.64.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.15.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.9.24)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","output_type":"stream"},{"name":"stdout","text":"hjjbjh\nDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 35.1MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1z1sWG4E9BqipP0iczLrbR4qxSa4KG95_\nTo: /kaggle/working/pcbi.1006752.s004.xls\n100%|███████████████████████████████████████| 5.24M/5.24M [00:00<00:00, 223MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 32.3MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1NoUKCRZ0CkNVFP1-578x_L0otPUMkz6T\nTo: /kaggle/working/unique934_cell.xlsx\n100%|████████████████████████████████████████| 255k/255k [00:00<00:00, 74.4MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\nTo: /kaggle/working/deleted_index.xls\n100%|██████████████████████████████████████| 58.9k/58.9k [00:00<00:00, 46.6MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\nTo: /kaggle/working/deleted_index.xls\n100%|██████████████████████████████████████| 58.9k/58.9k [00:00<00:00, 56.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1SvLGjU---dvwsuCTqRMehtV-Gmf2Hk52\nTo: /kaggle/working/pcbi.1006752.s004.csv\n100%|███████████████████████████████████████| 2.57M/2.57M [00:00<00:00, 155MB/s]\n(37810,)\n(28558, 934)\n(7244, 934)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow\nfrom networkx.readwrite.graph6 import data_to_n\nfrom tensorflow.python.training.tracking import data_structures\n!pip install spektral\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import add,PReLU, Flatten,Reshape,Dropout, Input,Dense,Add,concatenate,BatchNormalization, Activation#,MultiHeadAttention,AdditiveAttention\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n# from tensorflow.random import set_seed\nfrom spektral.data.loaders import SingleLoader\nfrom spektral.datasets.citation import Citation\nfrom spektral.layers import GATConv, GlobalAvgPool,GlobalMaxPool, GCNConv,GlobalAttentionPool\nfrom spektral.transforms import LayerPreprocess\n# import tensorflow.compat.v1.keras.backend as K\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score#,AUC\nfrom scipy.stats import pearsonr,spearmanr\nfrom sklearn.metrics import roc_curve,auc\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras import regularizers\ndef generate_network1(x_in1,a_in1,x_in2,a_in2,cell, inDrop, drop):\n    # fill the architecture params from dict\n    gcn_layers = [78,156,312]\n    \n    gan_layers = [128,128]\n    cell_layers = [512,265,128]#for gcn\n#     cell_layers = [2048,512,128*2] #for gan\n    snp_layers = [512,128]#for gcn\n#     snp_layers = [2048,512,256]# for gan\n    dsn1_layers = [1024,2048,1024]\n    dsn2_layers = [1024,2048,1024]\n    N = x_in1[0].shape[0]\n    F = x_in1[0].shape[1]\n    \n    channels = 8  # Number of channels in each head of the first GAT layer\n    n_attn_heads = 10  # Number of attention heads in first GAT layer\n    dropout = 0.2 # Dropout rate for the features and adjacency matrix\n    drop=0.2\n    l2_reg = 1e-3  # L2 regularization rate\n    # contruct two parallel networks\n    x_in1 = Input(shape=(N,F,),name='x_in1')\n    a_in1 = Input((N,N,),name='a_in1')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d1 = Dropout(dropout)(x_in1)\n             middle_layer_d1  = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activty_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d1, a_in1])\n#              middle_layer_d1=BatchNormalization()(middle_layer_d1)\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n             gcn_output1=GlobalMaxPool(name='a1')(middle_layer_d1)\n             \n#              gcn_output1=Flatten()(x_in1)\n#              gcn_output1=GlobalAttentionPool(1000)(x_in1)#(middle_layer_d1)\n             \n             \n        else:\n             middle_layer_d1 = Dropout(dropout)(middle_layer_d1)\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n#              middle_layer_d1=BatchNormalization()(middle_layer_d1)\n#     print(x_in1.shape)\n    # print(a_in1.shape)\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n#             middle_layer1  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in1, a_in1])\n            \n#         elif l == (len(gan_layers)-1):\n#             middle_layer1 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n            \n#             gan_output1=GlobalMaxPool()(middle_layer1)\n# #             gan_output1 = Dense(int(gan_layers[l]), activation='tanh')(gan_output1)\n#         else:\n#             middle_layer1 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n            \n            \n    \n#   156  concatModel1 = concatenate([gcn_output1, gan_output1])\n    gcn_output1 = Dense(256, activation='relu')(gcn_output1)\n    gcn_output1 = Dropout(dropout)(gcn_output1)\n    gcn_output1 = Dense(128, activation='linear')(gcn_output1)\n    \n    gan_output1 = Dense(128, activation='relu')(gan_output1)\n   \n    \n    concatModel1 =  gan_output1\n    # # addtModel = Add([gcn_output, gan_output])\n    x_in2 = Input(shape=(N,F,),name='x_in2')\n    a_in2 = Input((N,N,),name='a_in2')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d2 = Dropout(dropout)(x_in2)\n             middle_layer_d2 =  GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d2, a_in2])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n             gcn_output2=GlobalMaxPool(name='a2')(middle_layer_d2)\n#             gcn_output2=Flatten()(x_in2)\n#             gcn_output2=GlobalAttentionPool(1000)(x_in2)#(middle_layer_d2)\n             \n        else:\n             middle_layer_d2 = Dropout(dropout)(middle_layer_d2)\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n#             middle_layer2  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in2, a_in2])\n           \n#         elif l == (len(gan_layers)-1):\n#             middle_layer2 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n#             gan_output2=GlobalMaxPool()(middle_layer2)\n# #             gan_output2 = Dense(int(gan_layers[l]), activation='tanh')(gan_output2)\n#         else:\n#             middle_layer2 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n    # print(gcn_output2.shape)\n    # print(gan_output2.shape)        \n#   156  concatModel2 = concatenate([gcn_output2, gan_output2])\n    gcn_output2 = Dense(256, activation='relu')(gcn_output2)\n    gcn_output2 = Dropout(dropout)(gcn_output2)\n    gcn_output2 = Dense(128, activation='linear')(gcn_output2)\n    \n    gan_output2 = Dense(128, activation='relu')(gan_output2)\n    \n    concatModel2 = gcn_output2\n    # concatModel2 = Add()([gcn_output2, gan_output2])\n    for cell_layer in range(len(cell_layers)):\n      if cell_layer == 0:\n        input_cell    = Input(shape=(cell[0].shape[0],))\n#         input_cell1 = BatchNormalization()(input_cell)\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,)(input_cell)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      elif cell_layer == (len(cell_layers)-1):\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,)(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      #  snp_output = Dense(1, activation='linear')(snpFC)\n      else:\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,)(cellFC)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n    # print(concatModel1.shape)\n    # print(cellFC.shape)\n    \n    concatModel=concatenate([concatModel2,concatModel1,cellFC])\n    concatModel=BatchNormalization()(concatModel)\n#     input2=concatenate([concatModel2,cellFC])\n    # input1=concatenate([gan_output1,cellFC])\n    # input2=concatenate([gan_output2,cellFC])\n    # # addtModel = Add([concatModel1, concatModel2])\n    # dsn1_layers = [2048,4096,2048]\n    # dsn2_layers = [2048,4096,2048]\n    # snp_layers = [2048,1024]\n    # # contruct two parallel networks\n    \n       \n#     for snp_layer in range(len(snp_layers)):\n#        if snp_layer == 0:\n#          snpFC1 = Dense(int(snp_layers[snp_layer]), activation='linear',use_bias=True,)(concatModel)\n#          snpFC1=PReLU()(snpFC1)\n#          snpFC = Dropout(float(drop))(snpFC1)\n# #          snpFC=BatchNormalization()(snpFC)\n#        elif snp_layer == (len(snp_layers)-1):\n#          snpFC = Dense(int(snp_layers[snp_layer]), activation='linear',use_bias=True,)(snpFC)\n#          snpFC=PReLU()(snpFC)\n# #          layer = MultiHeadAttention(num_heads=2, key_dim=2)\n# #          rr_task1=Reshape([1,snpFC.shape[1]])(snpFC)\n# #          a_task1=layer(rr_task1,rr_task1)\n#          snp_output1 = Dense(1, activation='linear')(snpFC)\n#        else:\n#           snpFC = Dense(int(snp_layers[snp_layer]), activation='linear',use_bias=True,)(snpFC)\n#           snpFC=PReLU()(snpFC)\n#           snpFC = Dropout(float(drop))(snpFC)\n\n\n    for snp_layer in range(len(snp_layers)):\n       if snp_layer == 0:\n         snpFC1 = Dense(int(snp_layers[snp_layer]), activation='relu',use_bias=True,)(concatModel)\n#          snpFC1=PReLU()(snpFC1)\n         snpFC = Dropout(float(drop))(snpFC1)\n#          snpFC=BatchNormalization()(snpFC)\n       elif snp_layer == (len(snp_layers)-1):\n         snpFC = Dense(int(snp_layers[snp_layer]), activation='relu',use_bias=True,)(snpFC)\n#          snpFC=PReLU()(snpFC)\n#          layer = MultiHeadAttention(num_heads=2, key_dim=2)\n#          rr_task1=Reshape([1,snpFC.shape[1]])(snpFC)\n#          a_task1=layer(rr_task1,rr_task1)\n         snp_output1 = Dense(1, activation='linear')(snpFC)\n       else:\n          snpFC = Dense(int(snp_layers[snp_layer]), activation='relu',use_bias=True,)(snpFC)\n#           snpFC=PReLU()(snpFC)\n          snpFC = Dropout(float(drop))(snpFC)\n#     model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [f_task1,f_task2])\n    model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [snp_output1])\n    # tf.keras.utils.plot_model(model.build_graph(), to_file=\"model.png\",\n    #        expand_nested=True, show_shapes=True)\n    print(model.summary())\n    return model\n\ndef trainer1(model, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity, epo, batch_size, earlyStop):\n#     model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=float(l_rate), momentum=0.9,nesterov=True, clipvalue=0.3))\n    model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=float(l_rate)))#, beta_1=0.9, beta_2=0.999, amsgrad=True))\n    model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line], [train_synergy], shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 )#,restore_best_weights=True,validation_split=0.1) \n                  #  validation_data=([val_input1,val_input2], [val_synergy,val_senstivity]))\n    # model.fit({\"input1\":train_drug1, \"input2\":train_drug2}, {\"synergy\":train_synergy,\"senstivity\":train_senstivity}, epochs=epo, shuffle=True, batch_size=batch_size,verbose=1) \n    #                validation_data=({\"input1\":val_input1,\"input2\":val_input2}, {\"synergy\":val_synergy,\"senstivity\":val_senstivity}))\n\n    return model\ntrain_f_drug1=np.array(train_f_drug1)\ntrain_a_drug1=np.array(train_a_drug1)\ntrain_f_drug2=np.array(train_f_drug2)\ntrain_a_drug2=np.array(train_a_drug2)\ntrain_cell_line=np.array(train_cell_line)\ntest_f_drug1=np.array(test_f_drug1)\ntest_a_drug1=np.array(test_a_drug1)\ntest_f_drug2=np.array(test_f_drug2)\ntest_a_drug2=np.array(test_a_drug2)\ntest_cell_line=np.array(test_cell_line)\ntrain_synergy=np.array(train_synergy)\ntrain_senstivity=np.array(train_senstivity)\ntest_synergy=np.array(test_synergy)\ntest_senstivity=np.array(test_senstivity)\n\n\n# train_synergy\n# test_synergy\n# train_synergy.shape\n# test_synergy.shape\n# train_senstivity","metadata":{"execution":{"iopub.status.busy":"2022-12-12T15:51:23.641644Z","iopub.execute_input":"2022-12-12T15:51:23.642386Z","iopub.status.idle":"2022-12-12T15:51:53.298780Z","shell.execute_reply.started":"2022-12-12T15:51:23.642339Z","shell.execute_reply":"2022-12-12T15:51:53.297664Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.6.4)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nCollecting tensorboard<2.7,>=2.6.0\n  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (5.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.20.3)\nCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nCollecting numpy~=1.19.2\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.15.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.43.0)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.35.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (3.3.7)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.6.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.28.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.2.2)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.8.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.4.6)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (59.8.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (4.13.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2.1.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (3.2.0)\nInstalling collected packages: typing-extensions, numpy, h5py, tensorboard\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.1.1\n    Uninstalling typing_extensions-4.1.1:\n      Successfully uninstalled typing_extensions-4.1.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.7.0\n    Uninstalling h5py-3.7.0:\n      Successfully uninstalled h5py-3.7.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.10.1\n    Uninstalling tensorboard-2.10.1:\n      Successfully uninstalled tensorboard-2.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nxarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\ntensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\nrich 12.6.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.7.7 requires tensorboard>=2.9.1, but you have tensorboard 2.6.0 which is incompatible.\npytorch-lightning 1.7.7 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.12 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nonnx 1.12.0 requires protobuf<=3.20.1,>=3.12.2, but you have protobuf 3.20.3 which is incompatible.\nnnabla 1.31.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\nnnabla 1.31.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\njaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\njax 0.3.23 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\nflax 0.6.1 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\nfeaturetools 1.11.1 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndeepchem 2.7.1 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\ncmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\nallennlp 2.10.1 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\nallennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\naioitertools 0.11.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.93 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed h5py-3.1.0 numpy-1.19.5 tensorboard-2.6.0 typing-extensions-3.10.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting spektral\n  Downloading spektral-1.2.0-py3-none-any.whl (140 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m938.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from spektral) (1.0.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from spektral) (1.7.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from spektral) (2.5)\nRequirement already satisfied: tensorflow>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from spektral) (2.6.4)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from spektral) (4.9.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from spektral) (1.0.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from spektral) (4.64.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from spektral) (1.19.5)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from spektral) (1.3.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from spektral) (2.28.1)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (5.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\nRequirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.1.0)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (2.6.0)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (2.6.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.20.3)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.12)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.1.2)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.15.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.37.1)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.1.0)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.4.0)\nRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (2.6.0)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.12.1)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.43.0)\nRequirement already satisfied: typing-extensions<3.11,>=3.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.10.0.2)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.15.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->spektral) (5.1.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->spektral) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->spektral) (2022.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (1.26.12)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->spektral) (3.1.0)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=2.2.0->spektral) (1.5.2)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (1.35.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (59.8.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (3.3.7)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.4.6)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.6.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (2.2.2)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (1.8.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (4.8)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (4.2.4)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (4.13.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (3.2.0)\nInstalling collected packages: spektral\nSuccessfully installed spektral-1.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"### from sklearn.metrics import roc_curve,auc\nl_rate = 0.0001\ninDrop = 0.2\ndrop = 0.2\nmax_epoch = 500\n# batch_size = 128 #gcn\nbatch_size = 64 #gan\nearlyStop_patience = 20#np.ceil(train_f_drug1.shape[0]/batch_size)#1000\n\n# model1= generate_network1(train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line, inDrop, drop)\n# plot_model(model1, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n# model1=trainer1(model1, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity,max_epoch, batch_size,\n#                                 earlyStop_patience)\n# # p1,p2= predict(model, [test_input1,test_input2])\n# p1= model1.predict( [test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line])\n# # s1=np.zeros(len(p1))\n# # s2=np.zeros(len(p1))\n# # for i in range(len(p1)):\n# #     n1=p1[0]\n# #     h1=n1[0]\n# #     x1=h1[0]\n# #     s1[i]=x1\n# #     n2=p2[i]\n# #     h2=n2[0]\n# #     x2=h2[0]\n\n# synergy_error=mean_squared_error(test_synergy, p1)\n# # senstivity_error=mean_squared_error(test_senstivity, p2)\n# # p1,p2\n\n# # senstivity_error=mean_squared_error(test_senstivity, p2)\n# synergy_error1=mean_absolute_error(test_synergy, p1)\n# # senstivity_error1=mean_absolute_error(test_senstivity, p2)\n# p12=[]\n# for i in range(len(p1)):\n#   x=p1[i]\n#   p12.append(x[0])\n# p12=np.array(p12)\n# synergy_error2=r2_score(test_synergy, p12)\n# # senstivity_error2=r2_score(test_senstivity, p2)\n# print(\"synergy_mean_squared_error\",synergy_error)\n# # print(\"senstivity_mean_squared_error\",senstivity_error)\n# print(\"synergy_mean_absolute_error\",synergy_error1)\n# # print(\"senstivity_mean_absolute_error\",senstivity_error1)\n# print(\"synergy_r2_score\",synergy_error2)\n# # print(\"senstivity_r2_score\",senstivity_error2)\n# losses = model1.evaluate([test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line],[test_synergy],batch_size =64)\n# print(losses)\n\n\n# synergy_pear= pearsonr(test_synergy, p12)\n# synergy_spear= spearmanr(test_synergy, p12)\n# print(\"synergy_pear\",synergy_pear)\n# print(\"synergy_spear\",synergy_spear)\n# p1\n# # senstivity_pear= pearsonr(test_senstivity, p2)\n# # senstivity_spear= spearmanr(test_senstivity, p2)\n# # print(\"senstivity_pear\",senstivity_pear)\n# # print(\"senstivity_spear\",senstivity_spear)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T15:51:53.300726Z","iopub.execute_input":"2022-12-12T15:51:53.301256Z","iopub.status.idle":"2022-12-12T15:51:53.308835Z","shell.execute_reply.started":"2022-12-12T15:51:53.301213Z","shell.execute_reply":"2022-12-12T15:51:53.308007Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# import tensorflow_datasets as tfds\nimport tensorflow as tf\ndef scaled_dot_product_attention(q, k, v, mask=None):\n  \"\"\"Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead)\n  but it must be broadcastable for addition.\n\n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable\n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n  Returns:\n    output, attention_weights\n  \"\"\"\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)\n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits)#, axis=-1)#tf.nn.linear()#, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output,attention_weights \n\n#multi-head\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self,*, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n\n    assert d_model % self.num_heads == 0\n\n    self.depth = d_model // self.num_heads\n\n    self.wq = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wk = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wv = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n\n    self.dense = tf.keras.layers.Dense(d_model)#,activation='relu',use_bias='true')\n\n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n\n  def call(self, v, k, q, mask=None):\n    batch_size = tf.shape(q)[0]\n\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n#     q=PReLU()(q)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n#     k=PReLU()(k)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n#     v=PReLU()(v)\n\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention,\n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n    return output#, attention_weights\n\n\n\nclass CrossStitch(tf.keras.layers.Layer):\n\n    \"\"\"Cross-Stitch implementation according to arXiv:1604.03539\n    Implementation adapted from https://github.com/helloyide/Cross-stitch-Networks-for-Multi-task-Learning\"\"\"\n\n    def __init__(self, num_tasks, *args, **kwargs):\n        \"\"\"initialize class variables\"\"\"\n        self.num_tasks = num_tasks\n        super(CrossStitch, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        \"\"\"initialize the kernel and set the instance to 'built'\"\"\"\n        self.kernel = self.add_weight(name=\"kernel\",\n                                      shape=(self.num_tasks,\n                                             self.num_tasks),\n                                      initializer='identity',\n                                      trainable=True)\n        super(CrossStitch, self).build(input_shape)\n\n    def call(self, xl):\n        \"\"\"\n        called by TensorFlow when the model gets build. \n        Returns a stacked tensor with num_tasks channels in the 0 dimension, \n        which need to be unstacked.\n        \"\"\"\n        if (len(xl) != self.num_tasks):\n            # should not happen\n            raise ValueError()\n\n        out_values = []\n        for this_task in range(self.num_tasks):\n            this_weight = self.kernel[this_task, this_task]\n            out = tf.math.scalar_mul(this_weight, xl[this_task])\n            for other_task in range(self.num_tasks):\n                if this_task == other_task:\n                    continue  # already weighted!\n                other_weight = self.kernel[this_task, other_task]\n                out += tf.math.scalar_mul(other_weight, xl[other_task])\n            out_values.append(out)\n        # HACK!\n        # unless we stack, and then unstack the tensors, TF (2.0.0) can't follow\n        # the graph, so it aborts during model initialization.\n        # return tf.stack(out_values, axis=0)\n        return out_values[0],out_values[1]\n\n    def compute_output_shape(self, input_shape):\n        return [self.num_tasks] + input_shape\n\n    def get_config(self):\n        \"\"\"implemented so keras can save the model to json/yml\"\"\"\n        config = {\n            \"num_tasks\": self.num_tasks\n        }\n        base_config = super(CrossStitch, self).get_config()\n        return dict(list(config.items()) + list(base_config.items()))\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T15:51:53.312336Z","iopub.execute_input":"2022-12-12T15:51:53.312697Z","iopub.status.idle":"2022-12-12T15:51:53.335305Z","shell.execute_reply.started":"2022-12-12T15:51:53.312662Z","shell.execute_reply":"2022-12-12T15:51:53.334341Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\ndef generate_network_att1(x_in1,a_in1,x_in2,a_in2,cell, inDrop, drop):\n    # fill the architecture params from dict\n#     gcn_layers = [1024,512,156]\n#     gan_layers = [1024,512]\n#     cell_layers = [2048,512]#,2048]\n#     snp_layers = [1024,512,128]#,2048]\n#     dsn1_layers = [1024,2048,1024]\n#     dsn2_layers = [1024,2048,1024]\n    # fill the architecture params from dict\n    gcn_layers = [78,156,312]\n#     gan_layers = [128,128]\n#     gcn_layers = [32,64,128]\n    cell_layers = [512,265,128]#for gcn\n#     cell_layers = [2048,512,256] #for gan\n    snp_layers = [512,128]#for gcn\n#     snp_layers = [2048,512,256]# for gan\n    dsn1_layers = [1024,2048,1024]\n    dsn2_layers = [1024,2048,1024]\n    N = x_in1[0].shape[0]\n    F = x_in1[0].shape[1]\n    \n    channels = 8  # Number of channels in each head of the first GAT layer\n    n_attn_heads = 10  # Number of attention heads in first GAT layer\n    dropout = 0.2 # Dropout rate for the features and adjacency matrix\n    drop=0.2\n    l2_reg = 1e-3\n    N = x_in1[0].shape[0]\n    F = x_in1[0].shape[1]\n    \n    channels = 8  # Number of channels in each head of the first GAT layer\n    n_attn_heads = 8  # Number of attention heads in first GAT layer\n    dropout = 0.2 # Dropout rate for the features and adjacency matrix\n    drop=0.2\n    l2_reg = 1e-3  # L2 regularization rate\n    # contruct two parallel networks\n    x_in1 = Input(shape=(N,F,),name='x_in1')\n    a_in1 = Input((N,N,),name='a_in1')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d1 = Dropout(dropout)(x_in1)\n             middle_layer_d1  = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activty_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d1, a_in1])\n#              middle_layer_d1=concatenate([middle_layer_d1,do_1_d1])\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n             gcn_output1=GlobalMaxPool()(middle_layer_d1)\n#              gcn_output1=GlobalAttentionPool(gcn_layers[l],name='a1')(middle_layer_d1)\n             \n#              gcn_output1=Flatten()(x_in1)\n#              gcn_output1=GlobalAttentionPool(1000)(x_in1)#(middle_layer_d1)\n             \n             \n        else:\n             middle_layer_d1 = Dropout(dropout)(middle_layer_d1)\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n#              middle_layer_d1=concatenate([middle_layer_d1,middle_layer_d11])\n#              middle_layer_d1=BatchNormalization()(middle_layer_d1)\n#     print(x_in1.shape)\n    # print(a_in1.shape)\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n#             middle_layer1  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in1, a_in1])\n#             middle_layer1=concatenate([middle_layer1,x_in1]) \n#         elif l == (len(gan_layers)-1):\n#             middle_layer1 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n#             gan_output1=GlobalAttentionPool(512)(middle_layer1)\n# #             gan_output1 = Dense(int(gan_layers[l]), activation='elu')(gan_output1)\n#         else:\n#             middle_layer11 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n#             middle_layer1=concatenate([middle_layer1,middle_layer11])\n    \n#  156   concatModel1 = concatenate([gcn_output1, gan_output1])\n    gcn_output1 = Dense(256, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output1)\n    gcn_output1 = Dropout(dropout)(gcn_output1)\n    gcn_output1 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output1)\n    \n#     gan_output1 = Dense(128, activation='relu')(gan_output1)\n    \n    concatModel1 =  gcn_output1\n    # # addtModel = Add([gcn_output, gan_output])\n    x_in2 = Input(shape=(N,F,),name='x_in2')\n    a_in2 = Input((N,N,),name='a_in2')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d2 = Dropout(dropout)(x_in2)\n             middle_layer_d2 =  GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d2, a_in2])\n#              middle_layer_d2=concatenate([middle_layer_d2,do_1_d2])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n#              gcn_output2=GlobalAttentionPool(gcn_layers[l],name='a2')(middle_layer_d2)\n             gcn_output2=GlobalMaxPool()(middle_layer_d2)\n#             gcn_output2=Flatten()(x_in2)\n#             gcn_output2=GlobalAttentionPool(1000)(x_in2)#(middle_layer_d2)\n             \n        else:\n             middle_layer_d2 = Dropout(dropout)(middle_layer_d2)\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n#              middle_layer_d2=concatenate([middle_layer_d2,middle_layer_d21])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n            \n#             middle_layer2  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in2, a_in2])\n#             middle_layer2=concatenate([middle_layer2,x_in2])\n#         elif l == (len(gan_layers)-1):\n#             middle_layer2 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n#             gan_output2=GlobalAttentionPool(512)(middle_layer2)\n# #             gan_output2 = Dense(int(gan_layers[l]), activation='tanh')(gan_output2)\n#         else:\n#             middle_layer21 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n#             middle_layer2=concatenate([middle_layer2,middle_layer_d21])\n    # print(gcn_output2.shape)\n    # print(gan_output2.shape)        \n#  156   concatModel2 = concatenate([gcn_output2, gan_output2])\n    gcn_output2 = Dense(256, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output2)\n    gcn_output2 = Dropout(dropout)(gcn_output2)\n    gcn_output2 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output2)\n    \n#     gan_output2 = Dense(128, activation='relu')(gan_output2)\n        \n    concatModel2 = gcn_output2\n    # concatModel2 = Add()([gcn_output2, gan_output2])\n    for cell_layer in range(len(cell_layers)):\n      if cell_layer == 0:\n        input_cell    = Input(shape=(cell[0].shape[0],))\n#         input_cell1 = BatchNormalization()(input_cell)\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(input_cell)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      elif cell_layer == (len(cell_layers)-1):\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      #  snp_output = Dense(1, activation='linear')(snpFC)\n      else:\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n    # print(concatModel1.shape)\n    # print(cellFC.shape)\n    \n    concatModel=concatenate([concatModel2,concatModel1,cellFC])\n    concatModel=BatchNormalization()(concatModel)\n#     input2=concatenate([concatModel2,cellFC])\n    # input1=concatenate([gan_output1,cellFC])\n    # input2=concatenate([gan_output2,cellFC])\n    # # addtModel = Add([concatModel1, concatModel2])\n    # dsn1_layers = [2048,4096,2048]\n    # dsn2_layers = [2048,4096,2048]\n    # snp_layers = [2048,1024]\n    # # contruct two parallel networks\n  \n    \n#     task1=Dense(512, activation='relu',use_bias=True)(concatModel)\n# #     task1=PReLU()(task1)\n#     task2=Dense(512, activation='relu',use_bias=True)(concatModel)\n    \n    layer1 =MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task1= layer1(concatModel,concatModel,concatModel)\n    layer2 = MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task2= layer2(concatModel,concatModel,concatModel)\n    task11 = Reshape([a_task1.shape[2]])(a_task1)\n    task22 = Reshape([a_task2.shape[2]])(a_task2)\n    task1=concatenate([task11,concatModel])\n    task2=concatenate([task22,concatModel])\n#     task2=PReLU()(task2)\n#     task1=BatchNormalization()(task1)\n#     task2=BatchNormalization()(task2)\n    r_task1,r_task2 = CrossStitch(2)([task1,task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = apply_cross_stitch([task1,task2])\n#     r_task1=PReLU()(r_task1)\n#     r_task2=PReLU()(r_task2)\n\n#     r_task1=concatenate([r_task1,task1])\n#     r_task2=concatenate([r_task2,task2])\n#     r_task1=BatchNormalization()(r_task1)\n#     r_task2=BatchNormalization()(r_task2)\n#     r_task1=concatenate([r_task1,task1])\n#     r_task2=concatenate([r_task2,task2])\n    \n#     r_task1=Dense(256, activation='relu',use_bias=True)(r_task1)\n    \n#     r_task2=Dense(256, activation='relu',use_bias=True)(r_task2)\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n    \n    \n    r_task1=Dense(1024,activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    \n    r_task2=Dense(1024, activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n    r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n    \n#     layer1 =MultiHeadAttention(d_model=r_task1.shape[1], num_heads=4)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=4)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     task11 = Reshape([a_task1.shape[2]])(a_task1)\n#     task22 = Reshape([a_task2.shape[2]])(a_task2)\n#     r_task1=concatenate([task11,r_task1])\n#     r_task2=concatenate([task22,r_task2])\n    \n    r_task1=concatenate([r_task1,task1])\n    r_task2=concatenate([r_task2,task2])\n    \n#     r_task1=PReLU()(r_task1)\n#     r_task2=PReLU()(r_task2)\n\n \n   \n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = apply_cross_stitch([r_task1,r_task2])\n\n    \n#     r_task1=BatchNormalization()(r_task1)\n#     r_task2=BatchNormalization()(r_task2)\n    \n#     r_task1=Dense(128, activation='linear',use_bias=True)(r_task1)\n    \n#     r_task2=Dense(128, activation='linear',use_bias=True)(r_task2)\n#     r_task1=PReLU()(r_task1)\n#     r_task2=PReLU()(r_task2)\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = apply_cross_stitch(r_task1,r_task2)\n\n    \n#     r_task1=BatchNormalization()(r_task1)\n#     r_task2=BatchNormalization()(r_task2)\n    \n     \n#     layer1 =MultiHeadAttention(d_model=r_task1.shape[1], num_heads=4)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=4)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task1=BatchNormalization()(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n#     a_task2=BatchNormalization()(a_task2)\n    \n    \n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n# #  \n#     rr_task2=Reshape([1,r_task2.shape[1]])(r_task2)\n#     a_task1=layer(rr_task1,rr_task1)\n#     a_task2=layer(rr_task2,rr_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n    \n#     layer1 = MultiHeadAttention(d_model=r_task1.shape[1], num_heads=8)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=8)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n    \n#     r_task1 = Dense(64, activation='linear',name=\"synergy\")(a_task1)\n#     r_task2 = Dense(64, activation='linear',name=\"senstivity\")(a_task2)\n    \n#     layer1 = MultiHeadAttention(d_model=r_task1.shape[1], num_heads=8)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=8)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task1=BatchNormalization()(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n#     a_task2=BatchNormalization()(a_task2)\n   \n    r_task1 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(128, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n    r_task2=PReLU()(r_task2)\n    r_task1 = Dense(64, activation='linear',name=\"synergy\",kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(64, activation='relu',name=\"senstivity\",kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n# \n#     r_task1 = Dense(512, activation='relu')(r_task1)\n# #   \n#     r_task2 = Dense(512, activation='relu')(r_task2)\n\n#     r_task1 = Dense(256, activation='relu',name=\"synergy\")(r_task1)\n#     r_task2 = Dense(256, activation='relu',name=\"senstivity\")(r_task2)\n    \n    snp_output1 = Dense(1, activation='linear')(r_task1)\n#     snp_output1=PReLU()(snp_output1)\n    snp_output2 = Dense(1, activation='relu')(r_task2)\n#     model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [f_task1,f_task2])\n    model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [snp_output1,snp_output2])\n    # tf.keras.utils.plot_model(model.build_graph(), to_file=\"model.png\",\n    #        expand_nested=True, show_shapes=True)\n    print(model.summary())\n    return model\n\ndef trainer_att1(model, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity, epo, batch_size, earlyStop):\n    model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=float(l_rate)))#, beta_1=0.9, beta_2=0.999, amsgrad=True))\n#     model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=float(l_rate), momentum=0.9, nesterov=True, clipnorm=1.))\n    model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line], [train_synergy,train_senstivity], shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 )#,restore_best_weights=True,validation_split=0.1) \n                  #  validation_data=([val_input1,val_input2], [val_synergy,val_senstivity]))\n    # model.fit({\"input1\":train_drug1, \"input2\":train_drug2}, {\"synergy\":train_synergy,\"senstivity\":train_senstivity}, epochs=epo, shuffle=True, batch_size=batch_size,verbose=1) \n    #                validation_data=({\"input1\":val_input1,\"input2\":val_input2}, {\"synergy\":val_synergy,\"senstivity\":val_senstivity}))\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-12-12T15:51:53.338694Z","iopub.execute_input":"2022-12-12T15:51:53.339025Z","iopub.status.idle":"2022-12-12T15:51:53.380217Z","shell.execute_reply.started":"2022-12-12T15:51:53.339001Z","shell.execute_reply":"2022-12-12T15:51:53.379154Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# l_rate = 0.0001\n# inDrop = 0.2\n# drop = 0.2\n# max_epoch = 500\n# batch_size =16\n\n\n# earlyStop_patience = np.ceil(train_f_drug1.shape[0]/batch_size)#1000\nmodel_att1= generate_network_att1(train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line, inDrop, drop)\nmodel_att1=trainer_att1(model_att1, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity,max_epoch, batch_size,\n                                earlyStop_patience)\n\n# p1,p2= predict(model, [test_input1,test_input2])\nap111,ap221= model_att1.predict( [test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line])\nasynergy_error1=mean_squared_error(test_synergy, ap111)\nasenstivity_error1=mean_squared_error(test_senstivity, ap221)\nasynergy_error11=mean_absolute_error(test_synergy, ap111)\nasenstivity_error11=mean_absolute_error(test_senstivity, ap221)\nasynergy_error21=r2_score(test_synergy, ap111)\nasenstivity_error21=r2_score(test_senstivity, ap221)\nprint(\"msynergy_mean_squared_error\",asynergy_error1)\nprint(\"msenstivity_mean_squared_error\",asenstivity_error1)\nprint(\"msynergy_mean_absolute_error\",asynergy_error11)\nprint(\"msenstivity_mean_absolute_error\",asenstivity_error11)\nprint(\"msynergy_r2_score\",asynergy_error21)\nprint(\"msenstivity_r2_score\",asenstivity_error21)\ncross_att1 = model_att1.evaluate([test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line],[test_synergy,test_senstivity],batch_size =64)\nprint(cross_att1)\nasynergy_pear1= pearsonr(test_synergy, ap111)\nasynergy_spear1= spearmanr(test_synergy, ap111)\nprint(\"msynergy_pear\",asynergy_pear1)\nprint(\"msynergy_spear\",asynergy_spear1)\nasenstivity_pear1= pearsonr(test_senstivity, ap221)\nasenstivity_spear1= spearmanr(test_senstivity, ap221)\nprint(\"msenstivity_pear\",asenstivity_pear1)\nprint(\"msenstivity_spear\",asenstivity_spear1)\nap111,ap221\n# yourTerminal:prompt> jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10 \n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T15:51:53.381670Z","iopub.execute_input":"2022-12-12T15:51:53.382033Z","iopub.status.idle":"2022-12-12T16:50:36.088273Z","shell.execute_reply.started":"2022-12-12T15:51:53.381998Z","shell.execute_reply":"2022-12-12T16:50:36.087221Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nx_in2 (InputLayer)              [(None, 69, 32)]     0                                            \n__________________________________________________________________________________________________\nx_in1 (InputLayer)              [(None, 69, 32)]     0                                            \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 69, 32)       0           x_in2[0][0]                      \n__________________________________________________________________________________________________\na_in2 (InputLayer)              [(None, 69, 69)]     0                                            \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 69, 32)       0           x_in1[0][0]                      \n__________________________________________________________________________________________________\na_in1 (InputLayer)              [(None, 69, 69)]     0                                            \n__________________________________________________________________________________________________\ngcn_conv_3 (GCNConv)            (None, 69, 78)       2574        dropout_3[0][0]                  \n                                                                 a_in2[0][0]                      \n__________________________________________________________________________________________________\ngcn_conv (GCNConv)              (None, 69, 78)       2574        dropout[0][0]                    \n                                                                 a_in1[0][0]                      \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 69, 78)       0           gcn_conv_3[0][0]                 \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 69, 78)       0           gcn_conv[0][0]                   \n__________________________________________________________________________________________________\ngcn_conv_4 (GCNConv)            (None, 69, 156)      12324       dropout_4[0][0]                  \n                                                                 a_in2[0][0]                      \n__________________________________________________________________________________________________\ngcn_conv_1 (GCNConv)            (None, 69, 156)      12324       dropout_1[0][0]                  \n                                                                 a_in1[0][0]                      \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            [(None, 934)]        0                                            \n__________________________________________________________________________________________________\ngcn_conv_5 (GCNConv)            (None, 69, 312)      48984       gcn_conv_4[0][0]                 \n                                                                 a_in2[0][0]                      \n__________________________________________________________________________________________________\ngcn_conv_2 (GCNConv)            (None, 69, 312)      48984       gcn_conv_1[0][0]                 \n                                                                 a_in1[0][0]                      \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 512)          478720      input_1[0][0]                    \n__________________________________________________________________________________________________\nglobal_max_pool_1 (GlobalMaxPoo (None, 312)          0           gcn_conv_5[0][0]                 \n__________________________________________________________________________________________________\nglobal_max_pool (GlobalMaxPool) (None, 312)          0           gcn_conv_2[0][0]                 \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 256)          80128       global_max_pool_1[0][0]          \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 256)          80128       global_max_pool[0][0]            \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 265)          135945      dropout_6[0][0]                  \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 256)          0           dense_2[0][0]                    \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 256)          0           dense[0][0]                      \n__________________________________________________________________________________________________\ndropout_7 (Dropout)             (None, 265)          0           dense_5[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 128)          32896       dropout_5[0][0]                  \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 128)          32896       dropout_2[0][0]                  \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 128)          34048       dropout_7[0][0]                  \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 384)          0           dense_3[0][0]                    \n                                                                 dense_1[0][0]                    \n                                                                 dense_6[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 384)          1536        concatenate[0][0]                \n__________________________________________________________________________________________________\nmulti_head_attention (MultiHead (None, None, 384)    591360      batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\nmulti_head_attention_1 (MultiHe (None, None, 384)    591360      batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 384)          0           multi_head_attention[0][0]       \n__________________________________________________________________________________________________\nreshape_1 (Reshape)             (None, 384)          0           multi_head_attention_1[0][0]     \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 768)          0           reshape[0][0]                    \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 768)          0           reshape_1[0][0]                  \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\ncross_stitch (CrossStitch)      ((None, 768), (None, 4           concatenate_1[0][0]              \n                                                                 concatenate_2[0][0]              \n__________________________________________________________________________________________________\ndense_15 (Dense)                (None, 1024)         787456      cross_stitch[0][0]               \n__________________________________________________________________________________________________\ndense_16 (Dense)                (None, 1024)         787456      cross_stitch[0][1]               \n__________________________________________________________________________________________________\ncross_stitch_1 (CrossStitch)    ((None, 1024), (None 4           dense_15[0][0]                   \n                                                                 dense_16[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 1792)         0           cross_stitch_1[0][0]             \n                                                                 concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndense_17 (Dense)                (None, 128)          229504      concatenate_3[0][0]              \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 1792)         0           cross_stitch_1[0][1]             \n                                                                 concatenate_2[0][0]              \n__________________________________________________________________________________________________\np_re_lu (PReLU)                 (None, 128)          128         dense_17[0][0]                   \n__________________________________________________________________________________________________\ndense_18 (Dense)                (None, 128)          229504      concatenate_4[0][0]              \n__________________________________________________________________________________________________\nsynergy (Dense)                 (None, 64)           8256        p_re_lu[0][0]                    \n__________________________________________________________________________________________________\np_re_lu_1 (PReLU)               (None, 128)          128         dense_18[0][0]                   \n__________________________________________________________________________________________________\np_re_lu_2 (PReLU)               (None, 64)           64          synergy[0][0]                    \n__________________________________________________________________________________________________\nsenstivity (Dense)              (None, 64)           8256        p_re_lu_1[0][0]                  \n__________________________________________________________________________________________________\ndense_19 (Dense)                (None, 1)            65          p_re_lu_2[0][0]                  \n__________________________________________________________________________________________________\ndense_20 (Dense)                (None, 1)            65          senstivity[0][0]                 \n==================================================================================================\nTotal params: 4,237,671\nTrainable params: 4,236,903\nNon-trainable params: 768\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n2022-12-12 15:52:00.666026: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 252224256 exceeds 10% of free system memory.\n2022-12-12 15:52:01.157991: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 543858552 exceeds 10% of free system memory.\n2022-12-12 15:52:01.844854: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 252224256 exceeds 10% of free system memory.\n2022-12-12 15:52:02.372072: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 543858552 exceeds 10% of free system memory.\n2022-12-12 15:52:03.187001: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 252224256 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n447/447 [==============================] - 12s 15ms/step - loss: 763.8545 - dense_19_loss: 502.2223 - dense_20_loss: 132.5851\nEpoch 2/500\n447/447 [==============================] - 7s 16ms/step - loss: 679.6676 - dense_19_loss: 460.5901 - dense_20_loss: 93.3992\nEpoch 3/500\n447/447 [==============================] - 7s 15ms/step - loss: 650.1725 - dense_19_loss: 441.0129 - dense_20_loss: 85.5288\nEpoch 4/500\n447/447 [==============================] - 7s 15ms/step - loss: 627.0206 - dense_19_loss: 424.3870 - dense_20_loss: 80.6531\nEpoch 5/500\n447/447 [==============================] - 7s 15ms/step - loss: 612.8769 - dense_19_loss: 415.4354 - dense_20_loss: 76.8972\nEpoch 6/500\n447/447 [==============================] - 7s 15ms/step - loss: 594.4908 - dense_19_loss: 402.1929 - dense_20_loss: 73.0809\nEpoch 7/500\n447/447 [==============================] - 7s 16ms/step - loss: 577.5079 - dense_19_loss: 389.5041 - dense_20_loss: 69.9549\nEpoch 8/500\n447/447 [==============================] - 7s 15ms/step - loss: 565.6707 - dense_19_loss: 380.8203 - dense_20_loss: 67.7460\nEpoch 9/500\n447/447 [==============================] - 7s 16ms/step - loss: 547.8430 - dense_19_loss: 367.3026 - dense_20_loss: 64.4601\nEpoch 10/500\n447/447 [==============================] - 7s 15ms/step - loss: 534.7094 - dense_19_loss: 357.8456 - dense_20_loss: 61.7722\nEpoch 11/500\n447/447 [==============================] - 7s 16ms/step - loss: 524.5661 - dense_19_loss: 350.6432 - dense_20_loss: 59.7540\nEpoch 12/500\n447/447 [==============================] - 7s 16ms/step - loss: 512.8397 - dense_19_loss: 341.6616 - dense_20_loss: 57.8844\nEpoch 13/500\n447/447 [==============================] - 7s 15ms/step - loss: 502.4781 - dense_19_loss: 334.4839 - dense_20_loss: 55.4864\nEpoch 14/500\n447/447 [==============================] - 7s 15ms/step - loss: 496.5062 - dense_19_loss: 330.7424 - dense_20_loss: 54.0192\nEpoch 15/500\n447/447 [==============================] - 7s 16ms/step - loss: 490.5960 - dense_19_loss: 327.3616 - dense_20_loss: 52.1646\nEpoch 16/500\n447/447 [==============================] - 7s 16ms/step - loss: 478.0972 - dense_19_loss: 317.2421 - dense_20_loss: 50.4731\nEpoch 17/500\n447/447 [==============================] - 7s 16ms/step - loss: 469.9672 - dense_19_loss: 311.8529 - dense_20_loss: 48.3208\nEpoch 18/500\n447/447 [==============================] - 7s 16ms/step - loss: 464.5125 - dense_19_loss: 308.1071 - dense_20_loss: 47.2179\nEpoch 19/500\n447/447 [==============================] - 7s 15ms/step - loss: 454.9415 - dense_19_loss: 300.8204 - dense_20_loss: 45.4357\nEpoch 20/500\n447/447 [==============================] - 7s 16ms/step - loss: 444.5986 - dense_19_loss: 292.8050 - dense_20_loss: 43.6654\nEpoch 21/500\n447/447 [==============================] - 7s 15ms/step - loss: 442.4576 - dense_19_loss: 292.7857 - dense_20_loss: 41.9852\nEpoch 22/500\n447/447 [==============================] - 7s 15ms/step - loss: 431.3065 - dense_19_loss: 283.0749 - dense_20_loss: 41.0597\nEpoch 23/500\n447/447 [==============================] - 7s 16ms/step - loss: 433.2961 - dense_19_loss: 286.6663 - dense_20_loss: 39.9527\nEpoch 24/500\n447/447 [==============================] - 7s 15ms/step - loss: 414.4650 - dense_19_loss: 269.8535 - dense_20_loss: 38.4084\nEpoch 25/500\n447/447 [==============================] - 7s 16ms/step - loss: 408.8781 - dense_19_loss: 265.8987 - dense_20_loss: 37.0672\nEpoch 26/500\n447/447 [==============================] - 7s 15ms/step - loss: 402.7252 - dense_19_loss: 261.3365 - dense_20_loss: 35.8784\nEpoch 27/500\n447/447 [==============================] - 7s 15ms/step - loss: 398.4671 - dense_19_loss: 258.0808 - dense_20_loss: 35.2404\nEpoch 28/500\n447/447 [==============================] - 7s 15ms/step - loss: 388.0873 - dense_19_loss: 248.9916 - dense_20_loss: 34.3125\nEpoch 29/500\n447/447 [==============================] - 8s 17ms/step - loss: 382.9344 - dense_19_loss: 245.5351 - dense_20_loss: 33.0191\nEpoch 30/500\n447/447 [==============================] - 7s 15ms/step - loss: 382.8701 - dense_19_loss: 245.5211 - dense_20_loss: 33.2332\nEpoch 31/500\n447/447 [==============================] - 7s 15ms/step - loss: 367.7808 - dense_19_loss: 232.7462 - dense_20_loss: 31.3311\nEpoch 32/500\n447/447 [==============================] - 7s 15ms/step - loss: 366.9734 - dense_19_loss: 232.9426 - dense_20_loss: 30.6934\nEpoch 33/500\n447/447 [==============================] - 7s 15ms/step - loss: 364.0519 - dense_19_loss: 231.1843 - dense_20_loss: 29.8983\nEpoch 34/500\n447/447 [==============================] - 7s 16ms/step - loss: 352.7210 - dense_19_loss: 221.2054 - dense_20_loss: 28.8644\nEpoch 35/500\n447/447 [==============================] - 7s 15ms/step - loss: 346.7808 - dense_19_loss: 216.1268 - dense_20_loss: 28.2684\nEpoch 36/500\n447/447 [==============================] - 7s 15ms/step - loss: 341.6230 - dense_19_loss: 212.0793 - dense_20_loss: 27.4840\nEpoch 37/500\n447/447 [==============================] - 7s 15ms/step - loss: 332.6528 - dense_19_loss: 203.9667 - dense_20_loss: 26.9820\nEpoch 38/500\n447/447 [==============================] - 7s 16ms/step - loss: 330.3684 - dense_19_loss: 202.4465 - dense_20_loss: 26.5123\nEpoch 39/500\n447/447 [==============================] - 7s 15ms/step - loss: 326.2975 - dense_19_loss: 199.1483 - dense_20_loss: 26.0092\nEpoch 40/500\n447/447 [==============================] - 7s 15ms/step - loss: 328.7102 - dense_19_loss: 201.9042 - dense_20_loss: 25.9181\nEpoch 41/500\n447/447 [==============================] - 7s 15ms/step - loss: 327.8713 - dense_19_loss: 202.0130 - dense_20_loss: 25.2045\nEpoch 42/500\n447/447 [==============================] - 7s 15ms/step - loss: 315.2173 - dense_19_loss: 190.7856 - dense_20_loss: 24.0819\nEpoch 43/500\n447/447 [==============================] - 7s 16ms/step - loss: 312.4060 - dense_19_loss: 188.7716 - dense_20_loss: 23.6161\nEpoch 44/500\n447/447 [==============================] - 7s 15ms/step - loss: 313.4552 - dense_19_loss: 189.7331 - dense_20_loss: 23.8539\nEpoch 45/500\n447/447 [==============================] - 7s 15ms/step - loss: 299.0319 - dense_19_loss: 176.5160 - dense_20_loss: 22.9159\nEpoch 46/500\n447/447 [==============================] - 7s 15ms/step - loss: 295.1261 - dense_19_loss: 173.7636 - dense_20_loss: 22.0082\nEpoch 47/500\n447/447 [==============================] - 7s 16ms/step - loss: 291.9895 - dense_19_loss: 171.4464 - dense_20_loss: 21.4520\nEpoch 48/500\n447/447 [==============================] - 7s 15ms/step - loss: 285.6858 - dense_19_loss: 165.8364 - dense_20_loss: 20.9496\nEpoch 49/500\n447/447 [==============================] - 7s 15ms/step - loss: 293.0895 - dense_19_loss: 173.4106 - dense_20_loss: 21.0322\nEpoch 50/500\n447/447 [==============================] - 7s 15ms/step - loss: 286.3929 - dense_19_loss: 167.6229 - dense_20_loss: 20.2861\nEpoch 51/500\n447/447 [==============================] - 7s 15ms/step - loss: 275.4518 - dense_19_loss: 157.4832 - dense_20_loss: 19.8286\nEpoch 52/500\n447/447 [==============================] - 8s 17ms/step - loss: 276.1130 - dense_19_loss: 158.7193 - dense_20_loss: 19.5678\nEpoch 53/500\n447/447 [==============================] - 7s 16ms/step - loss: 270.2894 - dense_19_loss: 153.2667 - dense_20_loss: 19.2671\nEpoch 54/500\n447/447 [==============================] - 7s 15ms/step - loss: 272.5506 - dense_19_loss: 156.1470 - dense_20_loss: 18.8877\nEpoch 55/500\n447/447 [==============================] - 7s 15ms/step - loss: 262.1590 - dense_19_loss: 146.5824 - dense_20_loss: 18.3534\nEpoch 56/500\n447/447 [==============================] - 7s 16ms/step - loss: 259.3360 - dense_19_loss: 144.4553 - dense_20_loss: 17.8679\nEpoch 57/500\n447/447 [==============================] - 7s 15ms/step - loss: 260.6588 - dense_19_loss: 145.8884 - dense_20_loss: 17.9027\nEpoch 58/500\n447/447 [==============================] - 7s 15ms/step - loss: 253.8738 - dense_19_loss: 139.9761 - dense_20_loss: 17.2201\nEpoch 59/500\n447/447 [==============================] - 7s 15ms/step - loss: 251.4130 - dense_19_loss: 137.7048 - dense_20_loss: 17.2914\nEpoch 60/500\n447/447 [==============================] - 7s 15ms/step - loss: 246.3185 - dense_19_loss: 133.0088 - dense_20_loss: 17.1005\nEpoch 61/500\n447/447 [==============================] - 7s 17ms/step - loss: 248.2673 - dense_19_loss: 135.7751 - dense_20_loss: 16.4604\nEpoch 62/500\n447/447 [==============================] - 7s 15ms/step - loss: 245.6188 - dense_19_loss: 133.0010 - dense_20_loss: 16.7296\nEpoch 63/500\n447/447 [==============================] - 7s 15ms/step - loss: 240.9429 - dense_19_loss: 129.0213 - dense_20_loss: 16.1964\nEpoch 64/500\n447/447 [==============================] - 7s 15ms/step - loss: 235.8090 - dense_19_loss: 124.9560 - dense_20_loss: 15.4082\nEpoch 65/500\n447/447 [==============================] - 7s 15ms/step - loss: 235.9863 - dense_19_loss: 125.4293 - dense_20_loss: 15.3406\nEpoch 66/500\n447/447 [==============================] - 7s 16ms/step - loss: 231.4949 - dense_19_loss: 121.0500 - dense_20_loss: 15.3739\nEpoch 67/500\n447/447 [==============================] - 7s 15ms/step - loss: 224.4297 - dense_19_loss: 114.8311 - dense_20_loss: 14.6840\nEpoch 68/500\n447/447 [==============================] - 7s 15ms/step - loss: 225.4540 - dense_19_loss: 116.3991 - dense_20_loss: 14.3811\nEpoch 69/500\n447/447 [==============================] - 7s 16ms/step - loss: 221.7506 - dense_19_loss: 113.0503 - dense_20_loss: 14.1991\nEpoch 70/500\n447/447 [==============================] - 7s 16ms/step - loss: 220.2360 - dense_19_loss: 111.3636 - dense_20_loss: 14.4010\nEpoch 71/500\n447/447 [==============================] - 7s 15ms/step - loss: 216.9944 - dense_19_loss: 109.0040 - dense_20_loss: 13.7337\nEpoch 72/500\n447/447 [==============================] - 7s 15ms/step - loss: 213.4325 - dense_19_loss: 105.8392 - dense_20_loss: 13.6322\nEpoch 73/500\n447/447 [==============================] - 7s 15ms/step - loss: 211.9624 - dense_19_loss: 104.5095 - dense_20_loss: 13.6354\nEpoch 74/500\n447/447 [==============================] - 7s 16ms/step - loss: 207.6049 - dense_19_loss: 100.6707 - dense_20_loss: 13.2832\nEpoch 75/500\n447/447 [==============================] - 7s 16ms/step - loss: 209.9482 - dense_19_loss: 103.5066 - dense_20_loss: 12.9767\nEpoch 76/500\n447/447 [==============================] - 7s 15ms/step - loss: 202.8804 - dense_19_loss: 96.6794 - dense_20_loss: 12.8717\nEpoch 77/500\n447/447 [==============================] - 7s 15ms/step - loss: 201.4723 - dense_19_loss: 95.7106 - dense_20_loss: 12.7143\nEpoch 78/500\n447/447 [==============================] - 7s 15ms/step - loss: 198.6125 - dense_19_loss: 93.2635 - dense_20_loss: 12.3579\nEpoch 79/500\n447/447 [==============================] - 7s 16ms/step - loss: 194.6433 - dense_19_loss: 89.5514 - dense_20_loss: 12.2962\nEpoch 80/500\n447/447 [==============================] - 7s 15ms/step - loss: 189.8082 - dense_19_loss: 85.2384 - dense_20_loss: 12.0154\nEpoch 81/500\n447/447 [==============================] - 7s 15ms/step - loss: 195.3888 - dense_19_loss: 91.1174 - dense_20_loss: 11.8675\nEpoch 82/500\n447/447 [==============================] - 7s 15ms/step - loss: 193.3010 - dense_19_loss: 89.0139 - dense_20_loss: 12.0123\nEpoch 83/500\n447/447 [==============================] - 7s 15ms/step - loss: 188.6067 - dense_19_loss: 85.0986 - dense_20_loss: 11.4342\nEpoch 84/500\n447/447 [==============================] - 7s 16ms/step - loss: 193.9049 - dense_19_loss: 90.0825 - dense_20_loss: 11.8723\nEpoch 85/500\n447/447 [==============================] - 7s 15ms/step - loss: 186.1102 - dense_19_loss: 82.9660 - dense_20_loss: 11.4340\nEpoch 86/500\n447/447 [==============================] - 7s 15ms/step - loss: 182.4013 - dense_19_loss: 79.8624 - dense_20_loss: 11.0252\nEpoch 87/500\n447/447 [==============================] - 7s 16ms/step - loss: 180.6884 - dense_19_loss: 78.5888 - dense_20_loss: 10.7471\nEpoch 88/500\n447/447 [==============================] - 7s 16ms/step - loss: 177.6493 - dense_19_loss: 75.5368 - dense_20_loss: 10.9563\nEpoch 89/500\n447/447 [==============================] - 7s 15ms/step - loss: 179.9855 - dense_19_loss: 78.1808 - dense_20_loss: 10.7353\nEpoch 90/500\n447/447 [==============================] - 7s 15ms/step - loss: 178.0022 - dense_19_loss: 76.4255 - dense_20_loss: 10.7130\nEpoch 91/500\n447/447 [==============================] - 7s 15ms/step - loss: 177.4423 - dense_19_loss: 76.3570 - dense_20_loss: 10.4023\nEpoch 92/500\n447/447 [==============================] - 7s 15ms/step - loss: 177.8272 - dense_19_loss: 76.7266 - dense_20_loss: 10.5741\nEpoch 93/500\n447/447 [==============================] - 7s 16ms/step - loss: 171.5772 - dense_19_loss: 71.0336 - dense_20_loss: 10.2539\nEpoch 94/500\n447/447 [==============================] - 7s 15ms/step - loss: 166.2936 - dense_19_loss: 66.4285 - dense_20_loss: 9.8194\nEpoch 95/500\n447/447 [==============================] - 7s 15ms/step - loss: 173.1711 - dense_19_loss: 73.0777 - dense_20_loss: 10.2044\nEpoch 96/500\n447/447 [==============================] - 7s 15ms/step - loss: 169.1237 - dense_19_loss: 69.4334 - dense_20_loss: 10.0255\nEpoch 97/500\n447/447 [==============================] - 7s 15ms/step - loss: 165.9023 - dense_19_loss: 66.6205 - dense_20_loss: 9.7854\nEpoch 98/500\n447/447 [==============================] - 7s 16ms/step - loss: 164.5466 - dense_19_loss: 65.9011 - dense_20_loss: 9.3370\nEpoch 99/500\n447/447 [==============================] - 7s 15ms/step - loss: 162.2845 - dense_19_loss: 63.7545 - dense_20_loss: 9.4872\nEpoch 100/500\n447/447 [==============================] - 7s 15ms/step - loss: 162.9927 - dense_19_loss: 64.5044 - dense_20_loss: 9.5346\nEpoch 101/500\n447/447 [==============================] - 7s 15ms/step - loss: 160.9321 - dense_19_loss: 62.8651 - dense_20_loss: 9.3312\nEpoch 102/500\n447/447 [==============================] - 7s 16ms/step - loss: 160.8816 - dense_19_loss: 63.1565 - dense_20_loss: 9.1815\nEpoch 103/500\n447/447 [==============================] - 7s 15ms/step - loss: 159.3691 - dense_19_loss: 61.7581 - dense_20_loss: 9.2362\nEpoch 104/500\n447/447 [==============================] - 7s 16ms/step - loss: 158.4094 - dense_19_loss: 61.0165 - dense_20_loss: 9.2004\nEpoch 105/500\n447/447 [==============================] - 7s 15ms/step - loss: 155.5268 - dense_19_loss: 58.7684 - dense_20_loss: 8.8846\nEpoch 106/500\n447/447 [==============================] - 7s 15ms/step - loss: 150.8664 - dense_19_loss: 54.5389 - dense_20_loss: 8.7011\nEpoch 107/500\n447/447 [==============================] - 7s 16ms/step - loss: 150.9519 - dense_19_loss: 54.8590 - dense_20_loss: 8.6974\nEpoch 108/500\n447/447 [==============================] - 7s 15ms/step - loss: 151.8178 - dense_19_loss: 55.9305 - dense_20_loss: 8.6819\nEpoch 109/500\n447/447 [==============================] - 7s 15ms/step - loss: 148.8389 - dense_19_loss: 53.4550 - dense_20_loss: 8.4503\nEpoch 110/500\n447/447 [==============================] - 7s 15ms/step - loss: 148.8703 - dense_19_loss: 53.7880 - dense_20_loss: 8.3385\nEpoch 111/500\n447/447 [==============================] - 7s 16ms/step - loss: 150.4096 - dense_19_loss: 55.4360 - dense_20_loss: 8.4287\nEpoch 112/500\n447/447 [==============================] - 7s 15ms/step - loss: 146.3075 - dense_19_loss: 51.6913 - dense_20_loss: 8.2962\nEpoch 113/500\n447/447 [==============================] - 7s 15ms/step - loss: 142.0429 - dense_19_loss: 47.9856 - dense_20_loss: 8.0557\nEpoch 114/500\n447/447 [==============================] - 7s 15ms/step - loss: 146.0242 - dense_19_loss: 52.0051 - dense_20_loss: 8.1996\nEpoch 115/500\n447/447 [==============================] - 7s 15ms/step - loss: 148.4559 - dense_19_loss: 54.5372 - dense_20_loss: 8.3037\nEpoch 116/500\n447/447 [==============================] - 7s 16ms/step - loss: 144.2393 - dense_19_loss: 50.7823 - dense_20_loss: 8.0049\nEpoch 117/500\n447/447 [==============================] - 7s 15ms/step - loss: 143.3570 - dense_19_loss: 50.3134 - dense_20_loss: 7.8411\nEpoch 118/500\n447/447 [==============================] - 7s 15ms/step - loss: 140.8821 - dense_19_loss: 47.9644 - dense_20_loss: 7.9539\nEpoch 119/500\n447/447 [==============================] - 7s 15ms/step - loss: 140.1016 - dense_19_loss: 47.6525 - dense_20_loss: 7.7730\nEpoch 120/500\n447/447 [==============================] - 7s 16ms/step - loss: 141.8515 - dense_19_loss: 49.4235 - dense_20_loss: 7.8873\nEpoch 121/500\n447/447 [==============================] - 7s 15ms/step - loss: 137.9846 - dense_19_loss: 46.1471 - dense_20_loss: 7.6011\nEpoch 122/500\n447/447 [==============================] - 7s 16ms/step - loss: 136.0058 - dense_19_loss: 44.6056 - dense_20_loss: 7.4719\nEpoch 123/500\n447/447 [==============================] - 7s 15ms/step - loss: 134.4972 - dense_19_loss: 43.5676 - dense_20_loss: 7.2864\nEpoch 124/500\n447/447 [==============================] - 7s 15ms/step - loss: 136.1618 - dense_19_loss: 45.3691 - dense_20_loss: 7.3024\nEpoch 125/500\n447/447 [==============================] - 7s 16ms/step - loss: 134.7794 - dense_19_loss: 44.3312 - dense_20_loss: 7.1727\nEpoch 126/500\n447/447 [==============================] - 7s 15ms/step - loss: 131.6436 - dense_19_loss: 41.5845 - dense_20_loss: 7.1092\nEpoch 127/500\n447/447 [==============================] - 7s 15ms/step - loss: 133.7905 - dense_19_loss: 43.8304 - dense_20_loss: 7.2996\nEpoch 128/500\n447/447 [==============================] - 7s 15ms/step - loss: 131.3031 - dense_19_loss: 41.7186 - dense_20_loss: 7.1150\nEpoch 129/500\n447/447 [==============================] - 7s 15ms/step - loss: 129.2802 - dense_19_loss: 40.1061 - dense_20_loss: 6.9722\nEpoch 130/500\n447/447 [==============================] - 7s 17ms/step - loss: 131.1733 - dense_19_loss: 42.0458 - dense_20_loss: 7.2170\nEpoch 131/500\n447/447 [==============================] - 7s 15ms/step - loss: 136.7829 - dense_19_loss: 47.4088 - dense_20_loss: 7.3804\nEpoch 132/500\n447/447 [==============================] - 7s 15ms/step - loss: 131.7016 - dense_19_loss: 42.2798 - dense_20_loss: 7.1469\nEpoch 133/500\n447/447 [==============================] - 7s 15ms/step - loss: 128.5495 - dense_19_loss: 39.9654 - dense_20_loss: 6.8783\nEpoch 134/500\n447/447 [==============================] - 7s 16ms/step - loss: 126.6277 - dense_19_loss: 38.6450 - dense_20_loss: 6.7443\nEpoch 135/500\n447/447 [==============================] - 7s 15ms/step - loss: 126.9533 - dense_19_loss: 39.3931 - dense_20_loss: 6.6553\nEpoch 136/500\n447/447 [==============================] - 7s 15ms/step - loss: 122.5298 - dense_19_loss: 35.6083 - dense_20_loss: 6.4034\nEpoch 137/500\n447/447 [==============================] - 7s 15ms/step - loss: 123.7498 - dense_19_loss: 36.9973 - dense_20_loss: 6.5597\nEpoch 138/500\n447/447 [==============================] - 7s 15ms/step - loss: 123.3048 - dense_19_loss: 36.8101 - dense_20_loss: 6.5469\nEpoch 139/500\n447/447 [==============================] - 7s 16ms/step - loss: 123.9851 - dense_19_loss: 37.7302 - dense_20_loss: 6.5739\nEpoch 140/500\n447/447 [==============================] - 7s 15ms/step - loss: 123.0481 - dense_19_loss: 37.1630 - dense_20_loss: 6.4770\nEpoch 141/500\n447/447 [==============================] - 7s 15ms/step - loss: 123.7857 - dense_19_loss: 37.9817 - dense_20_loss: 6.6024\nEpoch 142/500\n447/447 [==============================] - 7s 15ms/step - loss: 124.4303 - dense_19_loss: 38.9121 - dense_20_loss: 6.5958\nEpoch 143/500\n447/447 [==============================] - 7s 16ms/step - loss: 122.5683 - dense_19_loss: 37.1933 - dense_20_loss: 6.5385\nEpoch 144/500\n447/447 [==============================] - 7s 15ms/step - loss: 122.0887 - dense_19_loss: 37.2790 - dense_20_loss: 6.3179\nEpoch 145/500\n447/447 [==============================] - 7s 15ms/step - loss: 120.7643 - dense_19_loss: 36.2742 - dense_20_loss: 6.2493\nEpoch 146/500\n447/447 [==============================] - 7s 16ms/step - loss: 115.9556 - dense_19_loss: 32.1508 - dense_20_loss: 5.9245\nEpoch 147/500\n447/447 [==============================] - 7s 15ms/step - loss: 116.7124 - dense_19_loss: 33.1324 - dense_20_loss: 5.9790\nEpoch 148/500\n447/447 [==============================] - 7s 16ms/step - loss: 115.1521 - dense_19_loss: 31.9678 - dense_20_loss: 5.8822\nEpoch 149/500\n447/447 [==============================] - 7s 15ms/step - loss: 117.5199 - dense_19_loss: 34.4661 - dense_20_loss: 6.0764\nEpoch 150/500\n447/447 [==============================] - 7s 15ms/step - loss: 117.8689 - dense_19_loss: 34.9292 - dense_20_loss: 6.1027\nEpoch 151/500\n447/447 [==============================] - 7s 15ms/step - loss: 114.5834 - dense_19_loss: 32.2387 - dense_20_loss: 5.8399\nEpoch 152/500\n447/447 [==============================] - 7s 16ms/step - loss: 113.6855 - dense_19_loss: 31.5927 - dense_20_loss: 5.9144\nEpoch 153/500\n447/447 [==============================] - 7s 15ms/step - loss: 114.1145 - dense_19_loss: 32.2969 - dense_20_loss: 5.8615\nEpoch 154/500\n447/447 [==============================] - 7s 15ms/step - loss: 114.6443 - dense_19_loss: 33.0493 - dense_20_loss: 5.8793\nEpoch 155/500\n447/447 [==============================] - 7s 15ms/step - loss: 116.3988 - dense_19_loss: 34.9994 - dense_20_loss: 5.8579\nEpoch 156/500\n447/447 [==============================] - 7s 15ms/step - loss: 110.3017 - dense_19_loss: 29.4784 - dense_20_loss: 5.6717\nEpoch 157/500\n447/447 [==============================] - 7s 17ms/step - loss: 108.6242 - dense_19_loss: 28.3818 - dense_20_loss: 5.4675\nEpoch 158/500\n447/447 [==============================] - 7s 15ms/step - loss: 123.0336 - dense_19_loss: 41.2830 - dense_20_loss: 6.7526\nEpoch 159/500\n447/447 [==============================] - 7s 15ms/step - loss: 115.3955 - dense_19_loss: 34.6880 - dense_20_loss: 5.9200\nEpoch 160/500\n447/447 [==============================] - 7s 15ms/step - loss: 109.4486 - dense_19_loss: 29.7002 - dense_20_loss: 5.4029\nEpoch 161/500\n447/447 [==============================] - 7s 15ms/step - loss: 107.8945 - dense_19_loss: 28.4726 - dense_20_loss: 5.4463\nEpoch 162/500\n447/447 [==============================] - 7s 16ms/step - loss: 105.8064 - dense_19_loss: 27.0074 - dense_20_loss: 5.1802\nEpoch 163/500\n447/447 [==============================] - 6s 14ms/step - loss: 105.9767 - dense_19_loss: 27.4183 - dense_20_loss: 5.2615\nEpoch 164/500\n447/447 [==============================] - 7s 15ms/step - loss: 106.6082 - dense_19_loss: 28.1295 - dense_20_loss: 5.4448\nEpoch 165/500\n447/447 [==============================] - 7s 15ms/step - loss: 110.2571 - dense_19_loss: 31.7212 - dense_20_loss: 5.6356\nEpoch 166/500\n447/447 [==============================] - 7s 16ms/step - loss: 107.2242 - dense_19_loss: 29.3303 - dense_20_loss: 5.3071\nEpoch 167/500\n447/447 [==============================] - 7s 15ms/step - loss: 106.6042 - dense_19_loss: 28.7073 - dense_20_loss: 5.5391\nEpoch 168/500\n447/447 [==============================] - 7s 15ms/step - loss: 103.7350 - dense_19_loss: 26.4970 - dense_20_loss: 5.2376\nEpoch 169/500\n447/447 [==============================] - 7s 15ms/step - loss: 106.7891 - dense_19_loss: 29.6823 - dense_20_loss: 5.3769\nEpoch 170/500\n447/447 [==============================] - 7s 15ms/step - loss: 106.4849 - dense_19_loss: 29.6225 - dense_20_loss: 5.3285\nEpoch 171/500\n447/447 [==============================] - 7s 16ms/step - loss: 102.1267 - dense_19_loss: 25.9719 - dense_20_loss: 5.0527\nEpoch 172/500\n447/447 [==============================] - 7s 15ms/step - loss: 105.5190 - dense_19_loss: 29.2390 - dense_20_loss: 5.3224\nEpoch 173/500\n447/447 [==============================] - 7s 15ms/step - loss: 102.9509 - dense_19_loss: 27.1325 - dense_20_loss: 5.1177\nEpoch 174/500\n447/447 [==============================] - 7s 15ms/step - loss: 101.7596 - dense_19_loss: 26.4281 - dense_20_loss: 5.0434\nEpoch 175/500\n447/447 [==============================] - 7s 16ms/step - loss: 102.0578 - dense_19_loss: 26.8746 - dense_20_loss: 5.1464\nEpoch 176/500\n447/447 [==============================] - 7s 15ms/step - loss: 101.5680 - dense_19_loss: 26.6829 - dense_20_loss: 5.0954\nEpoch 177/500\n447/447 [==============================] - 7s 16ms/step - loss: 101.3589 - dense_19_loss: 26.7920 - dense_20_loss: 5.0744\nEpoch 178/500\n447/447 [==============================] - 7s 15ms/step - loss: 99.8817 - dense_19_loss: 25.7836 - dense_20_loss: 4.8736\nEpoch 179/500\n447/447 [==============================] - 7s 15ms/step - loss: 98.4694 - dense_19_loss: 24.8125 - dense_20_loss: 4.7818\nEpoch 180/500\n447/447 [==============================] - 7s 16ms/step - loss: 103.7828 - dense_19_loss: 29.6745 - dense_20_loss: 5.3030\nEpoch 181/500\n447/447 [==============================] - 7s 15ms/step - loss: 106.0936 - dense_19_loss: 31.9704 - dense_20_loss: 5.3861\nEpoch 182/500\n447/447 [==============================] - 7s 15ms/step - loss: 98.5339 - dense_19_loss: 25.2017 - dense_20_loss: 4.9159\nEpoch 183/500\n447/447 [==============================] - 7s 15ms/step - loss: 97.0001 - dense_19_loss: 24.2581 - dense_20_loss: 4.7183\nEpoch 184/500\n447/447 [==============================] - 7s 15ms/step - loss: 98.7569 - dense_19_loss: 26.0365 - dense_20_loss: 4.9373\nEpoch 185/500\n447/447 [==============================] - 7s 16ms/step - loss: 98.0706 - dense_19_loss: 25.6566 - dense_20_loss: 4.8869\nEpoch 186/500\n447/447 [==============================] - 7s 15ms/step - loss: 95.3851 - dense_19_loss: 23.6497 - dense_20_loss: 4.5797\nEpoch 187/500\n447/447 [==============================] - 7s 15ms/step - loss: 96.5979 - dense_19_loss: 24.8630 - dense_20_loss: 4.8227\nEpoch 188/500\n447/447 [==============================] - 7s 15ms/step - loss: 94.9173 - dense_19_loss: 23.6372 - dense_20_loss: 4.6633\nEpoch 189/500\n447/447 [==============================] - 7s 15ms/step - loss: 100.1113 - dense_19_loss: 28.5850 - dense_20_loss: 4.9857\nEpoch 190/500\n447/447 [==============================] - 7s 15ms/step - loss: 99.1338 - dense_19_loss: 27.9628 - dense_20_loss: 4.8699\nEpoch 191/500\n447/447 [==============================] - 7s 15ms/step - loss: 94.2986 - dense_19_loss: 23.7544 - dense_20_loss: 4.6091\nEpoch 192/500\n447/447 [==============================] - 7s 16ms/step - loss: 93.6272 - dense_19_loss: 23.3940 - dense_20_loss: 4.5230\nEpoch 193/500\n447/447 [==============================] - 7s 15ms/step - loss: 96.7125 - dense_19_loss: 26.4168 - dense_20_loss: 4.7654\nEpoch 194/500\n447/447 [==============================] - 7s 16ms/step - loss: 97.4598 - dense_19_loss: 27.2513 - dense_20_loss: 4.8803\nEpoch 195/500\n447/447 [==============================] - 7s 15ms/step - loss: 99.2780 - dense_19_loss: 28.8009 - dense_20_loss: 5.1251\nEpoch 196/500\n447/447 [==============================] - 7s 15ms/step - loss: 92.8824 - dense_19_loss: 23.3467 - dense_20_loss: 4.5527\nEpoch 197/500\n447/447 [==============================] - 7s 15ms/step - loss: 96.5224 - dense_19_loss: 26.7078 - dense_20_loss: 4.8864\nEpoch 198/500\n447/447 [==============================] - 7s 16ms/step - loss: 90.3564 - dense_19_loss: 21.5662 - dense_20_loss: 4.2977\nEpoch 199/500\n447/447 [==============================] - 7s 15ms/step - loss: 89.5092 - dense_19_loss: 20.8987 - dense_20_loss: 4.4689\nEpoch 200/500\n447/447 [==============================] - 7s 15ms/step - loss: 89.3205 - dense_19_loss: 21.2426 - dense_20_loss: 4.2521\nEpoch 201/500\n447/447 [==============================] - 7s 15ms/step - loss: 88.8453 - dense_19_loss: 20.9730 - dense_20_loss: 4.3138\nEpoch 202/500\n447/447 [==============================] - 7s 15ms/step - loss: 90.3046 - dense_19_loss: 22.5914 - dense_20_loss: 4.4171\nEpoch 203/500\n447/447 [==============================] - 7s 16ms/step - loss: 91.3861 - dense_19_loss: 23.7288 - dense_20_loss: 4.5005\nEpoch 204/500\n447/447 [==============================] - 7s 15ms/step - loss: 88.4063 - dense_19_loss: 21.2620 - dense_20_loss: 4.3496\nEpoch 205/500\n447/447 [==============================] - 7s 15ms/step - loss: 88.1241 - dense_19_loss: 21.2679 - dense_20_loss: 4.3453\nEpoch 206/500\n447/447 [==============================] - 7s 15ms/step - loss: 91.3846 - dense_19_loss: 24.5327 - dense_20_loss: 4.4981\nEpoch 207/500\n447/447 [==============================] - 7s 16ms/step - loss: 91.9029 - dense_19_loss: 25.1771 - dense_20_loss: 4.5445\nEpoch 208/500\n447/447 [==============================] - 7s 15ms/step - loss: 87.7408 - dense_19_loss: 21.4404 - dense_20_loss: 4.3222\nEpoch 209/500\n447/447 [==============================] - 7s 15ms/step - loss: 85.7255 - dense_19_loss: 20.0790 - dense_20_loss: 4.0361\nEpoch 210/500\n447/447 [==============================] - 7s 15ms/step - loss: 86.4136 - dense_19_loss: 20.8749 - dense_20_loss: 4.1856\nEpoch 211/500\n447/447 [==============================] - 7s 15ms/step - loss: 85.8160 - dense_19_loss: 20.5366 - dense_20_loss: 4.1873\nEpoch 212/500\n447/447 [==============================] - 7s 16ms/step - loss: 88.7793 - dense_19_loss: 23.1827 - dense_20_loss: 4.5061\nEpoch 213/500\n447/447 [==============================] - 7s 15ms/step - loss: 86.9601 - dense_19_loss: 21.8097 - dense_20_loss: 4.3473\nEpoch 214/500\n447/447 [==============================] - 7s 15ms/step - loss: 89.5881 - dense_19_loss: 24.3585 - dense_20_loss: 4.4996\nEpoch 215/500\n447/447 [==============================] - 7s 16ms/step - loss: 85.8043 - dense_19_loss: 21.2389 - dense_20_loss: 4.1330\nEpoch 216/500\n447/447 [==============================] - 7s 15ms/step - loss: 84.8773 - dense_19_loss: 20.6014 - dense_20_loss: 4.1180\nEpoch 217/500\n447/447 [==============================] - 7s 16ms/step - loss: 83.3012 - dense_19_loss: 19.5262 - dense_20_loss: 3.9369\nEpoch 218/500\n447/447 [==============================] - 7s 15ms/step - loss: 84.5250 - dense_19_loss: 20.9616 - dense_20_loss: 3.9891\nEpoch 219/500\n447/447 [==============================] - 7s 15ms/step - loss: 83.6305 - dense_19_loss: 20.3494 - dense_20_loss: 3.9455\nEpoch 220/500\n447/447 [==============================] - 7s 15ms/step - loss: 84.1507 - dense_19_loss: 20.9013 - dense_20_loss: 4.1316\nEpoch 221/500\n447/447 [==============================] - 7s 16ms/step - loss: 84.1012 - dense_19_loss: 21.0723 - dense_20_loss: 4.0700\nEpoch 222/500\n447/447 [==============================] - 7s 15ms/step - loss: 85.9705 - dense_19_loss: 22.8603 - dense_20_loss: 4.2481\nEpoch 223/500\n447/447 [==============================] - 7s 15ms/step - loss: 83.0001 - dense_19_loss: 20.4778 - dense_20_loss: 3.9627\nEpoch 224/500\n447/447 [==============================] - 7s 15ms/step - loss: 81.6329 - dense_19_loss: 19.4744 - dense_20_loss: 3.8912\nEpoch 225/500\n447/447 [==============================] - 7s 15ms/step - loss: 88.7506 - dense_19_loss: 25.7249 - dense_20_loss: 4.6627\nEpoch 226/500\n447/447 [==============================] - 7s 15ms/step - loss: 80.3838 - dense_19_loss: 18.6179 - dense_20_loss: 3.8086\nEpoch 227/500\n447/447 [==============================] - 7s 16ms/step - loss: 80.1855 - dense_19_loss: 18.6504 - dense_20_loss: 3.8958\nEpoch 228/500\n447/447 [==============================] - 7s 15ms/step - loss: 79.7015 - dense_19_loss: 18.4631 - dense_20_loss: 3.8441\nEpoch 229/500\n447/447 [==============================] - 7s 15ms/step - loss: 81.8381 - dense_19_loss: 20.6203 - dense_20_loss: 4.0397\nEpoch 230/500\n447/447 [==============================] - 7s 17ms/step - loss: 82.4468 - dense_19_loss: 21.1582 - dense_20_loss: 4.1626\nEpoch 231/500\n447/447 [==============================] - 7s 15ms/step - loss: 80.1725 - dense_19_loss: 19.3787 - dense_20_loss: 3.9643\nEpoch 232/500\n447/447 [==============================] - 7s 15ms/step - loss: 78.8662 - dense_19_loss: 18.4273 - dense_20_loss: 3.8781\nEpoch 233/500\n447/447 [==============================] - 7s 15ms/step - loss: 78.0693 - dense_19_loss: 18.1241 - dense_20_loss: 3.6368\nEpoch 234/500\n447/447 [==============================] - 7s 15ms/step - loss: 77.8846 - dense_19_loss: 18.1675 - dense_20_loss: 3.7304\nEpoch 235/500\n447/447 [==============================] - 7s 16ms/step - loss: 79.2001 - dense_19_loss: 19.5476 - dense_20_loss: 3.8160\nEpoch 236/500\n447/447 [==============================] - 7s 15ms/step - loss: 78.0147 - dense_19_loss: 18.5665 - dense_20_loss: 3.7999\nEpoch 237/500\n447/447 [==============================] - 7s 15ms/step - loss: 79.4389 - dense_19_loss: 20.2353 - dense_20_loss: 3.8038\nEpoch 238/500\n447/447 [==============================] - 7s 15ms/step - loss: 77.7663 - dense_19_loss: 18.6822 - dense_20_loss: 3.7816\nEpoch 239/500\n447/447 [==============================] - 7s 15ms/step - loss: 77.3735 - dense_19_loss: 18.7150 - dense_20_loss: 3.6836\nEpoch 240/500\n447/447 [==============================] - 7s 16ms/step - loss: 76.8732 - dense_19_loss: 18.3168 - dense_20_loss: 3.7450\nEpoch 241/500\n447/447 [==============================] - 7s 15ms/step - loss: 76.6548 - dense_19_loss: 18.3419 - dense_20_loss: 3.6716\nEpoch 242/500\n447/447 [==============================] - 7s 15ms/step - loss: 76.7530 - dense_19_loss: 18.6188 - dense_20_loss: 3.7276\nEpoch 243/500\n447/447 [==============================] - 7s 15ms/step - loss: 76.5793 - dense_19_loss: 18.6928 - dense_20_loss: 3.6802\nEpoch 244/500\n447/447 [==============================] - 7s 16ms/step - loss: 75.2400 - dense_19_loss: 17.6559 - dense_20_loss: 3.6104\nEpoch 245/500\n447/447 [==============================] - 7s 15ms/step - loss: 75.4536 - dense_19_loss: 18.0745 - dense_20_loss: 3.5989\nEpoch 246/500\n447/447 [==============================] - 7s 15ms/step - loss: 75.6652 - dense_19_loss: 18.4674 - dense_20_loss: 3.6219\nEpoch 247/500\n447/447 [==============================] - 7s 15ms/step - loss: 74.3312 - dense_19_loss: 17.5442 - dense_20_loss: 3.5004\nEpoch 248/500\n447/447 [==============================] - 7s 15ms/step - loss: 74.5929 - dense_19_loss: 17.7872 - dense_20_loss: 3.6486\nEpoch 249/500\n447/447 [==============================] - 7s 16ms/step - loss: 74.7138 - dense_19_loss: 18.0695 - dense_20_loss: 3.6908\nEpoch 250/500\n447/447 [==============================] - 7s 15ms/step - loss: 76.7072 - dense_19_loss: 20.0805 - dense_20_loss: 3.8057\nEpoch 251/500\n447/447 [==============================] - 7s 15ms/step - loss: 76.1880 - dense_19_loss: 19.7862 - dense_20_loss: 3.6721\nEpoch 252/500\n447/447 [==============================] - 7s 15ms/step - loss: 74.1166 - dense_19_loss: 18.0034 - dense_20_loss: 3.5997\nEpoch 253/500\n447/447 [==============================] - 7s 16ms/step - loss: 74.4047 - dense_19_loss: 18.5204 - dense_20_loss: 3.5660\nEpoch 254/500\n447/447 [==============================] - 7s 16ms/step - loss: 73.6659 - dense_19_loss: 17.6094 - dense_20_loss: 3.7235\nEpoch 255/500\n447/447 [==============================] - 7s 15ms/step - loss: 71.1432 - dense_19_loss: 15.8715 - dense_20_loss: 3.3132\nEpoch 256/500\n447/447 [==============================] - 7s 15ms/step - loss: 71.9777 - dense_19_loss: 16.8903 - dense_20_loss: 3.3747\nEpoch 257/500\n447/447 [==============================] - 7s 15ms/step - loss: 72.0873 - dense_19_loss: 17.1170 - dense_20_loss: 3.4581\nEpoch 258/500\n447/447 [==============================] - 7s 15ms/step - loss: 71.6372 - dense_19_loss: 16.8967 - dense_20_loss: 3.4071\nEpoch 259/500\n447/447 [==============================] - 7s 15ms/step - loss: 73.5419 - dense_19_loss: 18.6582 - dense_20_loss: 3.6816\nEpoch 260/500\n447/447 [==============================] - 7s 16ms/step - loss: 74.3829 - dense_19_loss: 19.4373 - dense_20_loss: 3.7379\nEpoch 261/500\n447/447 [==============================] - 7s 15ms/step - loss: 72.4246 - dense_19_loss: 17.8106 - dense_20_loss: 3.5322\nEpoch 262/500\n447/447 [==============================] - 7s 15ms/step - loss: 71.5226 - dense_19_loss: 17.3130 - dense_20_loss: 3.4075\nEpoch 263/500\n447/447 [==============================] - 8s 19ms/step - loss: 77.5405 - dense_19_loss: 22.2578 - dense_20_loss: 4.0726\nEpoch 264/500\n447/447 [==============================] - 7s 16ms/step - loss: 71.3366 - dense_19_loss: 17.0132 - dense_20_loss: 3.4110\nEpoch 265/500\n447/447 [==============================] - 7s 16ms/step - loss: 69.2455 - dense_19_loss: 15.5505 - dense_20_loss: 3.2604\nEpoch 266/500\n447/447 [==============================] - 7s 16ms/step - loss: 70.2746 - dense_19_loss: 16.6916 - dense_20_loss: 3.3741\nEpoch 267/500\n447/447 [==============================] - 7s 16ms/step - loss: 79.2649 - dense_19_loss: 24.7281 - dense_20_loss: 3.9443\nEpoch 268/500\n447/447 [==============================] - 7s 15ms/step - loss: 71.0213 - dense_19_loss: 17.1379 - dense_20_loss: 3.4677\nEpoch 269/500\n447/447 [==============================] - 7s 15ms/step - loss: 68.7226 - dense_19_loss: 15.4874 - dense_20_loss: 3.2412\nEpoch 270/500\n447/447 [==============================] - 7s 15ms/step - loss: 68.8278 - dense_19_loss: 15.7905 - dense_20_loss: 3.2138\nEpoch 271/500\n447/447 [==============================] - 7s 16ms/step - loss: 68.1431 - dense_19_loss: 15.4196 - dense_20_loss: 3.2164\nEpoch 272/500\n447/447 [==============================] - 7s 15ms/step - loss: 68.8586 - dense_19_loss: 16.2210 - dense_20_loss: 3.3148\nEpoch 273/500\n447/447 [==============================] - 7s 15ms/step - loss: 68.4532 - dense_19_loss: 16.0036 - dense_20_loss: 3.3058\nEpoch 274/500\n447/447 [==============================] - 7s 16ms/step - loss: 69.3158 - dense_19_loss: 16.8884 - dense_20_loss: 3.4036\nEpoch 275/500\n447/447 [==============================] - 7s 15ms/step - loss: 71.3525 - dense_19_loss: 18.8404 - dense_20_loss: 3.4711\nEpoch 276/500\n447/447 [==============================] - 7s 16ms/step - loss: 69.6434 - dense_19_loss: 17.2636 - dense_20_loss: 3.4490\nEpoch 277/500\n447/447 [==============================] - 7s 16ms/step - loss: 67.4984 - dense_19_loss: 15.5891 - dense_20_loss: 3.2601\nEpoch 278/500\n447/447 [==============================] - 7s 15ms/step - loss: 69.3418 - dense_19_loss: 17.4741 - dense_20_loss: 3.3585\nEpoch 279/500\n447/447 [==============================] - 7s 15ms/step - loss: 70.0025 - dense_19_loss: 17.7789 - dense_20_loss: 3.5137\nEpoch 280/500\n447/447 [==============================] - 7s 15ms/step - loss: 66.0803 - dense_19_loss: 14.6918 - dense_20_loss: 3.1406\nEpoch 281/500\n447/447 [==============================] - 7s 16ms/step - loss: 66.4428 - dense_19_loss: 15.3292 - dense_20_loss: 3.1383\nEpoch 282/500\n447/447 [==============================] - 7s 15ms/step - loss: 69.7857 - dense_19_loss: 18.2693 - dense_20_loss: 3.5537\nEpoch 283/500\n447/447 [==============================] - 7s 15ms/step - loss: 71.5524 - dense_19_loss: 19.4196 - dense_20_loss: 3.7472\nEpoch 284/500\n447/447 [==============================] - 7s 15ms/step - loss: 66.7409 - dense_19_loss: 15.6740 - dense_20_loss: 3.1189\nEpoch 285/500\n447/447 [==============================] - 7s 16ms/step - loss: 65.4489 - dense_19_loss: 14.8757 - dense_20_loss: 2.9929\nEpoch 286/500\n447/447 [==============================] - 7s 15ms/step - loss: 64.7090 - dense_19_loss: 14.3849 - dense_20_loss: 2.9995\nEpoch 287/500\n447/447 [==============================] - 7s 15ms/step - loss: 68.7773 - dense_19_loss: 17.9238 - dense_20_loss: 3.3903\nEpoch 288/500\n447/447 [==============================] - 7s 15ms/step - loss: 65.4307 - dense_19_loss: 15.1918 - dense_20_loss: 3.0264\nEpoch 289/500\n447/447 [==============================] - 7s 16ms/step - loss: 65.0568 - dense_19_loss: 15.0833 - dense_20_loss: 3.0488\nEpoch 290/500\n447/447 [==============================] - 7s 16ms/step - loss: 63.6550 - dense_19_loss: 13.9910 - dense_20_loss: 2.9824\nEpoch 291/500\n447/447 [==============================] - 7s 15ms/step - loss: 66.7536 - dense_19_loss: 16.9711 - dense_20_loss: 3.1990\nEpoch 292/500\n447/447 [==============================] - 7s 15ms/step - loss: 66.7398 - dense_19_loss: 16.7716 - dense_20_loss: 3.3542\nEpoch 293/500\n447/447 [==============================] - 7s 15ms/step - loss: 67.4849 - dense_19_loss: 17.5324 - dense_20_loss: 3.3261\nEpoch 294/500\n447/447 [==============================] - 7s 16ms/step - loss: 68.5214 - dense_19_loss: 18.5603 - dense_20_loss: 3.4288\nEpoch 295/500\n447/447 [==============================] - 7s 15ms/step - loss: 67.5105 - dense_19_loss: 17.4445 - dense_20_loss: 3.3662\nEpoch 296/500\n447/447 [==============================] - 7s 15ms/step - loss: 64.7380 - dense_19_loss: 15.3519 - dense_20_loss: 3.1017\nEpoch 297/500\n447/447 [==============================] - 7s 16ms/step - loss: 64.7493 - dense_19_loss: 15.6191 - dense_20_loss: 3.0532\nEpoch 298/500\n447/447 [==============================] - 7s 15ms/step - loss: 63.4680 - dense_19_loss: 14.6603 - dense_20_loss: 2.9752\nEpoch 299/500\n447/447 [==============================] - 7s 16ms/step - loss: 62.0573 - dense_19_loss: 13.5728 - dense_20_loss: 2.8988\nEpoch 300/500\n447/447 [==============================] - 7s 15ms/step - loss: 62.3694 - dense_19_loss: 14.0599 - dense_20_loss: 2.9468\nEpoch 301/500\n447/447 [==============================] - 7s 15ms/step - loss: 66.3034 - dense_19_loss: 17.4416 - dense_20_loss: 3.3252\nEpoch 302/500\n447/447 [==============================] - 7s 15ms/step - loss: 64.2604 - dense_19_loss: 15.7999 - dense_20_loss: 3.1026\nEpoch 303/500\n447/447 [==============================] - 7s 16ms/step - loss: 63.3000 - dense_19_loss: 15.0972 - dense_20_loss: 3.0075\nEpoch 304/500\n447/447 [==============================] - 7s 15ms/step - loss: 62.9578 - dense_19_loss: 14.9098 - dense_20_loss: 3.0088\nEpoch 305/500\n447/447 [==============================] - 7s 15ms/step - loss: 63.4263 - dense_19_loss: 15.4450 - dense_20_loss: 3.1022\nEpoch 306/500\n447/447 [==============================] - 7s 15ms/step - loss: 63.4168 - dense_19_loss: 15.6380 - dense_20_loss: 3.0074\nEpoch 307/500\n447/447 [==============================] - 7s 15ms/step - loss: 62.6472 - dense_19_loss: 15.0016 - dense_20_loss: 3.0370\nEpoch 308/500\n447/447 [==============================] - 7s 16ms/step - loss: 61.9664 - dense_19_loss: 14.4982 - dense_20_loss: 3.0055\nEpoch 309/500\n447/447 [==============================] - 7s 16ms/step - loss: 61.8423 - dense_19_loss: 14.6800 - dense_20_loss: 2.9220\nEpoch 310/500\n447/447 [==============================] - 7s 15ms/step - loss: 78.9843 - dense_19_loss: 29.3460 - dense_20_loss: 4.7789\nEpoch 311/500\n447/447 [==============================] - 7s 17ms/step - loss: 71.3718 - dense_19_loss: 22.0362 - dense_20_loss: 3.7961\nEpoch 312/500\n447/447 [==============================] - 7s 16ms/step - loss: 61.9646 - dense_19_loss: 14.1010 - dense_20_loss: 2.9226\nEpoch 313/500\n447/447 [==============================] - 7s 16ms/step - loss: 59.8162 - dense_19_loss: 12.6655 - dense_20_loss: 2.6946\nEpoch 314/500\n447/447 [==============================] - 7s 16ms/step - loss: 59.1666 - dense_19_loss: 12.4106 - dense_20_loss: 2.6916\nEpoch 315/500\n447/447 [==============================] - 7s 16ms/step - loss: 59.7611 - dense_19_loss: 13.1991 - dense_20_loss: 2.7727\nEpoch 316/500\n447/447 [==============================] - 7s 16ms/step - loss: 59.7801 - dense_19_loss: 13.3639 - dense_20_loss: 2.7721\nEpoch 317/500\n447/447 [==============================] - 7s 16ms/step - loss: 59.6673 - dense_19_loss: 13.4571 - dense_20_loss: 2.7793\nEpoch 318/500\n447/447 [==============================] - 7s 15ms/step - loss: 59.9987 - dense_19_loss: 13.9327 - dense_20_loss: 2.8086\nEpoch 319/500\n447/447 [==============================] - 7s 16ms/step - loss: 59.3974 - dense_19_loss: 13.4109 - dense_20_loss: 2.8505\nEpoch 320/500\n447/447 [==============================] - 7s 16ms/step - loss: 58.9888 - dense_19_loss: 13.3507 - dense_20_loss: 2.7310\nEpoch 321/500\n447/447 [==============================] - 8s 17ms/step - loss: 62.8966 - dense_19_loss: 16.6035 - dense_20_loss: 3.2253\nEpoch 322/500\n447/447 [==============================] - 7s 17ms/step - loss: 59.6631 - dense_19_loss: 14.1254 - dense_20_loss: 2.7621\nEpoch 323/500\n447/447 [==============================] - 7s 16ms/step - loss: 60.6038 - dense_19_loss: 14.9647 - dense_20_loss: 2.9704\nEpoch 324/500\n447/447 [==============================] - 7s 15ms/step - loss: 60.0700 - dense_19_loss: 14.7114 - dense_20_loss: 2.8751\nEpoch 325/500\n447/447 [==============================] - 8s 18ms/step - loss: 61.6389 - dense_19_loss: 15.9480 - dense_20_loss: 3.0626\nEpoch 326/500\n447/447 [==============================] - 7s 15ms/step - loss: 58.1026 - dense_19_loss: 13.1303 - dense_20_loss: 2.7046\nEpoch 327/500\n447/447 [==============================] - 7s 16ms/step - loss: 58.2335 - dense_19_loss: 13.3982 - dense_20_loss: 2.7624\nEpoch 328/500\n447/447 [==============================] - 7s 15ms/step - loss: 59.5011 - dense_19_loss: 14.6955 - dense_20_loss: 2.8038\nEpoch 329/500\n447/447 [==============================] - 7s 15ms/step - loss: 58.5302 - dense_19_loss: 13.7842 - dense_20_loss: 2.8492\nEpoch 330/500\n447/447 [==============================] - 7s 17ms/step - loss: 57.3955 - dense_19_loss: 12.9436 - dense_20_loss: 2.7954\nEpoch 331/500\n447/447 [==============================] - 7s 16ms/step - loss: 57.2229 - dense_19_loss: 13.0836 - dense_20_loss: 2.6426\nEpoch 332/500\n447/447 [==============================] - 7s 15ms/step - loss: 58.0365 - dense_19_loss: 13.9072 - dense_20_loss: 2.7572\nEpoch 333/500\n447/447 [==============================] - 7s 16ms/step - loss: 61.1497 - dense_19_loss: 16.5085 - dense_20_loss: 3.1195\nEpoch 334/500\n447/447 [==============================] - 7s 16ms/step - loss: 60.2739 - dense_19_loss: 15.8599 - dense_20_loss: 2.9324\nEpoch 335/500\n447/447 [==============================] - 7s 16ms/step - loss: 61.8063 - dense_19_loss: 17.1025 - dense_20_loss: 3.2216\nEpoch 336/500\n447/447 [==============================] - 7s 16ms/step - loss: 69.0919 - dense_19_loss: 22.7419 - dense_20_loss: 3.9673\nEpoch 337/500\n447/447 [==============================] - 7s 16ms/step - loss: 57.9231 - dense_19_loss: 13.2527 - dense_20_loss: 2.7478\nEpoch 338/500\n447/447 [==============================] - 7s 15ms/step - loss: 56.3988 - dense_19_loss: 12.3519 - dense_20_loss: 2.6095\nEpoch 339/500\n447/447 [==============================] - 8s 17ms/step - loss: 55.7311 - dense_19_loss: 12.1365 - dense_20_loss: 2.5074\nEpoch 340/500\n447/447 [==============================] - 7s 17ms/step - loss: 56.4715 - dense_19_loss: 12.9627 - dense_20_loss: 2.6456\nEpoch 341/500\n447/447 [==============================] - 7s 16ms/step - loss: 55.4242 - dense_19_loss: 12.1618 - dense_20_loss: 2.6210\nEpoch 342/500\n447/447 [==============================] - 7s 16ms/step - loss: 56.6435 - dense_19_loss: 13.4775 - dense_20_loss: 2.6765\nEpoch 343/500\n447/447 [==============================] - 8s 17ms/step - loss: 57.1416 - dense_19_loss: 14.0069 - dense_20_loss: 2.6984\nEpoch 344/500\n447/447 [==============================] - 7s 16ms/step - loss: 57.3025 - dense_19_loss: 14.0055 - dense_20_loss: 2.8580\nEpoch 345/500\n447/447 [==============================] - 7s 16ms/step - loss: 56.3949 - dense_19_loss: 13.4859 - dense_20_loss: 2.6536\nEpoch 346/500\n447/447 [==============================] - 7s 16ms/step - loss: 56.2189 - dense_19_loss: 13.4132 - dense_20_loss: 2.6977\nEpoch 347/500\n447/447 [==============================] - 7s 16ms/step - loss: 56.2744 - dense_19_loss: 13.5627 - dense_20_loss: 2.7016\nEpoch 348/500\n447/447 [==============================] - 7s 16ms/step - loss: 58.4766 - dense_19_loss: 15.5951 - dense_20_loss: 2.8744\nEpoch 349/500\n447/447 [==============================] - 7s 15ms/step - loss: 61.2944 - dense_19_loss: 17.6916 - dense_20_loss: 3.2076\nEpoch 350/500\n447/447 [==============================] - 7s 15ms/step - loss: 59.5109 - dense_19_loss: 16.1961 - dense_20_loss: 3.0163\nEpoch 351/500\n447/447 [==============================] - 7s 15ms/step - loss: 54.6422 - dense_19_loss: 12.1955 - dense_20_loss: 2.5225\nEpoch 352/500\n447/447 [==============================] - 7s 16ms/step - loss: 54.5035 - dense_19_loss: 12.3661 - dense_20_loss: 2.4920\nEpoch 353/500\n447/447 [==============================] - 7s 15ms/step - loss: 54.6542 - dense_19_loss: 12.5888 - dense_20_loss: 2.5520\nEpoch 354/500\n447/447 [==============================] - 7s 15ms/step - loss: 54.5690 - dense_19_loss: 12.6371 - dense_20_loss: 2.5960\nEpoch 355/500\n447/447 [==============================] - 7s 15ms/step - loss: 54.5011 - dense_19_loss: 12.7415 - dense_20_loss: 2.5757\nEpoch 356/500\n447/447 [==============================] - 7s 16ms/step - loss: 54.8966 - dense_19_loss: 13.2590 - dense_20_loss: 2.6071\nEpoch 357/500\n447/447 [==============================] - 7s 15ms/step - loss: 54.9834 - dense_19_loss: 13.2715 - dense_20_loss: 2.6884\nEpoch 358/500\n447/447 [==============================] - 7s 15ms/step - loss: 54.3489 - dense_19_loss: 12.9308 - dense_20_loss: 2.5573\nEpoch 359/500\n447/447 [==============================] - 7s 16ms/step - loss: 54.6598 - dense_19_loss: 13.2878 - dense_20_loss: 2.6037\nEpoch 360/500\n447/447 [==============================] - 7s 15ms/step - loss: 54.6169 - dense_19_loss: 13.2924 - dense_20_loss: 2.6435\nEpoch 361/500\n447/447 [==============================] - 7s 16ms/step - loss: 52.9346 - dense_19_loss: 11.9836 - dense_20_loss: 2.4772\nEpoch 362/500\n447/447 [==============================] - 7s 15ms/step - loss: 53.6359 - dense_19_loss: 12.7513 - dense_20_loss: 2.5397\nEpoch 363/500\n447/447 [==============================] - 7s 15ms/step - loss: 53.6966 - dense_19_loss: 12.8556 - dense_20_loss: 2.5959\nEpoch 364/500\n447/447 [==============================] - 7s 15ms/step - loss: 56.3808 - dense_19_loss: 15.2582 - dense_20_loss: 2.7937\nEpoch 365/500\n447/447 [==============================] - 7s 15ms/step - loss: 55.1278 - dense_19_loss: 13.9547 - dense_20_loss: 2.8137\nEpoch 366/500\n447/447 [==============================] - 8s 17ms/step - loss: 52.7514 - dense_19_loss: 12.2122 - dense_20_loss: 2.4529\nEpoch 367/500\n447/447 [==============================] - 7s 15ms/step - loss: 52.2787 - dense_19_loss: 11.9814 - dense_20_loss: 2.4206\nEpoch 368/500\n447/447 [==============================] - 7s 15ms/step - loss: 52.7140 - dense_19_loss: 12.4414 - dense_20_loss: 2.5040\nEpoch 369/500\n447/447 [==============================] - 7s 15ms/step - loss: 52.1334 - dense_19_loss: 12.1052 - dense_20_loss: 2.4649\nEpoch 370/500\n447/447 [==============================] - 7s 16ms/step - loss: 53.4085 - dense_19_loss: 13.0782 - dense_20_loss: 2.6716\nEpoch 371/500\n447/447 [==============================] - 7s 15ms/step - loss: 52.0257 - dense_19_loss: 12.1672 - dense_20_loss: 2.4487\nEpoch 372/500\n447/447 [==============================] - 7s 15ms/step - loss: 53.9445 - dense_19_loss: 13.8115 - dense_20_loss: 2.6696\nEpoch 373/500\n447/447 [==============================] - 7s 15ms/step - loss: 51.9151 - dense_19_loss: 12.2341 - dense_20_loss: 2.4591\nEpoch 374/500\n447/447 [==============================] - 7s 15ms/step - loss: 52.8122 - dense_19_loss: 13.0653 - dense_20_loss: 2.5867\nEpoch 375/500\n447/447 [==============================] - 7s 17ms/step - loss: 52.8171 - dense_19_loss: 13.0288 - dense_20_loss: 2.6679\nEpoch 376/500\n447/447 [==============================] - 7s 15ms/step - loss: 53.1228 - dense_19_loss: 13.3780 - dense_20_loss: 2.6580\nEpoch 377/500\n447/447 [==============================] - 7s 15ms/step - loss: 52.0789 - dense_19_loss: 12.5421 - dense_20_loss: 2.5668\nEpoch 378/500\n447/447 [==============================] - 7s 16ms/step - loss: 50.7607 - dense_19_loss: 11.6275 - dense_20_loss: 2.3784\nEpoch 379/500\n447/447 [==============================] - 7s 16ms/step - loss: 51.3730 - dense_19_loss: 12.2767 - dense_20_loss: 2.4308\nEpoch 380/500\n447/447 [==============================] - 7s 15ms/step - loss: 52.4972 - dense_19_loss: 13.2895 - dense_20_loss: 2.5785\nEpoch 381/500\n447/447 [==============================] - 7s 15ms/step - loss: 51.5014 - dense_19_loss: 12.5217 - dense_20_loss: 2.4751\nEpoch 382/500\n447/447 [==============================] - 7s 15ms/step - loss: 52.2139 - dense_19_loss: 13.2341 - dense_20_loss: 2.5117\nEpoch 383/500\n447/447 [==============================] - 7s 16ms/step - loss: 51.2996 - dense_19_loss: 12.4780 - dense_20_loss: 2.4491\nEpoch 384/500\n447/447 [==============================] - 7s 16ms/step - loss: 51.0208 - dense_19_loss: 12.2084 - dense_20_loss: 2.5322\nEpoch 385/500\n447/447 [==============================] - 7s 15ms/step - loss: 51.4891 - dense_19_loss: 12.8695 - dense_20_loss: 2.4629\nEpoch 386/500\n447/447 [==============================] - 7s 15ms/step - loss: 49.7472 - dense_19_loss: 11.3483 - dense_20_loss: 2.4035\nEpoch 387/500\n447/447 [==============================] - 7s 15ms/step - loss: 50.6517 - dense_19_loss: 12.3372 - dense_20_loss: 2.4103\nEpoch 388/500\n447/447 [==============================] - 7s 16ms/step - loss: 51.3563 - dense_19_loss: 12.9071 - dense_20_loss: 2.5547\nEpoch 389/500\n447/447 [==============================] - 7s 16ms/step - loss: 50.7696 - dense_19_loss: 12.5327 - dense_20_loss: 2.4605\nEpoch 390/500\n447/447 [==============================] - 7s 15ms/step - loss: 49.4506 - dense_19_loss: 11.4792 - dense_20_loss: 2.3830\nEpoch 391/500\n447/447 [==============================] - 7s 15ms/step - loss: 54.9457 - dense_19_loss: 16.1334 - dense_20_loss: 2.8951\nEpoch 392/500\n447/447 [==============================] - 7s 15ms/step - loss: 50.7447 - dense_19_loss: 12.5137 - dense_20_loss: 2.4435\nEpoch 393/500\n447/447 [==============================] - 7s 16ms/step - loss: 49.0317 - dense_19_loss: 11.2383 - dense_20_loss: 2.2930\nEpoch 394/500\n447/447 [==============================] - 7s 15ms/step - loss: 48.2967 - dense_19_loss: 10.8273 - dense_20_loss: 2.2043\nEpoch 395/500\n447/447 [==============================] - 7s 15ms/step - loss: 49.1919 - dense_19_loss: 11.6873 - dense_20_loss: 2.3192\nEpoch 396/500\n447/447 [==============================] - 7s 15ms/step - loss: 50.1482 - dense_19_loss: 12.5668 - dense_20_loss: 2.5009\nEpoch 397/500\n447/447 [==============================] - 7s 17ms/step - loss: 66.5443 - dense_19_loss: 26.0158 - dense_20_loss: 4.2199\nEpoch 398/500\n447/447 [==============================] - 7s 15ms/step - loss: 51.5825 - dense_19_loss: 13.1752 - dense_20_loss: 2.4849\nEpoch 399/500\n447/447 [==============================] - 7s 15ms/step - loss: 48.7277 - dense_19_loss: 10.9358 - dense_20_loss: 2.2145\nEpoch 400/500\n447/447 [==============================] - 7s 15ms/step - loss: 47.9844 - dense_19_loss: 10.5663 - dense_20_loss: 2.1613\nEpoch 401/500\n447/447 [==============================] - 7s 15ms/step - loss: 48.2514 - dense_19_loss: 10.9667 - dense_20_loss: 2.2115\nEpoch 402/500\n447/447 [==============================] - 7s 17ms/step - loss: 55.2104 - dense_19_loss: 16.8286 - dense_20_loss: 2.9086\nEpoch 403/500\n447/447 [==============================] - 7s 15ms/step - loss: 50.0077 - dense_19_loss: 12.2473 - dense_20_loss: 2.3624\nEpoch 404/500\n447/447 [==============================] - 7s 15ms/step - loss: 48.1187 - dense_19_loss: 10.8928 - dense_20_loss: 2.2115\nEpoch 405/500\n447/447 [==============================] - 7s 15ms/step - loss: 47.2693 - dense_19_loss: 10.4137 - dense_20_loss: 2.1299\nEpoch 406/500\n447/447 [==============================] - 7s 15ms/step - loss: 48.1052 - dense_19_loss: 11.2876 - dense_20_loss: 2.2457\nEpoch 407/500\n447/447 [==============================] - 7s 15ms/step - loss: 48.1400 - dense_19_loss: 11.3867 - dense_20_loss: 2.2515\nEpoch 408/500\n447/447 [==============================] - 7s 15ms/step - loss: 49.7353 - dense_19_loss: 12.9254 - dense_20_loss: 2.3797\nEpoch 409/500\n447/447 [==============================] - 7s 15ms/step - loss: 51.2905 - dense_19_loss: 13.7409 - dense_20_loss: 2.6520\nEpoch 410/500\n447/447 [==============================] - 7s 15ms/step - loss: 48.2802 - dense_19_loss: 11.4638 - dense_20_loss: 2.2940\nEpoch 411/500\n447/447 [==============================] - 7s 16ms/step - loss: 47.6390 - dense_19_loss: 11.1951 - dense_20_loss: 2.1846\nEpoch 412/500\n447/447 [==============================] - 7s 15ms/step - loss: 47.7111 - dense_19_loss: 11.3500 - dense_20_loss: 2.2707\nEpoch 413/500\n447/447 [==============================] - 7s 15ms/step - loss: 47.8870 - dense_19_loss: 11.6506 - dense_20_loss: 2.2432\nEpoch 414/500\n447/447 [==============================] - 7s 15ms/step - loss: 46.8656 - dense_19_loss: 10.8345 - dense_20_loss: 2.1941\nEpoch 415/500\n447/447 [==============================] - 7s 15ms/step - loss: 55.3380 - dense_19_loss: 18.0380 - dense_20_loss: 2.9382\nEpoch 416/500\n447/447 [==============================] - 7s 16ms/step - loss: 49.8393 - dense_19_loss: 13.0279 - dense_20_loss: 2.4961\nEpoch 417/500\n447/447 [==============================] - 7s 15ms/step - loss: 48.2902 - dense_19_loss: 11.8441 - dense_20_loss: 2.3091\nEpoch 418/500\n447/447 [==============================] - 7s 15ms/step - loss: 46.8478 - dense_19_loss: 10.6462 - dense_20_loss: 2.2463\nEpoch 419/500\n447/447 [==============================] - 7s 15ms/step - loss: 48.2196 - dense_19_loss: 12.1577 - dense_20_loss: 2.3152\nEpoch 420/500\n447/447 [==============================] - 7s 16ms/step - loss: 46.8943 - dense_19_loss: 11.0571 - dense_20_loss: 2.2269\nEpoch 421/500\n447/447 [==============================] - 7s 15ms/step - loss: 46.4950 - dense_19_loss: 10.8353 - dense_20_loss: 2.2190\nEpoch 422/500\n447/447 [==============================] - 7s 15ms/step - loss: 47.3439 - dense_19_loss: 11.6614 - dense_20_loss: 2.2796\nEpoch 423/500\n447/447 [==============================] - 7s 15ms/step - loss: 47.7950 - dense_19_loss: 11.9474 - dense_20_loss: 2.3963\nEpoch 424/500\n447/447 [==============================] - 7s 15ms/step - loss: 46.9628 - dense_19_loss: 11.4832 - dense_20_loss: 2.2035\nEpoch 425/500\n447/447 [==============================] - 7s 16ms/step - loss: 48.2104 - dense_19_loss: 12.5008 - dense_20_loss: 2.3763\nEpoch 426/500\n447/447 [==============================] - 7s 16ms/step - loss: 46.0682 - dense_19_loss: 10.8246 - dense_20_loss: 2.1629\nEpoch 427/500\n447/447 [==============================] - 7s 15ms/step - loss: 47.0897 - dense_19_loss: 11.7516 - dense_20_loss: 2.2648\nEpoch 428/500\n447/447 [==============================] - 7s 15ms/step - loss: 45.8107 - dense_19_loss: 10.8478 - dense_20_loss: 2.1250\nEpoch 429/500\n447/447 [==============================] - 7s 16ms/step - loss: 46.3687 - dense_19_loss: 11.3053 - dense_20_loss: 2.2654\nEpoch 430/500\n447/447 [==============================] - 7s 15ms/step - loss: 61.1251 - dense_19_loss: 24.0721 - dense_20_loss: 3.5962\nEpoch 431/500\n447/447 [==============================] - 7s 15ms/step - loss: 54.9655 - dense_19_loss: 17.8754 - dense_20_loss: 3.0520\nEpoch 432/500\n447/447 [==============================] - 7s 15ms/step - loss: 48.5822 - dense_19_loss: 12.5574 - dense_20_loss: 2.2915\nEpoch 433/500\n447/447 [==============================] - 7s 15ms/step - loss: 45.0475 - dense_19_loss: 9.7309 - dense_20_loss: 2.0131\nEpoch 434/500\n447/447 [==============================] - 7s 16ms/step - loss: 43.7482 - dense_19_loss: 8.8230 - dense_20_loss: 1.9741\nEpoch 435/500\n447/447 [==============================] - 7s 15ms/step - loss: 43.6456 - dense_19_loss: 9.0073 - dense_20_loss: 1.9795\nEpoch 436/500\n447/447 [==============================] - 7s 15ms/step - loss: 46.4000 - dense_19_loss: 11.5615 - dense_20_loss: 2.2074\nEpoch 437/500\n447/447 [==============================] - 7s 15ms/step - loss: 45.0902 - dense_19_loss: 10.4581 - dense_20_loss: 2.1674\nEpoch 438/500\n447/447 [==============================] - 7s 16ms/step - loss: 49.0813 - dense_19_loss: 13.8315 - dense_20_loss: 2.5812\nEpoch 439/500\n447/447 [==============================] - 7s 16ms/step - loss: 45.7263 - dense_19_loss: 10.9737 - dense_20_loss: 2.2409\nEpoch 440/500\n447/447 [==============================] - 7s 15ms/step - loss: 44.5404 - dense_19_loss: 10.1703 - dense_20_loss: 2.1193\nEpoch 441/500\n447/447 [==============================] - 7s 15ms/step - loss: 44.2038 - dense_19_loss: 10.0261 - dense_20_loss: 2.0985\nEpoch 442/500\n447/447 [==============================] - 7s 15ms/step - loss: 45.6336 - dense_19_loss: 11.4195 - dense_20_loss: 2.1555\nEpoch 443/500\n447/447 [==============================] - 7s 16ms/step - loss: 44.0020 - dense_19_loss: 10.0613 - dense_20_loss: 2.0772\nEpoch 444/500\n447/447 [==============================] - 7s 15ms/step - loss: 45.7800 - dense_19_loss: 11.7502 - dense_20_loss: 2.1371\nEpoch 445/500\n447/447 [==============================] - 7s 15ms/step - loss: 45.6345 - dense_19_loss: 11.6745 - dense_20_loss: 2.1524\nEpoch 446/500\n447/447 [==============================] - 7s 15ms/step - loss: 44.7460 - dense_19_loss: 11.0139 - dense_20_loss: 2.0445\nEpoch 447/500\n447/447 [==============================] - 7s 15ms/step - loss: 44.6360 - dense_19_loss: 10.8649 - dense_20_loss: 2.1814\nEpoch 448/500\n447/447 [==============================] - 7s 15ms/step - loss: 45.0145 - dense_19_loss: 11.2725 - dense_20_loss: 2.2078\nEpoch 449/500\n447/447 [==============================] - 7s 15ms/step - loss: 42.8962 - dense_19_loss: 9.5248 - dense_20_loss: 2.0774\nEpoch 450/500\n447/447 [==============================] - 7s 16ms/step - loss: 44.2094 - dense_19_loss: 10.7480 - dense_20_loss: 2.1948\nEpoch 451/500\n447/447 [==============================] - 7s 15ms/step - loss: 44.7464 - dense_19_loss: 11.4157 - dense_20_loss: 2.1044\nEpoch 452/500\n447/447 [==============================] - 7s 16ms/step - loss: 43.9692 - dense_19_loss: 10.7652 - dense_20_loss: 2.1078\nEpoch 453/500\n447/447 [==============================] - 7s 15ms/step - loss: 44.7815 - dense_19_loss: 11.5847 - dense_20_loss: 2.1540\nEpoch 454/500\n447/447 [==============================] - 7s 15ms/step - loss: 45.8892 - dense_19_loss: 12.3249 - dense_20_loss: 2.3663\nEpoch 455/500\n447/447 [==============================] - 7s 15ms/step - loss: 47.9954 - dense_19_loss: 14.3996 - dense_20_loss: 2.3898\nEpoch 456/500\n447/447 [==============================] - 7s 15ms/step - loss: 48.5774 - dense_19_loss: 14.0908 - dense_20_loss: 2.6672\nEpoch 457/500\n447/447 [==============================] - 7s 16ms/step - loss: 44.1014 - dense_19_loss: 10.7021 - dense_20_loss: 2.0870\nEpoch 458/500\n447/447 [==============================] - 7s 15ms/step - loss: 44.8563 - dense_19_loss: 11.4340 - dense_20_loss: 2.1868\nEpoch 459/500\n447/447 [==============================] - 7s 15ms/step - loss: 45.0442 - dense_19_loss: 11.7596 - dense_20_loss: 2.1828\nEpoch 460/500\n447/447 [==============================] - 7s 15ms/step - loss: 42.9585 - dense_19_loss: 10.0031 - dense_20_loss: 2.0207\nEpoch 461/500\n447/447 [==============================] - 7s 15ms/step - loss: 42.1782 - dense_19_loss: 9.4960 - dense_20_loss: 1.9506\nEpoch 462/500\n447/447 [==============================] - 7s 15ms/step - loss: 42.0319 - dense_19_loss: 9.5172 - dense_20_loss: 1.9826\nEpoch 463/500\n447/447 [==============================] - 7s 15ms/step - loss: 42.5865 - dense_19_loss: 10.1778 - dense_20_loss: 1.9668\nEpoch 464/500\n447/447 [==============================] - 7s 15ms/step - loss: 42.4357 - dense_19_loss: 10.1040 - dense_20_loss: 2.0066\nEpoch 465/500\n447/447 [==============================] - 7s 15ms/step - loss: 43.0753 - dense_19_loss: 10.6465 - dense_20_loss: 2.1259\nEpoch 466/500\n447/447 [==============================] - 7s 16ms/step - loss: 46.7251 - dense_19_loss: 13.3278 - dense_20_loss: 2.5091\nEpoch 467/500\n447/447 [==============================] - 7s 15ms/step - loss: 44.1567 - dense_19_loss: 11.3200 - dense_20_loss: 2.1282\nEpoch 468/500\n447/447 [==============================] - 7s 15ms/step - loss: 42.3028 - dense_19_loss: 9.9447 - dense_20_loss: 1.9748\nEpoch 469/500\n447/447 [==============================] - 7s 15ms/step - loss: 42.0496 - dense_19_loss: 9.9444 - dense_20_loss: 1.9707\nEpoch 470/500\n447/447 [==============================] - 7s 15ms/step - loss: 42.4238 - dense_19_loss: 10.3376 - dense_20_loss: 2.0398\nEpoch 471/500\n447/447 [==============================] - 7s 16ms/step - loss: 42.8834 - dense_19_loss: 10.8965 - dense_20_loss: 2.0302\nEpoch 472/500\n447/447 [==============================] - 7s 15ms/step - loss: 42.4214 - dense_19_loss: 10.5047 - dense_20_loss: 2.0221\nEpoch 473/500\n447/447 [==============================] - 7s 15ms/step - loss: 42.0965 - dense_19_loss: 10.3171 - dense_20_loss: 2.0367\nEpoch 474/500\n447/447 [==============================] - 7s 15ms/step - loss: 52.8757 - dense_19_loss: 18.9912 - dense_20_loss: 3.2419\nEpoch 475/500\n447/447 [==============================] - 7s 17ms/step - loss: 48.3884 - dense_19_loss: 14.9348 - dense_20_loss: 2.4987\nEpoch 476/500\n447/447 [==============================] - 7s 16ms/step - loss: 42.2877 - dense_19_loss: 9.7931 - dense_20_loss: 2.0074\nEpoch 477/500\n447/447 [==============================] - 7s 15ms/step - loss: 41.1158 - dense_19_loss: 9.1204 - dense_20_loss: 1.8727\nEpoch 478/500\n447/447 [==============================] - 7s 15ms/step - loss: 41.0434 - dense_19_loss: 9.3022 - dense_20_loss: 1.8663\nEpoch 479/500\n447/447 [==============================] - 7s 16ms/step - loss: 43.0522 - dense_19_loss: 11.0907 - dense_20_loss: 2.0438\nEpoch 480/500\n447/447 [==============================] - 7s 16ms/step - loss: 41.0535 - dense_19_loss: 9.5122 - dense_20_loss: 1.8719\nEpoch 481/500\n447/447 [==============================] - 7s 16ms/step - loss: 41.0179 - dense_19_loss: 9.5971 - dense_20_loss: 1.9302\nEpoch 482/500\n447/447 [==============================] - 7s 15ms/step - loss: 40.6096 - dense_19_loss: 9.3779 - dense_20_loss: 1.8724\nEpoch 483/500\n447/447 [==============================] - 7s 15ms/step - loss: 40.7502 - dense_19_loss: 9.6254 - dense_20_loss: 1.9047\nEpoch 484/500\n447/447 [==============================] - 8s 17ms/step - loss: 40.5206 - dense_19_loss: 9.5180 - dense_20_loss: 1.8958\nEpoch 485/500\n447/447 [==============================] - 7s 15ms/step - loss: 46.8350 - dense_19_loss: 14.9740 - dense_20_loss: 2.4813\nEpoch 486/500\n447/447 [==============================] - 7s 15ms/step - loss: 43.9474 - dense_19_loss: 12.0388 - dense_20_loss: 2.2835\nEpoch 487/500\n447/447 [==============================] - 7s 15ms/step - loss: 41.9457 - dense_19_loss: 10.6217 - dense_20_loss: 1.9869\nEpoch 488/500\n447/447 [==============================] - 7s 15ms/step - loss: 42.5926 - dense_19_loss: 11.0730 - dense_20_loss: 2.1173\nEpoch 489/500\n447/447 [==============================] - 7s 16ms/step - loss: 40.8766 - dense_19_loss: 9.7519 - dense_20_loss: 1.9336\nEpoch 490/500\n447/447 [==============================] - 7s 15ms/step - loss: 40.0805 - dense_19_loss: 9.2630 - dense_20_loss: 1.8407\nEpoch 491/500\n447/447 [==============================] - 7s 15ms/step - loss: 40.5105 - dense_19_loss: 9.7528 - dense_20_loss: 1.8899\nEpoch 492/500\n447/447 [==============================] - 7s 15ms/step - loss: 41.5598 - dense_19_loss: 10.6488 - dense_20_loss: 2.0551\nEpoch 493/500\n447/447 [==============================] - 7s 16ms/step - loss: 41.2629 - dense_19_loss: 10.3970 - dense_20_loss: 2.0130\nEpoch 494/500\n447/447 [==============================] - 7s 15ms/step - loss: 40.2871 - dense_19_loss: 9.6937 - dense_20_loss: 1.8846\nEpoch 495/500\n447/447 [==============================] - 7s 15ms/step - loss: 39.5261 - dense_19_loss: 9.2226 - dense_20_loss: 1.8324\nEpoch 496/500\n447/447 [==============================] - 7s 15ms/step - loss: 39.8298 - dense_19_loss: 9.6632 - dense_20_loss: 1.8360\nEpoch 497/500\n447/447 [==============================] - 7s 16ms/step - loss: 42.4605 - dense_19_loss: 11.7725 - dense_20_loss: 2.0910\nEpoch 498/500\n447/447 [==============================] - 7s 16ms/step - loss: 39.7706 - dense_19_loss: 9.5785 - dense_20_loss: 1.8717\nEpoch 499/500\n447/447 [==============================] - 7s 15ms/step - loss: 39.5729 - dense_19_loss: 9.5484 - dense_20_loss: 1.8393\nEpoch 500/500\n447/447 [==============================] - 7s 15ms/step - loss: 39.4444 - dense_19_loss: 9.4278 - dense_20_loss: 1.9487\nmsynergy_mean_squared_error 258.15206483860356\nmsenstivity_mean_squared_error 16.035282536824344\nmsynergy_mean_absolute_error 9.433309089164334\nmsenstivity_mean_absolute_error 2.713351138953594\nmsynergy_r2_score 0.5458826110446593\nmsenstivity_r2_score 0.895027946027536\n114/114 [==============================] - 2s 8ms/step - loss: 302.2076 - dense_19_loss: 258.1520 - dense_20_loss: 16.0353\n[302.2075500488281, 258.1520080566406, 16.035280227661133]\nmsynergy_pear (array([0.7392311378149646], dtype=object), 0.0)\nmsynergy_spear SpearmanrResult(correlation=0.75181901472001, pvalue=0.0)\nmsenstivity_pear (array([0.9462944382896494], dtype=object), 0.0)\nmsenstivity_spear SpearmanrResult(correlation=0.946880634136496, pvalue=0.0)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(array([[-20.105814 ],\n        [ -5.829105 ],\n        [  2.6967146],\n        ...,\n        [  2.2402287],\n        [  4.9507804],\n        [  4.9399915]], dtype=float32),\n array([[31.538567 ],\n        [33.60592  ],\n        [37.667206 ],\n        ...,\n        [ 2.1181054],\n        [32.29947  ],\n        [10.417622 ]], dtype=float32))"},"metadata":{}}]},{"cell_type":"code","source":"\nfrom IPython.display import FileLink\nnp.savetxt('pred_syn2.csv', ap111 ,delimiter=',')\nFileLink(r'pred_syn2.csv')\n\nnp.savetxt('pred_sen2.csv', ap221 ,delimiter=',')\nFileLink(r'pred_sen2.csv')\n\nnp.savetxt('test_syn2.csv', test_synergy ,delimiter=',')\nFileLink(r'test_syn2.csv')\n\nnp.savetxt('test_sen2.csv', test_senstivity ,delimiter=',')\nFileLink(r'test_sen2.csv')","metadata":{"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/test_sen2.csv","text/html":"<a href='test_sen2.csv' target='_blank'>test_sen2.csv</a><br>"},"metadata":{}}]}]}