{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install deepchem\n!pip install pysmiles\n!pip install openpyxl\n# !pip install rdkit\n\n# !pip install MolGraphConvFeaturizer\n!pip install PubChemPy\n!pip install PyDrive\n!pip install xlrd==2.0.1\n!pip install Pandas==1.3.5\n# !pip install Pandas==1.1.5\n!pip install  gdown\nimport deepchem as dc\nfrom rdkit import Chem\nprint(\"hjjbjh\")\nfrom pysmiles import read_smiles\nimport networkx as nx\nfrom deepchem.feat import MolGraphConvFeaturizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n#   gauth = GoogleAuth()\n#   gauth.credentials = GoogleCredentials.get_application_default()\n#   drivea = GoogleDrive(gauth)\n#   drive.mount('/content/drive')\ndef normalize1(X, means1=None, std1=None, means2=None, std2=None, feat_filt=None, norm='tanh_norm'):\n    if std1 is None:\n        std1 = np.nanstd(X, axis=0)\n    if feat_filt is None:\n        feat_filt = std1!=0\n    X = X[:,feat_filt]\n    X = np.ascontiguousarray(X)\n    if means1 is None:\n        means1 = np.mean(X, axis=0)\n    X = (X-means1)/std1[feat_filt]\n    if norm == 'norm':\n        return(X, means1, std1, feat_filt)\n    elif norm == 'tanh':\n        return(np.tanh(X), means1, std1, feat_filt)\n    elif norm == 'tanh_norm':\n        X = np.tanh(X)\n        if means2 is None:\n            means2 = np.mean(X, axis=0)\n        if std2 is None:\n            std2 = np.std(X, axis=0)\n        X = (X-means2)/std2\n        X[:,std2==0]=0\n        return(X, means1, std1, means2, std2, feat_filt) \n    \n    \ndef get_data():\n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  smiles=pd.read_excel('pubchem.xls', header=None)\n  smiles=np.array(smiles)\n\n#   !gdown https://drive.google.com/uc?id=15bNKK7tacCJIFzvt5y4WfU6uKbPdYtA6\n  !gdown --id 1z1sWG4E9BqipP0iczLrbR4qxSa4KG95_\n  data_to_repeat=pd.read_excel('pcbi.1006752.s004.xls', header=None)\n  data_to_repeat=np.array(data_to_repeat)\n  \n#   !gdown --id 1TThHsLyORlcHuEBgOad20SQUYH88O9mm\n#   data_to_repeat=pd.read_excel('labels1.xlsx', header=None)\n#   data_to_repeat=np.array(data_to_repeat)\n    \n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  unique_drugs=pd.read_excel('pubchem.xls', header=None)\n  unique_drugs=np.array(unique_drugs)         \n\n#   !gdown https://drive.google.com/uc?id=1IMr5zMLRAXC5iE2MAHJbKmHf2-0DCZNd\n# #   !gdown --id 1bBJUFBA4Tm9YdE5OxA1mbUfBI8B7wqcr\n#   feature_cell=pd.read_excel('final_feature_cell.xlsx', header=None)\n#   feature_cell=np.array(feature_cell)\n\n  !gdown --id 1NoUKCRZ0CkNVFP1-578x_L0otPUMkz6T\n  feature_cell=pd.read_excel('unique934_cell.xlsx',header=None)\n  feature_cell=np.array(feature_cell)\n  a=np.zeros((1,934))\n  feature_cell[22,1:]=a\n  feature_cell[36,1:]=a\n#   !gdown --id 109nyFVOO_P9DdyrhWzbNg0FBB_Y8gD58\n#   feature_cell=pd.read_excel('final_deep_cell.xlsx',header=None)\n#   feature_cell=np.array(feature_cell)\n\n#   !gdown https://drive.google.com/uc?id=1YTe0v5PzwjlgPqo3OTU4FMghOFAHgeeC\n  !gdown --id 1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\n  deleted_index=pd.read_excel('deleted_index.xls', header=None)\n  deleted_index=np.array(deleted_index)  \n\n  return smiles[1:,1],data_to_repeat[1:,:],unique_drugs[1:,0],feature_cell,deleted_index\n\n\ndef atom_features(atom):\n    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n                                          ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As',\n                                           'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se',\n                                           'Ti', 'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n                                           'Pt', 'Hg', 'Pb', 'Unknown']) +\n                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    [atom.GetIsAromatic()])\n\n\ndef one_of_k_encoding(x, allowable_set):\n    if x not in allowable_set:\n        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))\n\n\ndef one_of_k_encoding_unk(x, allowable_set):\n    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\n\n\ndef smile_to_graph(smile):\n    mol = Chem.MolFromSmiles(smile)\n\n    c_size = mol.GetNumAtoms()\n\n    features = []\n    for atom in mol.GetAtoms():\n        feature = atom_features(atom)\n        features.append(feature / sum(feature))\n\n    edges = []\n    for bond in mol.GetBonds():\n        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n    g = nx.Graph(edges).to_directed()\n    edge_index = []\n    for e1, e2 in g.edges:\n        edge_index.append([e1, e2])\n\n    return c_size, features, edge_index\n\n\ndef graph_node(smiles):\n  node=[]\n  adj=[]\n  max=0\n  final_feature=[]\n  final_adjacency=[]\n  for i  in range(len(smiles)):\n     m=smiles[i]\n    #  aa=m#.item((0))\n     featurizer = MolGraphConvFeaturizer(use_edges=True,use_chirality=True)\n     out = featurizer.featurize(m)\n     feature=out[0].node_features\n        \n#      featurizer = DMPNNFeaturizer()\n#      out = featurizer.featurize(m)\n#      feature=out[0].node_features\n    \n     ma=feature.shape[0]\n     n=feature.shape[1]\n     if(ma>max):\n       max=ma\n    # out[0].edge_featuwres.shape\n     molecules=read_smiles(m)\n     adjacency1=nx.to_numpy_array(molecules)\n     node.append(feature)\n     adj.append(adjacency1)\n#   f_n=np.zeros((max,n))\n#   f_a=np.zeros((max,max))\n  feature=[]\n  adjacency=[]\n  for i  in range(len(smiles)):\n    n1=node[i]\n    a=adj[i]\n    f_n=np.zeros((max,n))\n    f_a=np.zeros((max,max))\n    f_n[0:n1.shape[0],]=n1\n    f_a[0:n1.shape[0],0:n1.shape[0]]=a\n    final_feature.append(f_n)\n    final_adjacency.append(f_a)\n\n\n  return final_feature,final_adjacency \n\n\ndef graph_node_edge(smiles):\n  node=[]\n  adj=[]\n  max=0\n  max1=0\n  final_feature=[]\n  final_adjacency=[]\n  for i  in range(len(smiles)):\n     m=smiles[i]\n    #  aa=m#.item((0))\n    #  featurizer = MolGraphConvFeaturizer(use_edges=True,use_chirality=True)\n    #  out = featurizer.featurize(m)\n    #  feature=out[0].node_features\n     num_atom,feature,edge_index=smile_to_graph(m)  \n#      featurizer = DMPNNFeaturizer()\n#      out = featurizer.featurize(m)\n#      feature=out[0].node_features\n     feature=np.stack( feature, axis=0)\n     edge_index=np.stack( edge_index, axis=0)\n    \n     ma=num_atom\n     n=feature.shape[1]\n     if(ma>max):\n       max=ma\n    # out[0].edge_featuwres.shape\n     node.append(feature)\n\n     \n     ne=edge_index.shape[0]\n     if(ne>max1):\n       max1=ne\n    # out[0].edge_featuwres.shape\n     adj.append(edge_index)\n#   f_n=np.zeros((max,n))\n#   f_a=np.zeros((max,max))\n  feature=[]\n  adjacency=[]\n  for i  in range(len(smiles)):\n    n1=node[i]\n    a=adj[i]\n    f_n=np.zeros((max,n))\n    f_n[0:n1.shape[0],]=n1\n    f_a=np.zeros((max1,2))\n    f_a[0:a.shape[0],]=a\n    final_feature.append(f_n)\n    final_adjacency.append(f_a)\n\n  return final_feature,final_adjacency \n\n\ndef repeat_smiles(data_to_repeat,unique_drugs,feature,adjacency,deleted_index,synergy,senstivity):\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  synergy1=[]\n  senstivity1=[]\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    if(i not in deleted_index):\n      f_drug1.append(feature[k1[0]])\n      f_drug2.append(feature[k2[0]])\n      a_drug1.append(adjacency[k1[0]])\n      a_drug2.append(adjacency[k2[0]])\n      synergy1.append(synergy[i,])\n      senstivity1.append(senstivity[i,])\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,synergy1,senstivity1\n\ndef repeat_smiles1(data_to_repeat,unique_drugs,feature,adjacency,unique_cell):\n  unique_feature=unique_cell[:,1:]\n  unique_name=unique_cell[:,0]\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  c=data_to_repeat[:,2]\n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  feature_cell=[]\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    cc=c[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    cc1=[m for m, v in enumerate(unique_name) if cc in v]\n    f_drug1.append(feature[k1[0]])\n    f_drug2.append(feature[k2[0]])\n    a_drug1.append(adjacency[k1[0]])\n    a_drug2.append(adjacency[k2[0]])\n    feature_cell.append(unique_feature[cc1[0]])\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,feature_cell \n\ndef repeat_smiles_deep(data_to_repeat,unique_drugs,feature,adjacency,unique_cell):\n  unique_feature=unique_cell[:,1:]\n  unique_name=unique_cell[:,0]\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  c=data_to_repeat[:,2]\n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  feature_cell=[]\n  f=np.zeros((feature[0].shape[0],feature[0].shape[1]))\n  a=np.zeros((adjacency[0].shape[0],adjacency[0].shape[1]))\n  cell=np.zeros((unique_cell[0].shape[0]))\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    cc=c[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    cc1=[m for m, v in enumerate(unique_name) if cc in v]\n    if(cc1):\n        f_drug1.append(feature[k1[0]])\n        f_drug2.append(feature[k2[0]])\n        a_drug1.append(adjacency[k1[0]])\n        a_drug2.append(adjacency[k2[0]])\n        feature_cell.append(unique_feature[cc1[0]])\n    else:\n        f_drug1.append(f)\n        f_drug2.append(f)\n        a_drug1.append(a)\n        a_drug2.append(a)\n        feature_cell.append(cell)\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,feature_cell\n\ndef split_data(feature_cell,synergy,senstivity):\n  row=feature_cell.shape[0]\n  col=feature_cell.shape[1]\n  input1=np.zeros((row,col+1))\n  # a=np.array('i')\n  for i in range(len(feature_cell)):\n    input1[i,0:col]=feature_cell[i,:]\n    input1[i,col]=i\n\n\n  index_train=[]\n  index_test=[]\n  output = np.c_[synergy,senstivity ]\n#   dataset1=np.c_[input11,output1]\n#   dataset=np.random.shuffle(dataset1)\n#   input1=dataset[:,0:len(dataset[0])-2]\n#   output=dataset[:,-2:]\n  x_train, x_test, y_train, y_test = train_test_split(input1, output,test_size=.20)\n  train_synergy=y_train[:,0]\n  train_senstivity=y_train[:,1]\n  test_synergy=y_test[:,0]\n  test_senstivity=y_test[:,1]\n  for i in range(len(x_train)):#.shape[0]):\n    for j in range(len(input1)):#.shape[0]):\n      if(all(x_train[i]==input1[j])):\n        index_train.append(j)\n        \n\n  for ii in range(len(x_test)):#.shape[0]):\n    for jj in range(len(input1)):#.shape[0]):\n      if(all(x_test[ii]==input1[jj])):\n        index_test.append(jj)\n        \n\n  return  train_synergy, train_senstivity,test_synergy, test_senstivity,index_train,index_test\n\n\ndef split_data1(feature_cell,synergy,senstivity):\n  row=feature_cell.shape[0]\n  col=feature_cell.shape[1]\n  input1=np.zeros((row,col+1))\n  # a=np.array('i')\n  k=0\n  for i in range(len(feature_cell)):\n    if((senstivity[i])):\n        input1[k,0:col]=feature_cell[i,:]\n        input1[k,col]=i\n        k=k+1\n\n\n  index_train=[]\n  index_test=[]\n  output=[]\n  for i in range(len(feature_cell)):\n    if((senstivity[i])):\n      output.append (np.c_[synergy[i],senstivity[i] ])\n#   dataset1=np.c_[input11,output1]\n#   dataset=np.random.shuffle(dataset1)\n#   input1=dataset[:,0:len(dataset[0])-2]\n#   output=dataset[:,-2:]\n  output= np.array(output)\n  x_train, x_test, y_train, y_test = train_test_split(input1, output,test_size=.20)\n  train_synergy=y_train[:,0]\n  train_senstivity=y_train[:,1]\n  test_synergy=y_test[:,0]\n  test_senstivity=y_test[:,1]\n  for i in range(len(x_train)):#.shape[0]):\n    for j in range(len(input1)):#.shape[0]):\n      if(all(x_train[i]==input1[j])):\n        index_train.append(j)\n        \n\n  for ii in range(len(x_test)):#.shape[0]):\n    for jj in range(len(input1)):#.shape[0]):\n      if(all(x_test[ii]==input1[jj])):\n        index_test.append(jj)\n        \n\n  return  index_train,index_test\n\n\ndef index(m):\n    if m==1:\n      !gdown --id 1b82ry7sSPqPIcJSr-i4rmC9XYp7Z1CpH\n      test_ind=pd.read_excel('ind1_test.xls', header=None)\n      test_ind=np.array(test_ind)\n    \n      !gdown --id 1aAaYK1EG5AVlWtwHlsajVJo4Qx-u2ig3\n      train_ind=pd.read_excel('ind1_train.xls', header=None)\n      train_ind=np.array(train_ind)\n\n    \n    if m==2:\n        !gdown --id 1EXKurPZ5ScjiZM0OG-bSv7IPXyKKW6B5\n        test_ind=pd.read_excel('ind2_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1-rayr-tZdwX4DDtSHChkoSdgEK1P3jWN\n        train_ind=pd.read_excel('ind2_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    if m==3:\n        !gdown --id 19O4yKBUCAyPrvdUne2IhwOY73A4y7pzT\n        test_ind=pd.read_excel('ind3_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1YNZBGdL--Ww9ZSGWUKmeC26DdJDywrR4\n        train_ind=pd.read_excel('ind3_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==4:\n        !gdown --id 1sJ3ksFbMOQoBTqNw5Ddv7bGgD9YESRGn\n        test_ind=pd.read_excel('ind4_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1-P14BxLrpnYS-9TbsvyATKtFTFG7QsLO\n        train_ind=pd.read_excel('ind4_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==5:\n        !gdown --id 1iZo5wJgiUOGBAVRy0Hw6vXngFHe9ciFH\n        test_ind=pd.read_excel('ind5_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1xlFp8g1gDGPa_Sf0zgrPWm3hnRCSkAxp\n        train_ind=pd.read_excel('ind5_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    return test_ind,train_ind\n\n\ndef index1(m):\n    if m==1:\n      !gdown --id 10a4cV1tOqomBo7RzWqnbs6j5-RGcBNjS\n      test_ind=pd.read_excel('another_ind1_test.xls', header=None)\n      test_ind=np.array(test_ind)\n    \n      !gdown --id 1Kn8Y2iGCH7DLwq72s_QmouA0Db1WBfVR\n      train_ind=pd.read_excel('another_ind1_train.xls', header=None)\n      train_ind=np.array(train_ind)\n\n    \n    if m==2:\n        !gdown --id 1WR4kDyNO6Xo-P-z3O2q6M_tHr6Soj6NZ\n        test_ind=pd.read_excel('another_ind2_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1aegtCzmVg9znXPTUvpzf2zLkmcjEeQ22\n        train_ind=pd.read_excel('another_ind2_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    if m==3:\n        !gdown --id 1W29vAF6mOoKmacbibxzTNQCfp42VVYYN\n        test_ind=pd.read_excel('another_ind3_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1B8h0ewSo4XJCvSWBdS5oiQiwtAL3H72T\n        train_ind=pd.read_excel('another_ind3_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==4:\n        !gdown --id 1tUXsGGT_-JdJTpT5sYEBB3zNFnh60tJR\n        test_ind=pd.read_excel('another_ind4_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1T9H0O6bzr2d01cPnSyE-vYk_9doKKBHZ\n        train_ind=pd.read_excel('another_ind4_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==5:\n        !gdown --id 1p1kOBJQRhsavnOwgeb_3z-gBqLSS08sE\n        test_ind=pd.read_excel('another_ind5_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1hhw-ITuxUnKwTlLveS1MJER_KM580SXa\n        train_ind=pd.read_excel('another_ind5_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    return test_ind,train_ind\n\n\ndef train_test_input(f_drug1,a_drug1,f_drug2,a_drug2,cell_line,index_train,index_test,synery,senstivity):\n  train_f_drug1=[]\n  train_a_drug1=[]\n  train_f_drug2=[]\n  train_a_drug2=[]\n  train_cell_line=[]\n  train_synergy=[]\n  train_senstivity=[]\n  test_f_drug1=[]\n  test_a_drug1=[]\n  test_f_drug2=[]\n  test_a_drug2=[]\n  test_cell_line=[]\n  test_synergy=[]\n  test_senstivity=[]\n  for i in range(len(index_train)):\n      train_a_drug1.append(a_drug1[index_train[i]])\n      train_a_drug2.append(a_drug2[index_train[i]])\n      train_f_drug1.append(f_drug1[index_train[i]])\n      train_f_drug2.append(f_drug2[index_train[i]])\n      train_cell_line.append(cell_line[index_train[i]])\n      train_synergy.append(synergy[index_train[i]])\n      train_senstivity.append(senstivity[index_train[i]])\n  for ii in range(len(index_test)):\n      test_a_drug1.append(a_drug1[index_test[ii]])\n      test_a_drug2.append(a_drug2[index_test[ii]])\n      test_f_drug1.append(f_drug1[index_test[ii]])\n      test_f_drug2.append(f_drug2[index_test[ii]])\n      test_cell_line.append(cell_line[index_test[ii]])\n      test_synergy.append(synergy[index_test[ii]])\n      test_senstivity.append(senstivity[index_test[ii]])\n\n  return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity\n\ndef train_test_input1(f_drug1,a_drug1,f_drug2,a_drug2,cell_line,index_test,synergy,senstivity):\n  train_f_drug1=[]\n  train_a_drug1=[]\n  train_f_drug2=[]\n  train_a_drug2=[]\n  train_cell_line=[]\n  train_synergy=[]\n  train_senstivity=[]\n  test_f_drug1=[]\n  test_a_drug1=[]\n  test_f_drug2=[]\n  test_a_drug2=[]\n  test_cell_line=[]\n  test_synergy=[]\n  test_senstivity=[]\n    \n  for i in range(len(f_drug1)):\n     ind=i+1\n     if(ind not in index_test):\n          train_a_drug1.append(a_drug1[i])\n          train_a_drug2.append(a_drug2[i])\n          train_f_drug1.append(f_drug1[i])\n          train_f_drug2.append(f_drug2[i])\n          train_cell_line.append(cell_line[i])\n          train_synergy.append(synergy[i])\n          train_senstivity.append(senstivity[i])\n  for ii in range(len(index_test)):\n#       ind1=ii-1\n      x=index_test[ii]-1\n      n=x[0]\n      test_a_drug1.append(a_drug1[n])\n      test_a_drug2.append(a_drug2[n])\n      test_f_drug1.append(f_drug1[n])\n      test_f_drug2.append(f_drug2[n])\n      test_cell_line.append(cell_line[n])\n      test_synergy.append(synergy[n])\n      test_senstivity.append(senstivity[n])\n    \n  return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity\n\n\ndef preprocess(index_train,index_test):\n    index_train1=[]\n    index_test1=[]\n    for i in range(len(index_train)):\n        index_train1.append((index_train[i])[0])\n        \n    for ii in range(len(index_test)):\n        index_test1.append((index_test[ii])[0])\n        \n    return index_train1,index_test1\n\n\n\ndef train_test_input2(f_drug1,a_drug1,f_drug2,a_drug2,cell_line,index_test,synergy,senstivity):\n  \n \n  train_a_drug1=a_drug1[3581:]\n  train_a_drug2=a_drug2[3581:]\n  train_f_drug1=f_drug1[3581:]\n  train_f_drug2=f_drug2[3581:]\n  train_cell_line=cell_line[3581:]\n  train_synergy=synergy[3581:]\n  train_senstivity=senstivity[3581:]\n  test_a_drug1=a_drug1[0:3581]\n  test_a_drug2=a_drug2[0:3581]\n  test_f_drug1=f_drug1[0:3581]\n  test_f_drug2=f_drug2[0:3581]\n  test_cell_line=cell_line[0:3581]\n  test_synergy=synergy[0:3581]\n  test_senstivity=senstivity[0:3581]\n    \n#   train_synergy=np.array(train_synergy)\n#   train_senstivity=np.array(train_senstivity)\n#   test_synergy=np.array(test_synergy)\n#   test_senstivity=np.array(test_senstivity)\n\n  return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity\n\n\ndef get_data_me1(s):\n    \n\n    !gdown --id 1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\n    dele=pd.read_excel('deleted_index.xls',header=None)\n    dele=np.array(dele)\n    dele=dele-1\n    \n    \n#     !gdown http://www.bioinf.jku.at/software/DeepSynergy/data_test_fold0_tanh.p.gz\n#     file = gzip.open(data_file, 'rb')\n#     X_tr, X_val, X_train, X_test, y_tr, y_val, y_train, y_test = pickle.load(file)\n#     file.close()\n    \n\n    !gdown 1SvLGjU---dvwsuCTqRMehtV-Gmf2Hk52\n    labels = pd.read_csv('pcbi.1006752.s004.csv', index_col=0) \n    \n    h=len(np.array(labels))\n    #labels are duplicated for the two different ways of ordering in the data\n#     labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['Fold']!=test_fold)\n    \n#     h=len(idx_train[0])\n    xx=np.where(idx_train == dele)[1]\n    idx_train=np.delete(idx_train,xx)\n    xx1=np.where(idx_train == (dele+h))[1]\n    idx_train=np.delete(idx_train,xx1)\n#     idx_train=idx_train[!dele]\n    #indices of test data for model testing: fold 0\n    idx_test = np.where(labels['Fold']==test_fold)\n#     h1=len(idx_test[0])\n    yy=np.where(idx_test == dele)[1]\n    idx_test=np.delete(idx_test,yy)\n    yy1=np.where(idx_test == (dele+h))[1]\n    idx_test=np.delete(idx_test,yy1)\n    \n#     X_train = X[idx_train]\n#     X_test = X[idx_test]\n    idx_train1=idx_train + (idx_train.shape)\n    idx_test1=idx_test + (idx_test.shape)\n    \n    idx_train1=idx_train + (h)\n    idx_test1=idx_test + (h)\n    idx_train1=np.r_[idx_train,idx_train1]\n    idx_test1=np.r_[idx_test,idx_test1]\n    #choose idx_train1 and idx_test1 if you want to duplicate data\n    return idx_train1,idx_test1\n\ndef get_data_me(s):\n   \n    !gdown --id 1R_E1txnkHrwMQlBKpG7a4BHcZM4kRSkj\n    dele=pd.read_excel('deleted_deep.xlsx',header=None)\n\n    dele=np.array(dele)\n    dele=dele-1\n \n#     !gdown http://www.bioinf.jku.at/software/DeepSynergy/data_test_fold0_tanh.p.gz\n#     file = gzip.open(data_file, 'rb')\n#     X_tr, X_val, X_train, X_test, y_tr, y_val, y_train, y_test = pickle.load(file)\n#     file.close()\n    \n    !gdown 1HNEch5czfqjThnZpFpP-Dcy0Qi-DQdc1\n    labels = pd.read_csv('labels1.csv', index_col=0)\n\n\n    h=len(np.array(labels))\n    #labels are duplicated for the two different ways of ordering in the data\n#     labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['fold']!=test_fold)\n    \n#     h=len(idx_train[0])\n    xx=np.where(idx_train == dele)[1]\n    idx_train=np.delete(idx_train,xx)\n    xx1=np.where(idx_train == (dele+h))[1]\n    idx_train=np.delete(idx_train,xx1)\n#     idx_train=idx_train[!dele]\n    #indices of test data for model testing: fold 0\n    idx_test = np.where(labels['fold']==test_fold)\n#     h1=len(idx_test[0])\n    yy=np.where(idx_test == dele)[1]\n    idx_test=np.delete(idx_test,yy)\n    yy1=np.where(idx_test == (dele+h))[1]\n    idx_test=np.delete(idx_test,yy1)\n    \n#     X_train = X[idx_train]\n#     X_test = X[idx_test]\n\n    y_train = labels.iloc[idx_train]['synergy'].values\n    y_test = labels.iloc[idx_test]['synergy'].values\n    y1_train = labels.iloc[idx_train]['senstivity'].values\n    y1_test = labels.iloc[idx_test]['senstivity'].values\n \n    return y_train,y_test,idx_train,idx_test,y1_train,y1_test\n\n\n\nsmiles,data_to_repeat,unique_drugs,unique_cell,deleted_index=get_data()\n# smiles,data_to_repeat,unique_drugs,unique_cell,deleted_index=get_data()\ndeleted_index=deleted_index-1\n\nfeature,adjacency=graph_node(smiles)\n# feature,edge_index=graph_node_edge(smiles)\ndata_to_repeat=np.r_[data_to_repeat,data_to_repeat]\nl=int((data_to_repeat.shape[0])/2)\ndata_to_repeat[l:,0]=data_to_repeat[0:l,1]\ndata_to_repeat[l:,1]=data_to_repeat[0:l,0]\n# np.random.shuffle(data_to_repeat)\nsynergy=data_to_repeat[:,13]\nsenstivity=data_to_repeat[:,5]\n# synergy=data_to_repeat[:,3]\n# senstivity=data_to_repeat[:,5]\n\n# f_drug1,f_drug2,a_drug1,a_drug2,synergy,senstivity=repeat_smiles(data_to_repeat,unique_drugs,feature,adjacency,deleted_index,synergy,senstivity)\n\nf_drug1,f_drug2,a_drug1,a_drug2,feature_cell=repeat_smiles1(data_to_repeat,unique_drugs,feature,adjacency,unique_cell)\n# f_drug1,f_drug2,a_drug1,a_drug2,feature_cell=repeat_smiles_deep(data_to_repeat,unique_drugs,feature,adjacency,unique_cell)\n# y_train,y_test,index_train,index_test,y1_train,y1_test=get_data_me(3)\nindex_train,index_test=get_data_me1(4)\n# train_synergy1, train_senstivity1,test_synergy1, test_senstivity1,index_train,index_test=split_data(feature_cell,synergy,senstivity)\n# feature_cell=np.array(feature_cell).astype(float)\n# index_train,index_test=split_data1(feature_cell,synergy,senstivity)\n# index_test,index_train=index1(1)\n# index_train,index_test=preprocess(index_train,index_test)\n# train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy, train_senstivity,test_synergy, test_senstivity=train_test_input1(f_drug1,a_drug1,f_drug2,a_drug2,feature_cell,index_test,synergy,senstivity)\ntrain_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity=train_test_input(f_drug1,a_drug1,f_drug2,a_drug2,feature_cell,index_train,index_test,synergy,senstivity)\ntrain_cell_line=np.array(train_cell_line).astype(float)\ntest_cell_line=np.array(test_cell_line).astype(float)  \nnorm = \"tanh_norm\"\nif norm == \"tanh_norm\":\n    train_cell_line, mean, std, mean2, std2, feat_filt = normalize1(train_cell_line, norm=norm)\n    test_cell_line, mean, std, mean2, std2, feat_filt = normalize1(test_cell_line, mean, std, mean2, std2, \n                                                          feat_filt=feat_filt, norm=norm)\nelse:\n    train_cell_line, mean, std, feat_filt = normalize1(train_cell_line, norm=norm)\n    test_cell_line, mean, std, feat_filt = normalize1(test_cell_line, mean, std, feat_filt=feat_filt, norm=norm)\n  \nprint(synergy.shape)\n# print(np.random.shuffle(dataset1))\n# cc\n# cc1=[m for m, v in enumerate(unique_name) if cc in v]\n# # int((l))ik2\n# unique_cell[0].shape[1]\n# unique_name=unique_cell[:,0]\nprint(train_cell_line.shape)  \nprint(test_cell_line.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T20:55:27.926022Z","iopub.execute_input":"2022-12-12T20:55:27.926877Z","iopub.status.idle":"2022-12-12T20:57:41.604020Z","shell.execute_reply.started":"2022-12-12T20:55:27.926786Z","shell.execute_reply":"2022-12-12T20:57:41.601962Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting deepchem\n  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.0.1)\nRequirement already satisfied: scipy<1.9 in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.7.3)\nRequirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.21.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.3.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.0.2)\nCollecting rdkit\n  Downloading rdkit-2022.9.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/29.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->deepchem) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->deepchem) (2022.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from rdkit->deepchem) (9.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->deepchem) (3.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\nInstalling collected packages: rdkit, deepchem\nSuccessfully installed deepchem-2.7.1 rdkit-2022.9.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pysmiles\n  Downloading pysmiles-1.0.2-py2.py3-none-any.whl (22 kB)\nRequirement already satisfied: pbr in /opt/conda/lib/python3.7/site-packages (from pysmiles) (5.10.0)\nRequirement already satisfied: networkx~=2.0 in /opt/conda/lib/python3.7/site-packages (from pysmiles) (2.5)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx~=2.0->pysmiles) (5.1.1)\nInstalling collected packages: pysmiles\nSuccessfully installed pysmiles-1.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: openpyxl in /opt/conda/lib/python3.7/site-packages (3.0.10)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.7/site-packages (from openpyxl) (1.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting PubChemPy\n  Downloading PubChemPy-1.0.4.tar.gz (29 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: PubChemPy\n  Building wheel for PubChemPy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PubChemPy: filename=PubChemPy-1.0.4-py3-none-any.whl size=13834 sha256=2a218e801f82654ccebcf9c1568e8afd20e54570411f775b174deee110e4c659\n  Stored in directory: /root/.cache/pip/wheels/7c/3d/8c/8192697412e9899dc55bbbb08bbc1197bef333caaa2a71c448\nSuccessfully built PubChemPy\nInstalling collected packages: PubChemPy\nSuccessfully installed PubChemPy-1.0.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting PyDrive\n  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (1.12.11)\nRequirement already satisfied: oauth2client>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (4.1.3)\nRequirement already satisfied: PyYAML>=3.0 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (6.0)\nRequirement already satisfied: google-auth<3dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.35.0)\nRequirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (0.20.4)\nRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\nRequirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (0.1.0)\nRequirement already satisfied: google-api-core<3dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.33.2)\nRequirement already satisfied: six<2dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.15.0)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (0.2.7)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (4.8)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.56.3)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.28.1)\nCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5\n  Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.2.4)\nRequirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (59.8.0)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.2->PyDrive) (3.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2022.9.24)\nBuilding wheels for collected packages: PyDrive\n  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PyDrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27453 sha256=541118619387b4e59b98a604af8678d6704e35eaba062cc4e2078c15c7f519aa\n  Stored in directory: /root/.cache/pip/wheels/57/cc/07/6aac75f5395a224650905accd38c868c2276782a56f1046b7b\nSuccessfully built PyDrive\nInstalling collected packages: protobuf, PyDrive\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Uninstalling protobuf-3.19.4:\n      Successfully uninstalled protobuf-3.19.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\ntensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\ntensorflow 2.6.4 requires tensorboard<2.7,>=2.6.0, but you have tensorboard 2.10.1 which is incompatible.\ntensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\ntensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\ntensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.12.0 requires protobuf<=3.20.1,>=3.12.2, but you have protobuf 3.20.3 which is incompatible.\nnnabla 1.31.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.47.0 requires grpcio>=1.47.0, but you have grpcio 1.43.0 which is incompatible.\ngcsfs 2022.5.0 requires fsspec==2022.5.0, but you have fsspec 2022.8.2 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed PyDrive-1.3.1 protobuf-3.20.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting xlrd==2.0.1\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m692.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: xlrd\nSuccessfully installed xlrd-2.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: Pandas==1.3.5 in /opt/conda/lib/python3.7/site-packages (1.3.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from Pandas==1.3.5) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from Pandas==1.3.5) (2022.1)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from Pandas==1.3.5) (1.21.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->Pandas==1.3.5) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting gdown\n  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.64.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.28.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.15.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.7.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.9.24)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","output_type":"stream"},{"name":"stdout","text":"hjjbjh\nDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 31.9MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1z1sWG4E9BqipP0iczLrbR4qxSa4KG95_\nTo: /kaggle/working/pcbi.1006752.s004.xls\n100%|███████████████████████████████████████| 5.24M/5.24M [00:00<00:00, 211MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 34.9MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1NoUKCRZ0CkNVFP1-578x_L0otPUMkz6T\nTo: /kaggle/working/unique934_cell.xlsx\n100%|████████████████████████████████████████| 255k/255k [00:00<00:00, 79.2MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\nTo: /kaggle/working/deleted_index.xls\n100%|██████████████████████████████████████| 58.9k/58.9k [00:00<00:00, 56.5MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\nTo: /kaggle/working/deleted_index.xls\n100%|██████████████████████████████████████| 58.9k/58.9k [00:00<00:00, 47.3MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1SvLGjU---dvwsuCTqRMehtV-Gmf2Hk52\nTo: /kaggle/working/pcbi.1006752.s004.csv\n100%|███████████████████████████████████████| 2.57M/2.57M [00:00<00:00, 184MB/s]\n(37810,)\n(28518, 934)\n(7284, 934)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow\nfrom networkx.readwrite.graph6 import data_to_n\nfrom tensorflow.python.training.tracking import data_structures\n!pip install spektral\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import add,PReLU, Flatten,Reshape,Dropout, Input,Dense,Add,concatenate,BatchNormalization, Activation#,MultiHeadAttention,AdditiveAttention\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n# from tensorflow.random import set_seed\nfrom spektral.data.loaders import SingleLoader\nfrom spektral.datasets.citation import Citation\nfrom spektral.layers import GATConv, GlobalAvgPool,GlobalMaxPool, GCNConv,GlobalAttentionPool\nfrom spektral.transforms import LayerPreprocess\n# import tensorflow.compat.v1.keras.backend as K\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score#,AUC\nfrom scipy.stats import pearsonr,spearmanr\nfrom sklearn.metrics import roc_curve,auc\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras import regularizers\ndef generate_network1(x_in1,a_in1,x_in2,a_in2,cell, inDrop, drop):\n    # fill the architecture params from dict\n    gcn_layers = [78,156,312]\n    \n    gan_layers = [128,128]\n    cell_layers = [512,265,128]#for gcn\n#     cell_layers = [2048,512,128*2] #for gan\n    snp_layers = [512,128]#for gcn\n#     snp_layers = [2048,512,256]# for gan\n    dsn1_layers = [1024,2048,1024]\n    dsn2_layers = [1024,2048,1024]\n    N = x_in1[0].shape[0]\n    F = x_in1[0].shape[1]\n    \n    channels = 8  # Number of channels in each head of the first GAT layer\n    n_attn_heads = 10  # Number of attention heads in first GAT layer\n    dropout = 0.2 # Dropout rate for the features and adjacency matrix\n    drop=0.2\n    l2_reg = 1e-3  # L2 regularization rate\n    # contruct two parallel networks\n    x_in1 = Input(shape=(N,F,),name='x_in1')\n    a_in1 = Input((N,N,),name='a_in1')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d1 = Dropout(dropout)(x_in1)\n             middle_layer_d1  = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activty_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d1, a_in1])\n#              middle_layer_d1=BatchNormalization()(middle_layer_d1)\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n             gcn_output1=GlobalMaxPool(name='a1')(middle_layer_d1)\n             \n#              gcn_output1=Flatten()(x_in1)\n#              gcn_output1=GlobalAttentionPool(1000)(x_in1)#(middle_layer_d1)\n             \n             \n        else:\n             middle_layer_d1 = Dropout(dropout)(middle_layer_d1)\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n#              middle_layer_d1=BatchNormalization()(middle_layer_d1)\n#     print(x_in1.shape)\n    # print(a_in1.shape)\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n#             middle_layer1  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in1, a_in1])\n            \n#         elif l == (len(gan_layers)-1):\n#             middle_layer1 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n            \n#             gan_output1=GlobalMaxPool()(middle_layer1)\n# #             gan_output1 = Dense(int(gan_layers[l]), activation='tanh')(gan_output1)\n#         else:\n#             middle_layer1 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n            \n            \n    \n#   156  concatModel1 = concatenate([gcn_output1, gan_output1])\n    gcn_output1 = Dense(256, activation='relu')(gcn_output1)\n    gcn_output1 = Dropout(dropout)(gcn_output1)\n    gcn_output1 = Dense(128, activation='linear')(gcn_output1)\n    \n    gan_output1 = Dense(128, activation='relu')(gan_output1)\n   \n    \n    concatModel1 =  gan_output1\n    # # addtModel = Add([gcn_output, gan_output])\n    x_in2 = Input(shape=(N,F,),name='x_in2')\n    a_in2 = Input((N,N,),name='a_in2')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d2 = Dropout(dropout)(x_in2)\n             middle_layer_d2 =  GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d2, a_in2])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n             gcn_output2=GlobalMaxPool(name='a2')(middle_layer_d2)\n#             gcn_output2=Flatten()(x_in2)\n#             gcn_output2=GlobalAttentionPool(1000)(x_in2)#(middle_layer_d2)\n             \n        else:\n             middle_layer_d2 = Dropout(dropout)(middle_layer_d2)\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n#             middle_layer2  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in2, a_in2])\n           \n#         elif l == (len(gan_layers)-1):\n#             middle_layer2 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n#             gan_output2=GlobalMaxPool()(middle_layer2)\n# #             gan_output2 = Dense(int(gan_layers[l]), activation='tanh')(gan_output2)\n#         else:\n#             middle_layer2 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n    # print(gcn_output2.shape)\n    # print(gan_output2.shape)        \n#   156  concatModel2 = concatenate([gcn_output2, gan_output2])\n    gcn_output2 = Dense(256, activation='relu')(gcn_output2)\n    gcn_output2 = Dropout(dropout)(gcn_output2)\n    gcn_output2 = Dense(128, activation='linear')(gcn_output2)\n    \n    gan_output2 = Dense(128, activation='relu')(gan_output2)\n    \n    concatModel2 = gcn_output2\n    # concatModel2 = Add()([gcn_output2, gan_output2])\n    for cell_layer in range(len(cell_layers)):\n      if cell_layer == 0:\n        input_cell    = Input(shape=(cell[0].shape[0],))\n#         input_cell1 = BatchNormalization()(input_cell)\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,)(input_cell)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      elif cell_layer == (len(cell_layers)-1):\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,)(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      #  snp_output = Dense(1, activation='linear')(snpFC)\n      else:\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,)(cellFC)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n    # print(concatModel1.shape)\n    # print(cellFC.shape)\n    \n    concatModel=concatenate([concatModel2,concatModel1,cellFC])\n    concatModel=BatchNormalization()(concatModel)\n#     input2=concatenate([concatModel2,cellFC])\n    # input1=concatenate([gan_output1,cellFC])\n    # input2=concatenate([gan_output2,cellFC])\n    # # addtModel = Add([concatModel1, concatModel2])\n    # dsn1_layers = [2048,4096,2048]\n    # dsn2_layers = [2048,4096,2048]\n    # snp_layers = [2048,1024]\n    # # contruct two parallel networks\n    \n       \n#     for snp_layer in range(len(snp_layers)):\n#        if snp_layer == 0:\n#          snpFC1 = Dense(int(snp_layers[snp_layer]), activation='linear',use_bias=True,)(concatModel)\n#          snpFC1=PReLU()(snpFC1)\n#          snpFC = Dropout(float(drop))(snpFC1)\n# #          snpFC=BatchNormalization()(snpFC)\n#        elif snp_layer == (len(snp_layers)-1):\n#          snpFC = Dense(int(snp_layers[snp_layer]), activation='linear',use_bias=True,)(snpFC)\n#          snpFC=PReLU()(snpFC)\n# #          layer = MultiHeadAttention(num_heads=2, key_dim=2)\n# #          rr_task1=Reshape([1,snpFC.shape[1]])(snpFC)\n# #          a_task1=layer(rr_task1,rr_task1)\n#          snp_output1 = Dense(1, activation='linear')(snpFC)\n#        else:\n#           snpFC = Dense(int(snp_layers[snp_layer]), activation='linear',use_bias=True,)(snpFC)\n#           snpFC=PReLU()(snpFC)\n#           snpFC = Dropout(float(drop))(snpFC)\n\n\n    for snp_layer in range(len(snp_layers)):\n       if snp_layer == 0:\n         snpFC1 = Dense(int(snp_layers[snp_layer]), activation='relu',use_bias=True,)(concatModel)\n#          snpFC1=PReLU()(snpFC1)\n         snpFC = Dropout(float(drop))(snpFC1)\n#          snpFC=BatchNormalization()(snpFC)\n       elif snp_layer == (len(snp_layers)-1):\n         snpFC = Dense(int(snp_layers[snp_layer]), activation='relu',use_bias=True,)(snpFC)\n#          snpFC=PReLU()(snpFC)\n#          layer = MultiHeadAttention(num_heads=2, key_dim=2)\n#          rr_task1=Reshape([1,snpFC.shape[1]])(snpFC)\n#          a_task1=layer(rr_task1,rr_task1)\n         snp_output1 = Dense(1, activation='linear')(snpFC)\n       else:\n          snpFC = Dense(int(snp_layers[snp_layer]), activation='relu',use_bias=True,)(snpFC)\n#           snpFC=PReLU()(snpFC)\n          snpFC = Dropout(float(drop))(snpFC)\n#     model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [f_task1,f_task2])\n    model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [snp_output1])\n    # tf.keras.utils.plot_model(model.build_graph(), to_file=\"model.png\",\n    #        expand_nested=True, show_shapes=True)\n    print(model.summary())\n    return model\n\ndef trainer1(model, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity, epo, batch_size, earlyStop):\n#     model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=float(l_rate), momentum=0.9,nesterov=True, clipvalue=0.3))\n    model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=float(l_rate)))#, beta_1=0.9, beta_2=0.999, amsgrad=True))\n    model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line], [train_synergy], shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 )#,restore_best_weights=True,validation_split=0.1) \n                  #  validation_data=([val_input1,val_input2], [val_synergy,val_senstivity]))\n    # model.fit({\"input1\":train_drug1, \"input2\":train_drug2}, {\"synergy\":train_synergy,\"senstivity\":train_senstivity}, epochs=epo, shuffle=True, batch_size=batch_size,verbose=1) \n    #                validation_data=({\"input1\":val_input1,\"input2\":val_input2}, {\"synergy\":val_synergy,\"senstivity\":val_senstivity}))\n\n    return model\ntrain_f_drug1=np.array(train_f_drug1)\ntrain_a_drug1=np.array(train_a_drug1)\ntrain_f_drug2=np.array(train_f_drug2)\ntrain_a_drug2=np.array(train_a_drug2)\ntrain_cell_line=np.array(train_cell_line)\ntest_f_drug1=np.array(test_f_drug1)\ntest_a_drug1=np.array(test_a_drug1)\ntest_f_drug2=np.array(test_f_drug2)\ntest_a_drug2=np.array(test_a_drug2)\ntest_cell_line=np.array(test_cell_line)\ntrain_synergy=np.array(train_synergy)\ntrain_senstivity=np.array(train_senstivity)\ntest_synergy=np.array(test_synergy)\ntest_senstivity=np.array(test_senstivity)\n\n\n# train_synergy\n# test_synergy\n# train_synergy.shape\n# test_synergy.shape\n# train_senstivity","metadata":{"execution":{"iopub.status.busy":"2022-12-12T20:57:41.607863Z","iopub.execute_input":"2022-12-12T20:57:41.608601Z","iopub.status.idle":"2022-12-12T20:58:12.098394Z","shell.execute_reply.started":"2022-12-12T20:57:41.608554Z","shell.execute_reply":"2022-12-12T20:58:12.097258Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.6.4)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.43.0)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\nCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (5.0)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.15.0)\nCollecting numpy~=1.19.2\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nCollecting tensorboard<2.7,>=2.6.0\n  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.35.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (59.8.0)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.6.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.28.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.8.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.2.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.4.6)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (3.3.7)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (4.13.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (1.26.12)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (3.2.0)\nInstalling collected packages: typing-extensions, numpy, h5py, tensorboard\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.1.1\n    Uninstalling typing_extensions-4.1.1:\n      Successfully uninstalled typing_extensions-4.1.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.7.0\n    Uninstalling h5py-3.7.0:\n      Successfully uninstalled h5py-3.7.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.10.1\n    Uninstalling tensorboard-2.10.1:\n      Successfully uninstalled tensorboard-2.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nxarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\ntensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\nrich 12.6.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.7.7 requires tensorboard>=2.9.1, but you have tensorboard 2.6.0 which is incompatible.\npytorch-lightning 1.7.7 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.12 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nonnx 1.12.0 requires protobuf<=3.20.1,>=3.12.2, but you have protobuf 3.20.3 which is incompatible.\nnnabla 1.31.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\nnnabla 1.31.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\njaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\njax 0.3.23 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\nflax 0.6.1 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\nfeaturetools 1.11.1 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndeepchem 2.7.1 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\ncmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\nallennlp 2.10.1 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\nallennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\naioitertools 0.11.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.93 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed h5py-3.1.0 numpy-1.19.5 tensorboard-2.6.0 typing-extensions-3.10.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting spektral\n  Downloading spektral-1.2.0-py3-none-any.whl (140 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m592.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from spektral) (2.6.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from spektral) (1.0.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from spektral) (1.0.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from spektral) (1.7.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from spektral) (2.28.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from spektral) (2.5)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from spektral) (4.64.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from spektral) (1.3.5)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from spektral) (4.9.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from spektral) (1.19.5)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (2.6.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.20.3)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.43.0)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (2.6.0)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.12.1)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.1.2)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.15.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.37.1)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.12)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.4.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.1.0)\nRequirement already satisfied: typing-extensions<3.11,>=3.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.10.0.2)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.15.0)\nRequirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.1.0)\nRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (2.6.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (5.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->spektral) (5.1.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->spektral) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->spektral) (2022.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (3.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->spektral) (3.1.0)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=2.2.0->spektral) (1.5.2)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (1.8.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (2.2.2)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (59.8.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (3.3.7)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.6.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (1.35.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.4.6)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (4.8)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (4.2.4)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (4.13.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (3.2.0)\nInstalling collected packages: spektral\nSuccessfully installed spektral-1.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"### from sklearn.metrics import roc_curve,auc\nl_rate = 0.0001\ninDrop = 0.2\ndrop = 0.2\nmax_epoch = 500\n# batch_size = 128 #gcn\nbatch_size = 64 #gan\nearlyStop_patience = 20#np.ceil(train_f_drug1.shape[0]/batch_size)#1000\n\n# model1= generate_network1(train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line, inDrop, drop)\n# plot_model(model1, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n# model1=trainer1(model1, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity,max_epoch, batch_size,\n#                                 earlyStop_patience)\n# # p1,p2= predict(model, [test_input1,test_input2])\n# p1= model1.predict( [test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line])\n# # s1=np.zeros(len(p1))\n# # s2=np.zeros(len(p1))\n# # for i in range(len(p1)):\n# #     n1=p1[0]\n# #     h1=n1[0]\n# #     x1=h1[0]\n# #     s1[i]=x1\n# #     n2=p2[i]\n# #     h2=n2[0]\n# #     x2=h2[0]\n\n# synergy_error=mean_squared_error(test_synergy, p1)\n# # senstivity_error=mean_squared_error(test_senstivity, p2)\n# # p1,p2\n\n# # senstivity_error=mean_squared_error(test_senstivity, p2)\n# synergy_error1=mean_absolute_error(test_synergy, p1)\n# # senstivity_error1=mean_absolute_error(test_senstivity, p2)\n# p12=[]\n# for i in range(len(p1)):\n#   x=p1[i]\n#   p12.append(x[0])\n# p12=np.array(p12)\n# synergy_error2=r2_score(test_synergy, p12)\n# # senstivity_error2=r2_score(test_senstivity, p2)\n# print(\"synergy_mean_squared_error\",synergy_error)\n# # print(\"senstivity_mean_squared_error\",senstivity_error)\n# print(\"synergy_mean_absolute_error\",synergy_error1)\n# # print(\"senstivity_mean_absolute_error\",senstivity_error1)\n# print(\"synergy_r2_score\",synergy_error2)\n# # print(\"senstivity_r2_score\",senstivity_error2)\n# losses = model1.evaluate([test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line],[test_synergy],batch_size =64)\n# print(losses)\n\n\n# synergy_pear= pearsonr(test_synergy, p12)\n# synergy_spear= spearmanr(test_synergy, p12)\n# print(\"synergy_pear\",synergy_pear)\n# print(\"synergy_spear\",synergy_spear)\n# p1\n# # senstivity_pear= pearsonr(test_senstivity, p2)\n# # senstivity_spear= spearmanr(test_senstivity, p2)\n# # print(\"senstivity_pear\",senstivity_pear)\n# # print(\"senstivity_spear\",senstivity_spear)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T20:58:12.100157Z","iopub.execute_input":"2022-12-12T20:58:12.100578Z","iopub.status.idle":"2022-12-12T20:58:12.108664Z","shell.execute_reply.started":"2022-12-12T20:58:12.100539Z","shell.execute_reply":"2022-12-12T20:58:12.107079Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# import tensorflow_datasets as tfds\nimport tensorflow as tf\ndef scaled_dot_product_attention(q, k, v, mask=None):\n  \"\"\"Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead)\n  but it must be broadcastable for addition.\n\n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable\n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n  Returns:\n    output, attention_weights\n  \"\"\"\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)\n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits)#, axis=-1)#tf.nn.linear()#, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output,attention_weights \n\n#multi-head\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self,*, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n\n    assert d_model % self.num_heads == 0\n\n    self.depth = d_model // self.num_heads\n\n    self.wq = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wk = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wv = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n\n    self.dense = tf.keras.layers.Dense(d_model)#,activation='relu',use_bias='true')\n\n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n\n  def call(self, v, k, q, mask=None):\n    batch_size = tf.shape(q)[0]\n\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n#     q=PReLU()(q)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n#     k=PReLU()(k)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n#     v=PReLU()(v)\n\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention,\n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n    return output#, attention_weights\n\n\n\nclass CrossStitch(tf.keras.layers.Layer):\n\n    \"\"\"Cross-Stitch implementation according to arXiv:1604.03539\n    Implementation adapted from https://github.com/helloyide/Cross-stitch-Networks-for-Multi-task-Learning\"\"\"\n\n    def __init__(self, num_tasks, *args, **kwargs):\n        \"\"\"initialize class variables\"\"\"\n        self.num_tasks = num_tasks\n        super(CrossStitch, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        \"\"\"initialize the kernel and set the instance to 'built'\"\"\"\n        self.kernel = self.add_weight(name=\"kernel\",\n                                      shape=(self.num_tasks,\n                                             self.num_tasks),\n                                      initializer='identity',\n                                      trainable=True)\n        super(CrossStitch, self).build(input_shape)\n\n    def call(self, xl):\n        \"\"\"\n        called by TensorFlow when the model gets build. \n        Returns a stacked tensor with num_tasks channels in the 0 dimension, \n        which need to be unstacked.\n        \"\"\"\n        if (len(xl) != self.num_tasks):\n            # should not happen\n            raise ValueError()\n\n        out_values = []\n        for this_task in range(self.num_tasks):\n            this_weight = self.kernel[this_task, this_task]\n            out = tf.math.scalar_mul(this_weight, xl[this_task])\n            for other_task in range(self.num_tasks):\n                if this_task == other_task:\n                    continue  # already weighted!\n                other_weight = self.kernel[this_task, other_task]\n                out += tf.math.scalar_mul(other_weight, xl[other_task])\n            out_values.append(out)\n        # HACK!\n        # unless we stack, and then unstack the tensors, TF (2.0.0) can't follow\n        # the graph, so it aborts during model initialization.\n        # return tf.stack(out_values, axis=0)\n        return out_values[0],out_values[1]\n\n    def compute_output_shape(self, input_shape):\n        return [self.num_tasks] + input_shape\n\n    def get_config(self):\n        \"\"\"implemented so keras can save the model to json/yml\"\"\"\n        config = {\n            \"num_tasks\": self.num_tasks\n        }\n        base_config = super(CrossStitch, self).get_config()\n        return dict(list(config.items()) + list(base_config.items()))\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T20:58:12.111847Z","iopub.execute_input":"2022-12-12T20:58:12.112664Z","iopub.status.idle":"2022-12-12T20:58:12.134922Z","shell.execute_reply.started":"2022-12-12T20:58:12.112623Z","shell.execute_reply":"2022-12-12T20:58:12.133922Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\ndef generate_network_att1(x_in1,a_in1,x_in2,a_in2,cell, inDrop, drop):\n    # fill the architecture params from dict\n#     gcn_layers = [1024,512,156]\n#     gan_layers = [1024,512]\n#     cell_layers = [2048,512]#,2048]\n#     snp_layers = [1024,512,128]#,2048]\n#     dsn1_layers = [1024,2048,1024]\n#     dsn2_layers = [1024,2048,1024]\n    # fill the architecture params from dict\n    gcn_layers = [78,156,312]\n#     gan_layers = [128,128]\n#     gcn_layers = [32,64,128]\n    cell_layers = [512,265,128]#for gcn\n#     cell_layers = [2048,512,256] #for gan\n    snp_layers = [512,128]#for gcn\n#     snp_layers = [2048,512,256]# for gan\n    dsn1_layers = [1024,2048,1024]\n    dsn2_layers = [1024,2048,1024]\n    N = x_in1[0].shape[0]\n    F = x_in1[0].shape[1]\n    \n    channels = 8  # Number of channels in each head of the first GAT layer\n    n_attn_heads = 10  # Number of attention heads in first GAT layer\n    dropout = 0.2 # Dropout rate for the features and adjacency matrix\n    drop=0.2\n    l2_reg = 1e-3\n    N = x_in1[0].shape[0]\n    F = x_in1[0].shape[1]\n    \n    channels = 8  # Number of channels in each head of the first GAT layer\n    n_attn_heads = 8  # Number of attention heads in first GAT layer\n    dropout = 0.2 # Dropout rate for the features and adjacency matrix\n    drop=0.2\n    l2_reg = 1e-3  # L2 regularization rate\n    # contruct two parallel networks\n    x_in1 = Input(shape=(N,F,),name='x_in1')\n    a_in1 = Input((N,N,),name='a_in1')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d1 = Dropout(dropout)(x_in1)\n             middle_layer_d1  = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activty_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d1, a_in1])\n#              middle_layer_d1=concatenate([middle_layer_d1,do_1_d1])\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n             gcn_output1=GlobalMaxPool()(middle_layer_d1)\n#              gcn_output1=GlobalAttentionPool(gcn_layers[l],name='a1')(middle_layer_d1)\n             \n#              gcn_output1=Flatten()(x_in1)\n#              gcn_output1=GlobalAttentionPool(1000)(x_in1)#(middle_layer_d1)\n             \n             \n        else:\n             middle_layer_d1 = Dropout(dropout)(middle_layer_d1)\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n#              middle_layer_d1=concatenate([middle_layer_d1,middle_layer_d11])\n#              middle_layer_d1=BatchNormalization()(middle_layer_d1)\n#     print(x_in1.shape)\n    # print(a_in1.shape)\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n#             middle_layer1  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in1, a_in1])\n#             middle_layer1=concatenate([middle_layer1,x_in1]) \n#         elif l == (len(gan_layers)-1):\n#             middle_layer1 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n#             gan_output1=GlobalAttentionPool(512)(middle_layer1)\n# #             gan_output1 = Dense(int(gan_layers[l]), activation='elu')(gan_output1)\n#         else:\n#             middle_layer11 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n#             middle_layer1=concatenate([middle_layer1,middle_layer11])\n    \n#  156   concatModel1 = concatenate([gcn_output1, gan_output1])\n    gcn_output1 = Dense(256, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output1)\n    gcn_output1 = Dropout(dropout)(gcn_output1)\n    gcn_output1 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output1)\n    \n#     gan_output1 = Dense(128, activation='relu')(gan_output1)\n    \n    concatModel1 =  gcn_output1\n    # # addtModel = Add([gcn_output, gan_output])\n    x_in2 = Input(shape=(N,F,),name='x_in2')\n    a_in2 = Input((N,N,),name='a_in2')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d2 = Dropout(dropout)(x_in2)\n             middle_layer_d2 =  GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d2, a_in2])\n#              middle_layer_d2=concatenate([middle_layer_d2,do_1_d2])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n#              gcn_output2=GlobalAttentionPool(gcn_layers[l],name='a2')(middle_layer_d2)\n             gcn_output2=GlobalMaxPool()(middle_layer_d2)\n#             gcn_output2=Flatten()(x_in2)\n#             gcn_output2=GlobalAttentionPool(1000)(x_in2)#(middle_layer_d2)\n             \n        else:\n             middle_layer_d2 = Dropout(dropout)(middle_layer_d2)\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n#              middle_layer_d2=concatenate([middle_layer_d2,middle_layer_d21])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n            \n#             middle_layer2  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in2, a_in2])\n#             middle_layer2=concatenate([middle_layer2,x_in2])\n#         elif l == (len(gan_layers)-1):\n#             middle_layer2 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n#             gan_output2=GlobalAttentionPool(512)(middle_layer2)\n# #             gan_output2 = Dense(int(gan_layers[l]), activation='tanh')(gan_output2)\n#         else:\n#             middle_layer21 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n#             middle_layer2=concatenate([middle_layer2,middle_layer_d21])\n    # print(gcn_output2.shape)\n    # print(gan_output2.shape)        \n#  156   concatModel2 = concatenate([gcn_output2, gan_output2])\n    gcn_output2 = Dense(256, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output2)\n    gcn_output2 = Dropout(dropout)(gcn_output2)\n    gcn_output2 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output2)\n    \n#     gan_output2 = Dense(128, activation='relu')(gan_output2)\n        \n    concatModel2 = gcn_output2\n    # concatModel2 = Add()([gcn_output2, gan_output2])\n    for cell_layer in range(len(cell_layers)):\n      if cell_layer == 0:\n        input_cell    = Input(shape=(cell[0].shape[0],))\n#         input_cell1 = BatchNormalization()(input_cell)\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(input_cell)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      elif cell_layer == (len(cell_layers)-1):\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      #  snp_output = Dense(1, activation='linear')(snpFC)\n      else:\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n    # print(concatModel1.shape)\n    # print(cellFC.shape)\n    \n    concatModel=concatenate([concatModel2,concatModel1,cellFC])\n    concatModel=BatchNormalization()(concatModel)\n#     input2=concatenate([concatModel2,cellFC])\n    # input1=concatenate([gan_output1,cellFC])\n    # input2=concatenate([gan_output2,cellFC])\n    # # addtModel = Add([concatModel1, concatModel2])\n    # dsn1_layers = [2048,4096,2048]\n    # dsn2_layers = [2048,4096,2048]\n    # snp_layers = [2048,1024]\n    # # contruct two parallel networks\n  \n    \n#     task1=Dense(512, activation='relu',use_bias=True)(concatModel)\n# #     task1=PReLU()(task1)\n#     task2=Dense(512, activation='relu',use_bias=True)(concatModel)\n    \n    layer1 =MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task1= layer1(concatModel,concatModel,concatModel)\n    layer2 = MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task2= layer2(concatModel,concatModel,concatModel)\n    task11 = Reshape([a_task1.shape[2]])(a_task1)\n    task22 = Reshape([a_task2.shape[2]])(a_task2)\n    task1=concatenate([task11,concatModel])\n    task2=concatenate([task22,concatModel])\n#     task2=PReLU()(task2)\n#     task1=BatchNormalization()(task1)\n#     task2=BatchNormalization()(task2)\n    r_task1,r_task2 = CrossStitch(2)([task1,task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = apply_cross_stitch([task1,task2])\n#     r_task1=PReLU()(r_task1)\n#     r_task2=PReLU()(r_task2)\n\n#     r_task1=concatenate([r_task1,task1])\n#     r_task2=concatenate([r_task2,task2])\n#     r_task1=BatchNormalization()(r_task1)\n#     r_task2=BatchNormalization()(r_task2)\n#     r_task1=concatenate([r_task1,task1])\n#     r_task2=concatenate([r_task2,task2])\n    \n#     r_task1=Dense(256, activation='relu',use_bias=True)(r_task1)\n    \n#     r_task2=Dense(256, activation='relu',use_bias=True)(r_task2)\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n    \n    \n    r_task1=Dense(1024,activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    \n    r_task2=Dense(1024, activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n    r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n    \n#     layer1 =MultiHeadAttention(d_model=r_task1.shape[1], num_heads=4)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=4)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     task11 = Reshape([a_task1.shape[2]])(a_task1)\n#     task22 = Reshape([a_task2.shape[2]])(a_task2)\n#     r_task1=concatenate([task11,r_task1])\n#     r_task2=concatenate([task22,r_task2])\n    \n    r_task1=concatenate([r_task1,task1])\n    r_task2=concatenate([r_task2,task2])\n    \n#     r_task1=PReLU()(r_task1)\n#     r_task2=PReLU()(r_task2)\n\n \n   \n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = apply_cross_stitch([r_task1,r_task2])\n\n    \n#     r_task1=BatchNormalization()(r_task1)\n#     r_task2=BatchNormalization()(r_task2)\n    \n#     r_task1=Dense(128, activation='linear',use_bias=True)(r_task1)\n    \n#     r_task2=Dense(128, activation='linear',use_bias=True)(r_task2)\n#     r_task1=PReLU()(r_task1)\n#     r_task2=PReLU()(r_task2)\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = apply_cross_stitch(r_task1,r_task2)\n\n    \n#     r_task1=BatchNormalization()(r_task1)\n#     r_task2=BatchNormalization()(r_task2)\n    \n     \n#     layer1 =MultiHeadAttention(d_model=r_task1.shape[1], num_heads=4)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=4)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task1=BatchNormalization()(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n#     a_task2=BatchNormalization()(a_task2)\n    \n    \n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n# #  \n#     rr_task2=Reshape([1,r_task2.shape[1]])(r_task2)\n#     a_task1=layer(rr_task1,rr_task1)\n#     a_task2=layer(rr_task2,rr_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n    \n#     layer1 = MultiHeadAttention(d_model=r_task1.shape[1], num_heads=8)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=8)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n    \n#     r_task1 = Dense(64, activation='linear',name=\"synergy\")(a_task1)\n#     r_task2 = Dense(64, activation='linear',name=\"senstivity\")(a_task2)\n    \n#     layer1 = MultiHeadAttention(d_model=r_task1.shape[1], num_heads=8)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=8)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task1=BatchNormalization()(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n#     a_task2=BatchNormalization()(a_task2)\n   \n    r_task1 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(128, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n    r_task2=PReLU()(r_task2)\n    r_task1 = Dense(64, activation='linear',name=\"synergy\",kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(64, activation='relu',name=\"senstivity\",kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n# \n#     r_task1 = Dense(512, activation='relu')(r_task1)\n# #   \n#     r_task2 = Dense(512, activation='relu')(r_task2)\n\n#     r_task1 = Dense(256, activation='relu',name=\"synergy\")(r_task1)\n#     r_task2 = Dense(256, activation='relu',name=\"senstivity\")(r_task2)\n    \n    snp_output1 = Dense(1, activation='linear')(r_task1)\n#     snp_output1=PReLU()(snp_output1)\n    snp_output2 = Dense(1, activation='relu')(r_task2)\n#     model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [f_task1,f_task2])\n    model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [snp_output1,snp_output2])\n    # tf.keras.utils.plot_model(model.build_graph(), to_file=\"model.png\",\n    #        expand_nested=True, show_shapes=True)\n    print(model.summary())\n    return model\n\ndef trainer_att1(model, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity, epo, batch_size, earlyStop):\n    model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=float(l_rate)))#, beta_1=0.9, beta_2=0.999, amsgrad=True))\n#     model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=float(l_rate), momentum=0.9, nesterov=True, clipnorm=1.))\n    model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line], [train_synergy,train_senstivity], shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 )#,restore_best_weights=True,validation_split=0.1) \n                  #  validation_data=([val_input1,val_input2], [val_synergy,val_senstivity]))\n    # model.fit({\"input1\":train_drug1, \"input2\":train_drug2}, {\"synergy\":train_synergy,\"senstivity\":train_senstivity}, epochs=epo, shuffle=True, batch_size=batch_size,verbose=1) \n    #                validation_data=({\"input1\":val_input1,\"input2\":val_input2}, {\"synergy\":val_synergy,\"senstivity\":val_senstivity}))\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-12-12T20:58:12.136629Z","iopub.execute_input":"2022-12-12T20:58:12.137083Z","iopub.status.idle":"2022-12-12T20:58:12.180122Z","shell.execute_reply.started":"2022-12-12T20:58:12.137048Z","shell.execute_reply":"2022-12-12T20:58:12.179239Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# l_rate = 0.0001\n# inDrop = 0.2\n# drop = 0.2\n# max_epoch = 500\n# batch_size =16\n\n\n# earlyStop_patience = np.ceil(train_f_drug1.shape[0]/batch_size)#1000\nmodel_att1= generate_network_att1(train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line, inDrop, drop)\nmodel_att1=trainer_att1(model_att1, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity,max_epoch, batch_size,\n                                earlyStop_patience)\n\n# p1,p2= predict(model, [test_input1,test_input2])\nap111,ap221= model_att1.predict( [test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line])\nasynergy_error1=mean_squared_error(test_synergy, ap111)\nasenstivity_error1=mean_squared_error(test_senstivity, ap221)\nasynergy_error11=mean_absolute_error(test_synergy, ap111)\nasenstivity_error11=mean_absolute_error(test_senstivity, ap221)\nasynergy_error21=r2_score(test_synergy, ap111)\nasenstivity_error21=r2_score(test_senstivity, ap221)\nprint(\"msynergy_mean_squared_error\",asynergy_error1)\nprint(\"msenstivity_mean_squared_error\",asenstivity_error1)\nprint(\"msynergy_mean_absolute_error\",asynergy_error11)\nprint(\"msenstivity_mean_absolute_error\",asenstivity_error11)\nprint(\"msynergy_r2_score\",asynergy_error21)\nprint(\"msenstivity_r2_score\",asenstivity_error21)\ncross_att1 = model_att1.evaluate([test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line],[test_synergy,test_senstivity],batch_size =64)\nprint(cross_att1)\nasynergy_pear1= pearsonr(test_synergy, ap111)\nasynergy_spear1= spearmanr(test_synergy, ap111)\nprint(\"msynergy_pear\",asynergy_pear1)\nprint(\"msynergy_spear\",asynergy_spear1)\nasenstivity_pear1= pearsonr(test_senstivity, ap221)\nasenstivity_spear1= spearmanr(test_senstivity, ap221)\nprint(\"msenstivity_pear\",asenstivity_pear1)\nprint(\"msenstivity_spear\",asenstivity_spear1)\nap111,ap221\n# yourTerminal:prompt> jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10 \n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T20:58:12.181625Z","iopub.execute_input":"2022-12-12T20:58:12.181973Z","iopub.status.idle":"2022-12-12T21:56:03.962681Z","shell.execute_reply.started":"2022-12-12T20:58:12.181937Z","shell.execute_reply":"2022-12-12T21:56:03.961603Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nx_in2 (InputLayer)              [(None, 69, 32)]     0                                            \n__________________________________________________________________________________________________\nx_in1 (InputLayer)              [(None, 69, 32)]     0                                            \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 69, 32)       0           x_in2[0][0]                      \n__________________________________________________________________________________________________\na_in2 (InputLayer)              [(None, 69, 69)]     0                                            \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 69, 32)       0           x_in1[0][0]                      \n__________________________________________________________________________________________________\na_in1 (InputLayer)              [(None, 69, 69)]     0                                            \n__________________________________________________________________________________________________\ngcn_conv_3 (GCNConv)            (None, 69, 78)       2574        dropout_3[0][0]                  \n                                                                 a_in2[0][0]                      \n__________________________________________________________________________________________________\ngcn_conv (GCNConv)              (None, 69, 78)       2574        dropout[0][0]                    \n                                                                 a_in1[0][0]                      \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 69, 78)       0           gcn_conv_3[0][0]                 \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 69, 78)       0           gcn_conv[0][0]                   \n__________________________________________________________________________________________________\ngcn_conv_4 (GCNConv)            (None, 69, 156)      12324       dropout_4[0][0]                  \n                                                                 a_in2[0][0]                      \n__________________________________________________________________________________________________\ngcn_conv_1 (GCNConv)            (None, 69, 156)      12324       dropout_1[0][0]                  \n                                                                 a_in1[0][0]                      \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            [(None, 934)]        0                                            \n__________________________________________________________________________________________________\ngcn_conv_5 (GCNConv)            (None, 69, 312)      48984       gcn_conv_4[0][0]                 \n                                                                 a_in2[0][0]                      \n__________________________________________________________________________________________________\ngcn_conv_2 (GCNConv)            (None, 69, 312)      48984       gcn_conv_1[0][0]                 \n                                                                 a_in1[0][0]                      \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 512)          478720      input_1[0][0]                    \n__________________________________________________________________________________________________\nglobal_max_pool_1 (GlobalMaxPoo (None, 312)          0           gcn_conv_5[0][0]                 \n__________________________________________________________________________________________________\nglobal_max_pool (GlobalMaxPool) (None, 312)          0           gcn_conv_2[0][0]                 \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 256)          80128       global_max_pool_1[0][0]          \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 256)          80128       global_max_pool[0][0]            \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 265)          135945      dropout_6[0][0]                  \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 256)          0           dense_2[0][0]                    \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 256)          0           dense[0][0]                      \n__________________________________________________________________________________________________\ndropout_7 (Dropout)             (None, 265)          0           dense_5[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 128)          32896       dropout_5[0][0]                  \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 128)          32896       dropout_2[0][0]                  \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 128)          34048       dropout_7[0][0]                  \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 384)          0           dense_3[0][0]                    \n                                                                 dense_1[0][0]                    \n                                                                 dense_6[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 384)          1536        concatenate[0][0]                \n__________________________________________________________________________________________________\nmulti_head_attention (MultiHead (None, None, 384)    591360      batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\nmulti_head_attention_1 (MultiHe (None, None, 384)    591360      batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 384)          0           multi_head_attention[0][0]       \n__________________________________________________________________________________________________\nreshape_1 (Reshape)             (None, 384)          0           multi_head_attention_1[0][0]     \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 768)          0           reshape[0][0]                    \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 768)          0           reshape_1[0][0]                  \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\ncross_stitch (CrossStitch)      ((None, 768), (None, 4           concatenate_1[0][0]              \n                                                                 concatenate_2[0][0]              \n__________________________________________________________________________________________________\ndense_15 (Dense)                (None, 1024)         787456      cross_stitch[0][0]               \n__________________________________________________________________________________________________\ndense_16 (Dense)                (None, 1024)         787456      cross_stitch[0][1]               \n__________________________________________________________________________________________________\ncross_stitch_1 (CrossStitch)    ((None, 1024), (None 4           dense_15[0][0]                   \n                                                                 dense_16[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 1792)         0           cross_stitch_1[0][0]             \n                                                                 concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndense_17 (Dense)                (None, 128)          229504      concatenate_3[0][0]              \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 1792)         0           cross_stitch_1[0][1]             \n                                                                 concatenate_2[0][0]              \n__________________________________________________________________________________________________\np_re_lu (PReLU)                 (None, 128)          128         dense_17[0][0]                   \n__________________________________________________________________________________________________\ndense_18 (Dense)                (None, 128)          229504      concatenate_4[0][0]              \n__________________________________________________________________________________________________\nsynergy (Dense)                 (None, 64)           8256        p_re_lu[0][0]                    \n__________________________________________________________________________________________________\np_re_lu_1 (PReLU)               (None, 128)          128         dense_18[0][0]                   \n__________________________________________________________________________________________________\np_re_lu_2 (PReLU)               (None, 64)           64          synergy[0][0]                    \n__________________________________________________________________________________________________\nsenstivity (Dense)              (None, 64)           8256        p_re_lu_1[0][0]                  \n__________________________________________________________________________________________________\ndense_19 (Dense)                (None, 1)            65          p_re_lu_2[0][0]                  \n__________________________________________________________________________________________________\ndense_20 (Dense)                (None, 1)            65          senstivity[0][0]                 \n==================================================================================================\nTotal params: 4,237,671\nTrainable params: 4,236,903\nNon-trainable params: 768\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n2022-12-12 20:58:19.214995: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 251870976 exceeds 10% of free system memory.\n2022-12-12 20:58:19.694823: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 543096792 exceeds 10% of free system memory.\n2022-12-12 20:58:20.374073: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 251870976 exceeds 10% of free system memory.\n2022-12-12 20:58:20.858639: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 543096792 exceeds 10% of free system memory.\n2022-12-12 20:58:21.662215: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 251870976 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n446/446 [==============================] - 12s 16ms/step - loss: 760.3818 - dense_19_loss: 502.5660 - dense_20_loss: 129.1681\nEpoch 2/500\n446/446 [==============================] - 7s 16ms/step - loss: 681.3441 - dense_19_loss: 464.9842 - dense_20_loss: 91.1401\nEpoch 3/500\n446/446 [==============================] - 7s 15ms/step - loss: 659.7344 - dense_19_loss: 452.8633 - dense_20_loss: 83.7808\nEpoch 4/500\n446/446 [==============================] - 7s 15ms/step - loss: 641.5800 - dense_19_loss: 439.9701 - dense_20_loss: 80.1384\nEpoch 5/500\n446/446 [==============================] - 7s 16ms/step - loss: 623.4213 - dense_19_loss: 426.7648 - dense_20_loss: 76.5339\nEpoch 6/500\n446/446 [==============================] - 7s 16ms/step - loss: 610.1206 - dense_19_loss: 418.1237 - dense_20_loss: 73.0088\nEpoch 7/500\n446/446 [==============================] - 7s 16ms/step - loss: 594.2212 - dense_19_loss: 405.8358 - dense_20_loss: 70.4975\nEpoch 8/500\n446/446 [==============================] - 7s 16ms/step - loss: 577.6245 - dense_19_loss: 393.6982 - dense_20_loss: 67.0136\nEpoch 9/500\n446/446 [==============================] - 7s 15ms/step - loss: 564.2222 - dense_19_loss: 383.2305 - dense_20_loss: 64.9748\nEpoch 10/500\n446/446 [==============================] - 7s 16ms/step - loss: 553.7404 - dense_19_loss: 375.3847 - dense_20_loss: 63.2527\nEpoch 11/500\n446/446 [==============================] - 7s 15ms/step - loss: 539.2278 - dense_19_loss: 363.9881 - dense_20_loss: 61.0278\nEpoch 12/500\n446/446 [==============================] - 7s 15ms/step - loss: 531.2064 - dense_19_loss: 359.2685 - dense_20_loss: 58.5815\nEpoch 13/500\n446/446 [==============================] - 7s 15ms/step - loss: 522.5052 - dense_19_loss: 352.6271 - dense_20_loss: 57.3115\nEpoch 14/500\n446/446 [==============================] - 7s 15ms/step - loss: 513.1252 - dense_19_loss: 346.4602 - dense_20_loss: 54.7983\nEpoch 15/500\n446/446 [==============================] - 8s 17ms/step - loss: 501.3856 - dense_19_loss: 337.4193 - dense_20_loss: 52.7150\nEpoch 16/500\n446/446 [==============================] - 7s 16ms/step - loss: 488.1506 - dense_19_loss: 326.9708 - dense_20_loss: 50.5870\nEpoch 17/500\n446/446 [==============================] - 7s 15ms/step - loss: 485.5526 - dense_19_loss: 326.1746 - dense_20_loss: 49.4485\nEpoch 18/500\n446/446 [==============================] - 7s 16ms/step - loss: 477.0617 - dense_19_loss: 319.5376 - dense_20_loss: 48.1657\nEpoch 19/500\n446/446 [==============================] - 7s 17ms/step - loss: 469.3553 - dense_19_loss: 314.1076 - dense_20_loss: 46.3978\nEpoch 20/500\n446/446 [==============================] - 7s 15ms/step - loss: 465.2774 - dense_19_loss: 311.8665 - dense_20_loss: 45.0357\nEpoch 21/500\n446/446 [==============================] - 7s 16ms/step - loss: 457.7317 - dense_19_loss: 306.3341 - dense_20_loss: 43.5614\nEpoch 22/500\n446/446 [==============================] - 7s 15ms/step - loss: 448.9821 - dense_19_loss: 299.6322 - dense_20_loss: 42.0046\nEpoch 23/500\n446/446 [==============================] - 7s 16ms/step - loss: 441.0211 - dense_19_loss: 292.8574 - dense_20_loss: 41.2308\nEpoch 24/500\n446/446 [==============================] - 7s 16ms/step - loss: 433.0497 - dense_19_loss: 287.2797 - dense_20_loss: 39.2346\nEpoch 25/500\n446/446 [==============================] - 7s 15ms/step - loss: 426.6942 - dense_19_loss: 282.7184 - dense_20_loss: 37.8516\nEpoch 26/500\n446/446 [==============================] - 7s 15ms/step - loss: 419.0077 - dense_19_loss: 275.8261 - dense_20_loss: 37.4480\nEpoch 27/500\n446/446 [==============================] - 7s 15ms/step - loss: 415.1760 - dense_19_loss: 274.1378 - dense_20_loss: 35.7233\nEpoch 28/500\n446/446 [==============================] - 7s 16ms/step - loss: 404.1857 - dense_19_loss: 264.3835 - dense_20_loss: 34.8845\nEpoch 29/500\n446/446 [==============================] - 7s 15ms/step - loss: 401.1403 - dense_19_loss: 262.9725 - dense_20_loss: 33.6608\nEpoch 30/500\n446/446 [==============================] - 7s 15ms/step - loss: 393.5892 - dense_19_loss: 256.4750 - dense_20_loss: 32.9011\nEpoch 31/500\n446/446 [==============================] - 7s 15ms/step - loss: 380.4077 - dense_19_loss: 244.8403 - dense_20_loss: 31.7293\nEpoch 32/500\n446/446 [==============================] - 7s 16ms/step - loss: 381.1664 - dense_19_loss: 246.2394 - dense_20_loss: 31.3778\nEpoch 33/500\n446/446 [==============================] - 7s 16ms/step - loss: 378.6900 - dense_19_loss: 245.2237 - dense_20_loss: 30.2781\nEpoch 34/500\n446/446 [==============================] - 7s 15ms/step - loss: 376.5356 - dense_19_loss: 243.4280 - dense_20_loss: 30.2107\nEpoch 35/500\n446/446 [==============================] - 7s 16ms/step - loss: 355.8413 - dense_19_loss: 224.4519 - dense_20_loss: 28.7564\nEpoch 36/500\n446/446 [==============================] - 7s 15ms/step - loss: 355.8625 - dense_19_loss: 224.7658 - dense_20_loss: 28.7106\nEpoch 37/500\n446/446 [==============================] - 7s 15ms/step - loss: 349.5064 - dense_19_loss: 219.2233 - dense_20_loss: 28.1247\nEpoch 38/500\n446/446 [==============================] - 7s 16ms/step - loss: 341.1457 - dense_19_loss: 212.2040 - dense_20_loss: 27.1348\nEpoch 39/500\n446/446 [==============================] - 7s 15ms/step - loss: 342.8706 - dense_19_loss: 214.5630 - dense_20_loss: 26.7769\nEpoch 40/500\n446/446 [==============================] - 7s 15ms/step - loss: 328.0186 - dense_19_loss: 201.5539 - dense_20_loss: 25.2940\nEpoch 41/500\n446/446 [==============================] - 7s 15ms/step - loss: 326.5943 - dense_19_loss: 200.5204 - dense_20_loss: 25.1732\nEpoch 42/500\n446/446 [==============================] - 7s 16ms/step - loss: 327.0480 - dense_19_loss: 201.3054 - dense_20_loss: 25.0867\nEpoch 43/500\n446/446 [==============================] - 7s 15ms/step - loss: 315.2875 - dense_19_loss: 190.8361 - dense_20_loss: 24.1644\nEpoch 44/500\n446/446 [==============================] - 7s 15ms/step - loss: 313.1436 - dense_19_loss: 189.8558 - dense_20_loss: 23.2737\nEpoch 45/500\n446/446 [==============================] - 7s 15ms/step - loss: 306.7747 - dense_19_loss: 184.2252 - dense_20_loss: 22.7139\nEpoch 46/500\n446/446 [==============================] - 7s 15ms/step - loss: 299.4004 - dense_19_loss: 177.7350 - dense_20_loss: 22.1308\nEpoch 47/500\n446/446 [==============================] - 7s 16ms/step - loss: 297.3742 - dense_19_loss: 175.9029 - dense_20_loss: 22.1933\nEpoch 48/500\n446/446 [==============================] - 7s 15ms/step - loss: 300.8757 - dense_19_loss: 180.0119 - dense_20_loss: 21.8224\nEpoch 49/500\n446/446 [==============================] - 7s 15ms/step - loss: 296.3836 - dense_19_loss: 176.1129 - dense_20_loss: 21.5193\nEpoch 50/500\n446/446 [==============================] - 7s 15ms/step - loss: 285.9426 - dense_19_loss: 167.0200 - dense_20_loss: 20.3680\nEpoch 51/500\n446/446 [==============================] - 7s 16ms/step - loss: 287.2397 - dense_19_loss: 168.6175 - dense_20_loss: 20.3900\nEpoch 52/500\n446/446 [==============================] - 7s 15ms/step - loss: 283.2742 - dense_19_loss: 165.3509 - dense_20_loss: 19.9219\nEpoch 53/500\n446/446 [==============================] - 7s 15ms/step - loss: 274.0675 - dense_19_loss: 156.4735 - dense_20_loss: 19.7451\nEpoch 54/500\n446/446 [==============================] - 7s 15ms/step - loss: 273.0609 - dense_19_loss: 156.5230 - dense_20_loss: 19.0048\nEpoch 55/500\n446/446 [==============================] - 7s 15ms/step - loss: 260.9926 - dense_19_loss: 145.3708 - dense_20_loss: 18.3520\nEpoch 56/500\n446/446 [==============================] - 7s 17ms/step - loss: 268.4567 - dense_19_loss: 152.8778 - dense_20_loss: 18.5640\nEpoch 57/500\n446/446 [==============================] - 7s 15ms/step - loss: 260.9793 - dense_19_loss: 146.2459 - dense_20_loss: 18.0299\nEpoch 58/500\n446/446 [==============================] - 7s 15ms/step - loss: 252.8897 - dense_19_loss: 138.8754 - dense_20_loss: 17.4834\nEpoch 59/500\n446/446 [==============================] - 7s 15ms/step - loss: 256.4094 - dense_19_loss: 142.6569 - dense_20_loss: 17.4475\nEpoch 60/500\n446/446 [==============================] - 7s 15ms/step - loss: 254.2045 - dense_19_loss: 140.8454 - dense_20_loss: 17.2167\nEpoch 61/500\n446/446 [==============================] - 7s 16ms/step - loss: 243.9012 - dense_19_loss: 131.3238 - dense_20_loss: 16.6675\nEpoch 62/500\n446/446 [==============================] - 7s 15ms/step - loss: 247.9180 - dense_19_loss: 135.5808 - dense_20_loss: 16.6659\nEpoch 63/500\n446/446 [==============================] - 7s 15ms/step - loss: 242.2999 - dense_19_loss: 130.5610 - dense_20_loss: 16.3314\nEpoch 64/500\n446/446 [==============================] - 7s 16ms/step - loss: 234.4360 - dense_19_loss: 123.5308 - dense_20_loss: 15.6807\nEpoch 65/500\n446/446 [==============================] - 7s 16ms/step - loss: 232.3780 - dense_19_loss: 122.0955 - dense_20_loss: 15.3519\nEpoch 66/500\n446/446 [==============================] - 7s 15ms/step - loss: 233.8104 - dense_19_loss: 123.5628 - dense_20_loss: 15.4640\nEpoch 67/500\n446/446 [==============================] - 7s 15ms/step - loss: 232.1419 - dense_19_loss: 122.4490 - dense_20_loss: 15.1350\nEpoch 68/500\n446/446 [==============================] - 7s 15ms/step - loss: 226.8393 - dense_19_loss: 117.4308 - dense_20_loss: 15.0130\nEpoch 69/500\n446/446 [==============================] - 7s 15ms/step - loss: 219.2222 - dense_19_loss: 110.7390 - dense_20_loss: 14.3175\nEpoch 70/500\n446/446 [==============================] - 7s 17ms/step - loss: 225.5783 - dense_19_loss: 117.0375 - dense_20_loss: 14.6003\nEpoch 71/500\n446/446 [==============================] - 7s 15ms/step - loss: 218.2886 - dense_19_loss: 110.4994 - dense_20_loss: 14.0893\nEpoch 72/500\n446/446 [==============================] - 7s 15ms/step - loss: 218.0182 - dense_19_loss: 110.6759 - dense_20_loss: 13.8389\nEpoch 73/500\n446/446 [==============================] - 7s 15ms/step - loss: 212.2528 - dense_19_loss: 105.3000 - dense_20_loss: 13.6122\nEpoch 74/500\n446/446 [==============================] - 7s 16ms/step - loss: 210.7051 - dense_19_loss: 104.2643 - dense_20_loss: 13.3792\nEpoch 75/500\n446/446 [==============================] - 7s 15ms/step - loss: 211.4844 - dense_19_loss: 105.6335 - dense_20_loss: 13.0602\nEpoch 76/500\n446/446 [==============================] - 7s 15ms/step - loss: 209.6468 - dense_19_loss: 103.8062 - dense_20_loss: 13.2014\nEpoch 77/500\n446/446 [==============================] - 7s 15ms/step - loss: 202.8176 - dense_19_loss: 97.5724 - dense_20_loss: 12.8884\nEpoch 78/500\n446/446 [==============================] - 7s 16ms/step - loss: 202.6355 - dense_19_loss: 97.7459 - dense_20_loss: 12.7216\nEpoch 79/500\n446/446 [==============================] - 7s 16ms/step - loss: 202.7488 - dense_19_loss: 98.2307 - dense_20_loss: 12.5156\nEpoch 80/500\n446/446 [==============================] - 7s 15ms/step - loss: 195.7447 - dense_19_loss: 91.9154 - dense_20_loss: 12.0453\nEpoch 81/500\n446/446 [==============================] - 7s 15ms/step - loss: 193.5154 - dense_19_loss: 90.2162 - dense_20_loss: 11.8078\nEpoch 82/500\n446/446 [==============================] - 7s 15ms/step - loss: 194.3521 - dense_19_loss: 91.1212 - dense_20_loss: 11.9500\nEpoch 83/500\n446/446 [==============================] - 7s 16ms/step - loss: 193.4692 - dense_19_loss: 90.5875 - dense_20_loss: 11.7988\nEpoch 84/500\n446/446 [==============================] - 7s 15ms/step - loss: 188.9074 - dense_19_loss: 86.6219 - dense_20_loss: 11.3278\nEpoch 85/500\n446/446 [==============================] - 7s 15ms/step - loss: 186.6682 - dense_19_loss: 84.6808 - dense_20_loss: 11.4203\nEpoch 86/500\n446/446 [==============================] - 7s 15ms/step - loss: 184.1781 - dense_19_loss: 82.7130 - dense_20_loss: 11.0596\nEpoch 87/500\n446/446 [==============================] - 7s 15ms/step - loss: 187.0663 - dense_19_loss: 85.7636 - dense_20_loss: 11.0967\nEpoch 88/500\n446/446 [==============================] - 7s 16ms/step - loss: 179.9501 - dense_19_loss: 79.2308 - dense_20_loss: 10.7288\nEpoch 89/500\n446/446 [==============================] - 7s 16ms/step - loss: 184.6725 - dense_19_loss: 83.9882 - dense_20_loss: 10.8000\nEpoch 90/500\n446/446 [==============================] - 7s 15ms/step - loss: 181.1323 - dense_19_loss: 80.7341 - dense_20_loss: 10.7763\nEpoch 91/500\n446/446 [==============================] - 7s 15ms/step - loss: 180.4777 - dense_19_loss: 80.4400 - dense_20_loss: 10.6491\nEpoch 92/500\n446/446 [==============================] - 7s 15ms/step - loss: 180.0203 - dense_19_loss: 80.7046 - dense_20_loss: 10.2011\nEpoch 93/500\n446/446 [==============================] - 7s 16ms/step - loss: 174.7639 - dense_19_loss: 75.6361 - dense_20_loss: 10.1977\nEpoch 94/500\n446/446 [==============================] - 7s 15ms/step - loss: 170.9450 - dense_19_loss: 71.9775 - dense_20_loss: 10.2732\nEpoch 95/500\n446/446 [==============================] - 7s 15ms/step - loss: 167.9488 - dense_19_loss: 69.6877 - dense_20_loss: 9.7885\nEpoch 96/500\n446/446 [==============================] - 7s 15ms/step - loss: 168.9938 - dense_19_loss: 70.7718 - dense_20_loss: 9.8964\nEpoch 97/500\n446/446 [==============================] - 7s 16ms/step - loss: 168.2589 - dense_19_loss: 70.6414 - dense_20_loss: 9.5521\nEpoch 98/500\n446/446 [==============================] - 7s 15ms/step - loss: 167.5517 - dense_19_loss: 70.1703 - dense_20_loss: 9.5694\nEpoch 99/500\n446/446 [==============================] - 7s 15ms/step - loss: 161.4387 - dense_19_loss: 64.7624 - dense_20_loss: 9.1303\nEpoch 100/500\n446/446 [==============================] - 7s 15ms/step - loss: 166.7966 - dense_19_loss: 69.9627 - dense_20_loss: 9.4949\nEpoch 101/500\n446/446 [==============================] - 7s 15ms/step - loss: 168.7172 - dense_19_loss: 71.8796 - dense_20_loss: 9.5611\nEpoch 102/500\n446/446 [==============================] - 7s 16ms/step - loss: 161.8687 - dense_19_loss: 65.3622 - dense_20_loss: 9.3456\nEpoch 103/500\n446/446 [==============================] - 7s 15ms/step - loss: 159.3908 - dense_19_loss: 63.4773 - dense_20_loss: 9.0894\nEpoch 104/500\n446/446 [==============================] - 7s 15ms/step - loss: 157.1333 - dense_19_loss: 61.8173 - dense_20_loss: 8.7856\nEpoch 105/500\n446/446 [==============================] - 7s 16ms/step - loss: 156.1912 - dense_19_loss: 61.0933 - dense_20_loss: 8.8479\nEpoch 106/500\n446/446 [==============================] - 7s 15ms/step - loss: 158.7215 - dense_19_loss: 63.6287 - dense_20_loss: 8.9707\nEpoch 107/500\n446/446 [==============================] - 7s 16ms/step - loss: 158.5988 - dense_19_loss: 63.8437 - dense_20_loss: 8.7619\nEpoch 108/500\n446/446 [==============================] - 7s 15ms/step - loss: 150.4781 - dense_19_loss: 56.4516 - dense_20_loss: 8.3751\nEpoch 109/500\n446/446 [==============================] - 7s 16ms/step - loss: 151.1707 - dense_19_loss: 57.3688 - dense_20_loss: 8.4370\nEpoch 110/500\n446/446 [==============================] - 7s 16ms/step - loss: 152.4333 - dense_19_loss: 58.7111 - dense_20_loss: 8.5192\nEpoch 111/500\n446/446 [==============================] - 7s 16ms/step - loss: 149.4799 - dense_19_loss: 56.4245 - dense_20_loss: 8.0925\nEpoch 112/500\n446/446 [==============================] - 7s 15ms/step - loss: 152.0388 - dense_19_loss: 58.7028 - dense_20_loss: 8.5190\nEpoch 113/500\n446/446 [==============================] - 7s 16ms/step - loss: 149.0174 - dense_19_loss: 56.2753 - dense_20_loss: 8.1741\nEpoch 114/500\n446/446 [==============================] - 7s 15ms/step - loss: 145.4992 - dense_19_loss: 53.2078 - dense_20_loss: 8.0080\nEpoch 115/500\n446/446 [==============================] - 7s 15ms/step - loss: 145.8800 - dense_19_loss: 54.0546 - dense_20_loss: 7.8095\nEpoch 116/500\n446/446 [==============================] - 7s 16ms/step - loss: 144.7181 - dense_19_loss: 53.0244 - dense_20_loss: 7.8939\nEpoch 117/500\n446/446 [==============================] - 7s 15ms/step - loss: 142.1545 - dense_19_loss: 50.9519 - dense_20_loss: 7.7037\nEpoch 118/500\n446/446 [==============================] - 7s 16ms/step - loss: 143.8483 - dense_19_loss: 52.6711 - dense_20_loss: 7.8433\nEpoch 119/500\n446/446 [==============================] - 7s 15ms/step - loss: 149.0064 - dense_19_loss: 57.8815 - dense_20_loss: 7.9193\nEpoch 120/500\n446/446 [==============================] - 7s 16ms/step - loss: 141.9084 - dense_19_loss: 51.2370 - dense_20_loss: 7.6326\nEpoch 121/500\n446/446 [==============================] - 7s 15ms/step - loss: 139.9196 - dense_19_loss: 49.6939 - dense_20_loss: 7.5108\nEpoch 122/500\n446/446 [==============================] - 7s 15ms/step - loss: 140.8762 - dense_19_loss: 50.9474 - dense_20_loss: 7.4390\nEpoch 123/500\n446/446 [==============================] - 7s 15ms/step - loss: 139.3175 - dense_19_loss: 49.6878 - dense_20_loss: 7.4239\nEpoch 124/500\n446/446 [==============================] - 7s 15ms/step - loss: 135.6058 - dense_19_loss: 46.4454 - dense_20_loss: 7.2196\nEpoch 125/500\n446/446 [==============================] - 7s 16ms/step - loss: 134.7379 - dense_19_loss: 45.9853 - dense_20_loss: 7.1073\nEpoch 126/500\n446/446 [==============================] - 7s 15ms/step - loss: 131.3258 - dense_19_loss: 42.9176 - dense_20_loss: 7.0311\nEpoch 127/500\n446/446 [==============================] - 7s 15ms/step - loss: 136.3089 - dense_19_loss: 47.8875 - dense_20_loss: 7.2698\nEpoch 128/500\n446/446 [==============================] - 7s 15ms/step - loss: 132.9045 - dense_19_loss: 44.7679 - dense_20_loss: 7.1339\nEpoch 129/500\n446/446 [==============================] - 7s 16ms/step - loss: 132.6008 - dense_19_loss: 45.0306 - dense_20_loss: 6.8502\nEpoch 130/500\n446/446 [==============================] - 7s 15ms/step - loss: 129.1623 - dense_19_loss: 42.0125 - dense_20_loss: 6.7950\nEpoch 131/500\n446/446 [==============================] - 7s 16ms/step - loss: 130.4132 - dense_19_loss: 43.6014 - dense_20_loss: 6.6924\nEpoch 132/500\n446/446 [==============================] - 7s 15ms/step - loss: 126.5451 - dense_19_loss: 40.0980 - dense_20_loss: 6.6060\nEpoch 133/500\n446/446 [==============================] - 7s 15ms/step - loss: 125.1150 - dense_19_loss: 39.1051 - dense_20_loss: 6.5031\nEpoch 134/500\n446/446 [==============================] - 7s 17ms/step - loss: 127.8510 - dense_19_loss: 42.0356 - dense_20_loss: 6.5541\nEpoch 135/500\n446/446 [==============================] - 7s 15ms/step - loss: 126.9552 - dense_19_loss: 41.3200 - dense_20_loss: 6.6203\nEpoch 136/500\n446/446 [==============================] - 7s 16ms/step - loss: 131.7793 - dense_19_loss: 46.2988 - dense_20_loss: 6.7413\nEpoch 137/500\n446/446 [==============================] - 7s 15ms/step - loss: 132.3054 - dense_19_loss: 46.0731 - dense_20_loss: 7.3203\nEpoch 138/500\n446/446 [==============================] - 7s 15ms/step - loss: 124.2173 - dense_19_loss: 39.2427 - dense_20_loss: 6.4794\nEpoch 139/500\n446/446 [==============================] - 7s 16ms/step - loss: 120.6645 - dense_19_loss: 36.5298 - dense_20_loss: 6.0490\nEpoch 140/500\n446/446 [==============================] - 7s 15ms/step - loss: 123.5759 - dense_19_loss: 39.4864 - dense_20_loss: 6.2364\nEpoch 141/500\n446/446 [==============================] - 7s 15ms/step - loss: 121.2765 - dense_19_loss: 37.4430 - dense_20_loss: 6.2158\nEpoch 142/500\n446/446 [==============================] - 7s 15ms/step - loss: 121.3175 - dense_19_loss: 37.6988 - dense_20_loss: 6.3042\nEpoch 143/500\n446/446 [==============================] - 7s 16ms/step - loss: 121.1354 - dense_19_loss: 37.9062 - dense_20_loss: 6.1356\nEpoch 144/500\n446/446 [==============================] - 7s 15ms/step - loss: 119.2794 - dense_19_loss: 36.3442 - dense_20_loss: 6.1896\nEpoch 145/500\n446/446 [==============================] - 7s 15ms/step - loss: 120.3607 - dense_19_loss: 37.3981 - dense_20_loss: 6.3461\nEpoch 146/500\n446/446 [==============================] - 7s 15ms/step - loss: 117.5312 - dense_19_loss: 35.3322 - dense_20_loss: 5.9730\nEpoch 147/500\n446/446 [==============================] - 7s 16ms/step - loss: 117.2929 - dense_19_loss: 35.1895 - dense_20_loss: 6.1095\nEpoch 148/500\n446/446 [==============================] - 7s 16ms/step - loss: 120.3023 - dense_19_loss: 38.5788 - dense_20_loss: 5.9579\nEpoch 149/500\n446/446 [==============================] - 7s 15ms/step - loss: 119.6007 - dense_19_loss: 37.9495 - dense_20_loss: 6.1516\nEpoch 150/500\n446/446 [==============================] - 7s 15ms/step - loss: 117.6106 - dense_19_loss: 36.2140 - dense_20_loss: 6.0803\nEpoch 151/500\n446/446 [==============================] - 7s 15ms/step - loss: 114.4940 - dense_19_loss: 33.8032 - dense_20_loss: 5.7305\nEpoch 152/500\n446/446 [==============================] - 7s 15ms/step - loss: 114.6034 - dense_19_loss: 34.2547 - dense_20_loss: 5.6976\nEpoch 153/500\n446/446 [==============================] - 7s 16ms/step - loss: 113.5949 - dense_19_loss: 33.5291 - dense_20_loss: 5.6638\nEpoch 154/500\n446/446 [==============================] - 7s 15ms/step - loss: 114.6373 - dense_19_loss: 34.7276 - dense_20_loss: 5.8017\nEpoch 155/500\n446/446 [==============================] - 7s 16ms/step - loss: 114.1678 - dense_19_loss: 34.4156 - dense_20_loss: 5.8783\nEpoch 156/500\n446/446 [==============================] - 7s 15ms/step - loss: 111.9434 - dense_19_loss: 32.7399 - dense_20_loss: 5.5825\nEpoch 157/500\n446/446 [==============================] - 7s 16ms/step - loss: 114.5848 - dense_19_loss: 35.3461 - dense_20_loss: 5.8395\nEpoch 158/500\n446/446 [==============================] - 7s 15ms/step - loss: 115.6778 - dense_19_loss: 36.5444 - dense_20_loss: 5.8588\nEpoch 159/500\n446/446 [==============================] - 7s 15ms/step - loss: 109.5675 - dense_19_loss: 31.1820 - dense_20_loss: 5.4746\nEpoch 160/500\n446/446 [==============================] - 7s 15ms/step - loss: 109.2601 - dense_19_loss: 31.1964 - dense_20_loss: 5.4883\nEpoch 161/500\n446/446 [==============================] - 7s 15ms/step - loss: 108.0439 - dense_19_loss: 30.3366 - dense_20_loss: 5.3753\nEpoch 162/500\n446/446 [==============================] - 7s 16ms/step - loss: 107.7612 - dense_19_loss: 30.3519 - dense_20_loss: 5.4459\nEpoch 163/500\n446/446 [==============================] - 7s 15ms/step - loss: 108.4789 - dense_19_loss: 31.3121 - dense_20_loss: 5.4887\nEpoch 164/500\n446/446 [==============================] - 7s 15ms/step - loss: 108.2236 - dense_19_loss: 31.2983 - dense_20_loss: 5.3928\nEpoch 165/500\n446/446 [==============================] - 7s 15ms/step - loss: 105.2174 - dense_19_loss: 28.9569 - dense_20_loss: 5.1000\nEpoch 166/500\n446/446 [==============================] - 7s 16ms/step - loss: 108.9752 - dense_19_loss: 32.4658 - dense_20_loss: 5.6238\nEpoch 167/500\n446/446 [==============================] - 7s 15ms/step - loss: 109.2602 - dense_19_loss: 33.0033 - dense_20_loss: 5.4327\nEpoch 168/500\n446/446 [==============================] - 7s 15ms/step - loss: 109.8715 - dense_19_loss: 33.3590 - dense_20_loss: 5.7943\nEpoch 169/500\n446/446 [==============================] - 7s 15ms/step - loss: 103.9770 - dense_19_loss: 28.5341 - dense_20_loss: 5.1139\nEpoch 170/500\n446/446 [==============================] - 7s 15ms/step - loss: 103.2760 - dense_19_loss: 28.1126 - dense_20_loss: 5.1615\nEpoch 171/500\n446/446 [==============================] - 7s 16ms/step - loss: 103.2677 - dense_19_loss: 28.5181 - dense_20_loss: 5.0443\nEpoch 172/500\n446/446 [==============================] - 7s 15ms/step - loss: 103.3238 - dense_19_loss: 28.7628 - dense_20_loss: 5.1484\nEpoch 173/500\n446/446 [==============================] - 7s 15ms/step - loss: 102.7592 - dense_19_loss: 28.5944 - dense_20_loss: 5.0398\nEpoch 174/500\n446/446 [==============================] - 7s 15ms/step - loss: 106.3370 - dense_19_loss: 31.9634 - dense_20_loss: 5.3616\nEpoch 175/500\n446/446 [==============================] - 7s 15ms/step - loss: 104.3782 - dense_19_loss: 30.4104 - dense_20_loss: 5.1747\nEpoch 176/500\n446/446 [==============================] - 7s 17ms/step - loss: 101.4155 - dense_19_loss: 27.7871 - dense_20_loss: 5.1027\nEpoch 177/500\n446/446 [==============================] - 7s 15ms/step - loss: 100.8290 - dense_19_loss: 27.6264 - dense_20_loss: 4.9669\nEpoch 178/500\n446/446 [==============================] - 7s 15ms/step - loss: 99.4594 - dense_19_loss: 26.5614 - dense_20_loss: 4.9162\nEpoch 179/500\n446/446 [==============================] - 7s 15ms/step - loss: 100.0974 - dense_19_loss: 27.4187 - dense_20_loss: 4.9954\nEpoch 180/500\n446/446 [==============================] - 7s 16ms/step - loss: 98.5553 - dense_19_loss: 26.3006 - dense_20_loss: 4.8560\nEpoch 181/500\n446/446 [==============================] - 7s 15ms/step - loss: 100.3168 - dense_19_loss: 28.0515 - dense_20_loss: 5.1290\nEpoch 182/500\n446/446 [==============================] - 7s 15ms/step - loss: 98.8241 - dense_19_loss: 27.0069 - dense_20_loss: 4.9354\nEpoch 183/500\n446/446 [==============================] - 7s 15ms/step - loss: 100.4558 - dense_19_loss: 28.7592 - dense_20_loss: 5.0119\nEpoch 184/500\n446/446 [==============================] - 7s 15ms/step - loss: 99.3911 - dense_19_loss: 28.0385 - dense_20_loss: 4.8877\nEpoch 185/500\n446/446 [==============================] - 8s 17ms/step - loss: 98.8584 - dense_19_loss: 27.7080 - dense_20_loss: 4.9575\nEpoch 186/500\n446/446 [==============================] - 7s 15ms/step - loss: 96.7776 - dense_19_loss: 26.0219 - dense_20_loss: 4.8002\nEpoch 187/500\n446/446 [==============================] - 7s 15ms/step - loss: 94.6939 - dense_19_loss: 24.3796 - dense_20_loss: 4.6852\nEpoch 188/500\n446/446 [==============================] - 7s 15ms/step - loss: 96.1985 - dense_19_loss: 26.0731 - dense_20_loss: 4.7099\nEpoch 189/500\n446/446 [==============================] - 7s 16ms/step - loss: 95.2571 - dense_19_loss: 25.4495 - dense_20_loss: 4.6555\nEpoch 190/500\n446/446 [==============================] - 7s 15ms/step - loss: 97.2453 - dense_19_loss: 27.4551 - dense_20_loss: 4.8174\nEpoch 191/500\n446/446 [==============================] - 7s 15ms/step - loss: 94.2148 - dense_19_loss: 24.9515 - dense_20_loss: 4.5449\nEpoch 192/500\n446/446 [==============================] - 7s 15ms/step - loss: 92.7380 - dense_19_loss: 23.9243 - dense_20_loss: 4.4478\nEpoch 193/500\n446/446 [==============================] - 7s 15ms/step - loss: 94.0536 - dense_19_loss: 25.3224 - dense_20_loss: 4.5703\nEpoch 194/500\n446/446 [==============================] - 7s 16ms/step - loss: 95.2815 - dense_19_loss: 26.4961 - dense_20_loss: 4.7543\nEpoch 195/500\n446/446 [==============================] - 7s 16ms/step - loss: 95.3017 - dense_19_loss: 28.3622 - dense_20_loss: 4.7884\nEpoch 204/500\n446/446 [==============================] - 7s 15ms/step - loss: 88.0488 - dense_19_loss: 22.0044 - dense_20_loss: 4.2429\nEpoch 205/500\n446/446 [==============================] - 7s 15ms/step - loss: 89.6444 - dense_19_loss: 23.6405 - dense_20_loss: 4.3742\nEpoch 206/500\n446/446 [==============================] - 7s 16ms/step - loss: 87.7283 - dense_19_loss: 21.9472 - dense_20_loss: 4.4219\nEpoch 207/500\n446/446 [==============================] - 7s 15ms/step - loss: 86.9037 - dense_19_loss: 21.6142 - dense_20_loss: 4.2642\nEpoch 208/500\n446/446 [==============================] - 7s 16ms/step - loss: 86.4843 - dense_19_loss: 21.3873 - dense_20_loss: 4.2727\nEpoch 209/500\n446/446 [==============================] - 7s 15ms/step - loss: 86.5520 - dense_19_loss: 21.6784 - dense_20_loss: 4.2925\nEpoch 210/500\n446/446 [==============================] - 7s 15ms/step - loss: 88.2581 - dense_19_loss: 23.6333 - dense_20_loss: 4.3023\nEpoch 211/500\n446/446 [==============================] - 7s 15ms/step - loss: 86.4279 - dense_19_loss: 22.0244 - dense_20_loss: 4.2676\nEpoch 212/500\n446/446 [==============================] - 7s 16ms/step - loss: 84.5220 - dense_19_loss: 20.6523 - dense_20_loss: 4.0781\nEpoch 213/500\n446/446 [==============================] - 7s 15ms/step - loss: 85.2987 - dense_19_loss: 21.5513 - dense_20_loss: 4.1854\nEpoch 214/500\n446/446 [==============================] - 7s 15ms/step - loss: 85.9662 - dense_19_loss: 22.2715 - dense_20_loss: 4.3282\nEpoch 215/500\n446/446 [==============================] - 7s 15ms/step - loss: 86.1326 - dense_19_loss: 22.7030 - dense_20_loss: 4.2862\nEpoch 216/500\n446/446 [==============================] - 7s 15ms/step - loss: 84.9698 - dense_19_loss: 21.9025 - dense_20_loss: 4.1522\nEpoch 217/500\n446/446 [==============================] - 7s 16ms/step - loss: 84.3212 - dense_19_loss: 21.3670 - dense_20_loss: 4.2222\nEpoch 218/500\n446/446 [==============================] - 7s 15ms/step - loss: 83.3507 - dense_19_loss: 20.7580 - dense_20_loss: 4.0154\nEpoch 219/500\n446/446 [==============================] - 7s 16ms/step - loss: 83.1305 - dense_19_loss: 20.7458 - dense_20_loss: 4.1395\nEpoch 220/500\n446/446 [==============================] - 7s 16ms/step - loss: 83.3107 - dense_19_loss: 21.2357 - dense_20_loss: 4.0751\nEpoch 221/500\n446/446 [==============================] - 7s 15ms/step - loss: 83.5922 - dense_19_loss: 21.6752 - dense_20_loss: 4.0948\nEpoch 222/500\n446/446 [==============================] - 7s 16ms/step - loss: 85.0073 - dense_19_loss: 23.1666 - dense_20_loss: 4.1867\nEpoch 223/500\n446/446 [==============================] - 7s 15ms/step - loss: 82.6991 - dense_19_loss: 21.0264 - dense_20_loss: 4.1732\nEpoch 224/500\n446/446 [==============================] - 7s 15ms/step - loss: 81.8652 - dense_19_loss: 20.6464 - dense_20_loss: 4.0176\nEpoch 225/500\n446/446 [==============================] - 7s 15ms/step - loss: 80.9383 - dense_19_loss: 20.0278 - dense_20_loss: 3.9161\nEpoch 226/500\n446/446 [==============================] - 7s 16ms/step - loss: 80.1880 - dense_19_loss: 19.5269 - dense_20_loss: 3.9369\nEpoch 227/500\n446/446 [==============================] - 7s 15ms/step - loss: 81.5040 - dense_19_loss: 20.9065 - dense_20_loss: 4.0383\nEpoch 228/500\n446/446 [==============================] - 7s 15ms/step - loss: 81.3074 - dense_19_loss: 20.8141 - dense_20_loss: 4.1169\nEpoch 229/500\n446/446 [==============================] - 7s 15ms/step - loss: 82.0095 - dense_19_loss: 21.6778 - dense_20_loss: 4.0758\nEpoch 230/500\n446/446 [==============================] - 7s 15ms/step - loss: 85.4142 - dense_19_loss: 24.7316 - dense_20_loss: 4.4682\nEpoch 231/500\n446/446 [==============================] - 7s 16ms/step - loss: 82.0772 - dense_19_loss: 21.9590 - dense_20_loss: 4.0020\nEpoch 232/500\n446/446 [==============================] - 7s 15ms/step - loss: 80.2697 - dense_19_loss: 20.3513 - dense_20_loss: 4.0859\nEpoch 233/500\n446/446 [==============================] - 7s 15ms/step - loss: 77.7360 - dense_19_loss: 18.4554 - dense_20_loss: 3.7482\nEpoch 234/500\n446/446 [==============================] - 7s 15ms/step - loss: 78.2469 - dense_19_loss: 19.0523 - dense_20_loss: 3.8449\nEpoch 235/500\n446/446 [==============================] - 7s 16ms/step - loss: 77.5726 - dense_19_loss: 18.8243 - dense_20_loss: 3.6868\nEpoch 236/500\n446/446 [==============================] - 7s 16ms/step - loss: 76.6993 - dense_19_loss: 18.2169 - dense_20_loss: 3.6589\nEpoch 237/500\n446/446 [==============================] - 7s 15ms/step - loss: 78.1008 - dense_19_loss: 19.5364 - dense_20_loss: 3.8675\nEpoch 238/500\n446/446 [==============================] - 7s 15ms/step - loss: 77.2035 - dense_19_loss: 18.9884 - dense_20_loss: 3.7787\nEpoch 239/500\n446/446 [==============================] - 7s 15ms/step - loss: 85.2311 - dense_19_loss: 25.5555 - dense_20_loss: 4.9722\nEpoch 240/500\n446/446 [==============================] - 7s 16ms/step - loss: 80.1147 - dense_19_loss: 21.1801 - dense_20_loss: 4.1564\nEpoch 241/500\n446/446 [==============================] - 7s 15ms/step - loss: 76.3161 - dense_19_loss: 18.3361 - dense_20_loss: 3.7036\nEpoch 242/500\n446/446 [==============================] - 7s 15ms/step - loss: 74.7998 - dense_19_loss: 17.1837 - dense_20_loss: 3.6575\nEpoch 243/500\n446/446 [==============================] - 7s 15ms/step - loss: 75.2704 - dense_19_loss: 18.0304 - dense_20_loss: 3.5431\nEpoch 244/500\n446/446 [==============================] - 7s 16ms/step - loss: 75.5133 - dense_19_loss: 18.4276 - dense_20_loss: 3.6383\nEpoch 245/500\n446/446 [==============================] - 7s 16ms/step - loss: 74.4404 - dense_19_loss: 17.5896 - dense_20_loss: 3.6117\nEpoch 246/500\n446/446 [==============================] - 7s 16ms/step - loss: 74.9100 - dense_19_loss: 18.2738 - dense_20_loss: 3.6144\nEpoch 247/500\n446/446 [==============================] - 7s 15ms/step - loss: 74.1860 - dense_19_loss: 17.6742 - dense_20_loss: 3.7047\nEpoch 248/500\n446/446 [==============================] - 7s 15ms/step - loss: 77.8120 - dense_19_loss: 21.2327 - dense_20_loss: 3.8771\nEpoch 249/500\n446/446 [==============================] - 7s 16ms/step - loss: 81.1290 - dense_19_loss: 24.1719 - dense_20_loss: 4.1818\nEpoch 250/500\n446/446 [==============================] - 7s 15ms/step - loss: 74.9202 - dense_19_loss: 18.5524 - dense_20_loss: 3.7329\nEpoch 251/500\n446/446 [==============================] - 7s 15ms/step - loss: 73.7807 - dense_19_loss: 17.8253 - dense_20_loss: 3.6274\nEpoch 252/500\n446/446 [==============================] - 7s 16ms/step - loss: 74.5132 - dense_19_loss: 18.5843 - dense_20_loss: 3.7224\nEpoch 253/500\n446/446 [==============================] - 7s 16ms/step - loss: 73.0606 - dense_19_loss: 17.4865 - dense_20_loss: 3.6557\nEpoch 254/500\n446/446 [==============================] - 7s 16ms/step - loss: 72.9835 - dense_19_loss: 17.6381 - dense_20_loss: 3.6062\nEpoch 255/500\n446/446 [==============================] - 7s 15ms/step - loss: 73.4218 - dense_19_loss: 18.2244 - dense_20_loss: 3.6402\nEpoch 256/500\n446/446 [==============================] - 7s 15ms/step - loss: 72.6731 - dense_19_loss: 17.7472 - dense_20_loss: 3.5420\nEpoch 257/500\n446/446 [==============================] - 7s 16ms/step - loss: 72.3525 - dense_19_loss: 17.6250 - dense_20_loss: 3.5735\nEpoch 258/500\n446/446 [==============================] - 7s 16ms/step - loss: 71.8385 - dense_19_loss: 17.2274 - dense_20_loss: 3.5980\nEpoch 259/500\n446/446 [==============================] - 7s 16ms/step - loss: 72.1896 - dense_19_loss: 17.7271 - dense_20_loss: 3.6512\nEpoch 260/500\n446/446 [==============================] - 7s 16ms/step - loss: 73.2083 - dense_19_loss: 18.9273 - dense_20_loss: 3.6005\nEpoch 261/500\n446/446 [==============================] - 7s 16ms/step - loss: 71.9838 - dense_19_loss: 17.8112 - dense_20_loss: 3.6040\nEpoch 262/500\n446/446 [==============================] - 7s 16ms/step - loss: 77.1381 - dense_19_loss: 22.2871 - dense_20_loss: 4.0391\nEpoch 263/500\n446/446 [==============================] - 8s 17ms/step - loss: 73.9576 - dense_19_loss: 19.6518 - dense_20_loss: 3.7781\nEpoch 264/500\n446/446 [==============================] - 7s 16ms/step - loss: 71.8557 - dense_19_loss: 17.7627 - dense_20_loss: 3.6585\nEpoch 265/500\n446/446 [==============================] - 7s 16ms/step - loss: 68.6214 - dense_19_loss: 15.2951 - dense_20_loss: 3.3243\nEpoch 266/500\n446/446 [==============================] - 7s 16ms/step - loss: 68.4838 - dense_19_loss: 15.3710 - dense_20_loss: 3.3822\nEpoch 267/500\n446/446 [==============================] - 7s 16ms/step - loss: 68.5655 - dense_19_loss: 15.7841 - dense_20_loss: 3.2850\nEpoch 268/500\n446/446 [==============================] - 7s 15ms/step - loss: 69.1792 - dense_19_loss: 16.4523 - dense_20_loss: 3.3625\nEpoch 269/500\n446/446 [==============================] - 7s 15ms/step - loss: 68.7516 - dense_19_loss: 16.2708 - dense_20_loss: 3.3319\nEpoch 270/500\n446/446 [==============================] - 7s 15ms/step - loss: 72.9351 - dense_19_loss: 20.0674 - dense_20_loss: 3.7399\nEpoch 271/500\n446/446 [==============================] - 7s 15ms/step - loss: 72.3573 - dense_19_loss: 19.3786 - dense_20_loss: 3.7837\nEpoch 272/500\n446/446 [==============================] - 7s 16ms/step - loss: 69.4211 - dense_19_loss: 17.1273 - dense_20_loss: 3.4170\nEpoch 273/500\n446/446 [==============================] - 7s 15ms/step - loss: 68.1679 - dense_19_loss: 16.1624 - dense_20_loss: 3.3469\nEpoch 274/500\n446/446 [==============================] - 7s 16ms/step - loss: 85.5008 - dense_19_loss: 32.0820 - dense_20_loss: 4.4706\nEpoch 275/500\n446/446 [==============================] - 7s 15ms/step - loss: 72.8322 - dense_19_loss: 19.9278 - dense_20_loss: 3.8574\nEpoch 276/500\n446/446 [==============================] - 7s 16ms/step - loss: 71.7980 - dense_19_loss: 19.1593 - dense_20_loss: 3.6977\nEpoch 277/500\n446/446 [==============================] - 7s 16ms/step - loss: 69.0720 - dense_19_loss: 17.1336 - dense_20_loss: 3.3226\nEpoch 278/500\n446/446 [==============================] - 7s 15ms/step - loss: 66.6965 - dense_19_loss: 15.1593 - dense_20_loss: 3.2356\nEpoch 279/500\n446/446 [==============================] - 7s 15ms/step - loss: 65.3339 - dense_19_loss: 14.2605 - dense_20_loss: 3.0899\nEpoch 280/500\n446/446 [==============================] - 7s 15ms/step - loss: 66.7897 - dense_19_loss: 15.6865 - dense_20_loss: 3.3019\nEpoch 281/500\n446/446 [==============================] - 7s 16ms/step - loss: 66.5782 - dense_19_loss: 15.6630 - dense_20_loss: 3.2652\nEpoch 282/500\n446/446 [==============================] - 7s 15ms/step - loss: 66.6271 - dense_19_loss: 15.9113 - dense_20_loss: 3.2434\nEpoch 283/500\n446/446 [==============================] - 7s 15ms/step - loss: 66.3417 - dense_19_loss: 15.6730 - dense_20_loss: 3.3250\nEpoch 284/500\n446/446 [==============================] - 7s 15ms/step - loss: 66.2184 - dense_19_loss: 15.8169 - dense_20_loss: 3.2897\nEpoch 285/500\n446/446 [==============================] - 7s 15ms/step - loss: 70.4266 - dense_19_loss: 19.5875 - dense_20_loss: 3.6506\nEpoch 286/500\n446/446 [==============================] - 7s 16ms/step - loss: 70.7653 - dense_19_loss: 19.9279 - dense_20_loss: 3.6848\nEpoch 287/500\n446/446 [==============================] - 7s 16ms/step - loss: 67.4471 - dense_19_loss: 17.2001 - dense_20_loss: 3.2550\nEpoch 288/500\n446/446 [==============================] - 7s 15ms/step - loss: 65.8168 - dense_19_loss: 15.7808 - dense_20_loss: 3.2469\nEpoch 289/500\n446/446 [==============================] - 7s 15ms/step - loss: 64.1591 - dense_19_loss: 14.4899 - dense_20_loss: 3.1915\nEpoch 290/500\n446/446 [==============================] - 7s 17ms/step - loss: 64.3203 - dense_19_loss: 14.8830 - dense_20_loss: 3.1539\nEpoch 291/500\n446/446 [==============================] - 7s 15ms/step - loss: 64.6925 - dense_19_loss: 15.3310 - dense_20_loss: 3.2500\nEpoch 292/500\n446/446 [==============================] - 7s 16ms/step - loss: 64.5367 - dense_19_loss: 15.3629 - dense_20_loss: 3.2033\nEpoch 293/500\n446/446 [==============================] - 7s 16ms/step - loss: 64.4231 - dense_19_loss: 15.3850 - dense_20_loss: 3.2594\nEpoch 294/500\n446/446 [==============================] - 7s 15ms/step - loss: 64.7155 - dense_19_loss: 15.7608 - dense_20_loss: 3.2714\nEpoch 295/500\n446/446 [==============================] - 7s 16ms/step - loss: 63.7650 - dense_19_loss: 15.1189 - dense_20_loss: 3.1337\nEpoch 296/500\n446/446 [==============================] - 7s 15ms/step - loss: 63.9586 - dense_19_loss: 15.4604 - dense_20_loss: 3.1421\nEpoch 297/500\n446/446 [==============================] - 7s 15ms/step - loss: 63.8243 - dense_19_loss: 15.4587 - dense_20_loss: 3.1480\nEpoch 298/500\n446/446 [==============================] - 7s 15ms/step - loss: 65.2988 - dense_19_loss: 16.8222 - dense_20_loss: 3.3482\nEpoch 299/500\n446/446 [==============================] - 7s 15ms/step - loss: 64.4545 - dense_19_loss: 16.0435 - dense_20_loss: 3.2998\nEpoch 300/500\n446/446 [==============================] - 7s 16ms/step - loss: 63.0535 - dense_19_loss: 15.1413 - dense_20_loss: 3.0756\nEpoch 301/500\n446/446 [==============================] - 7s 15ms/step - loss: 62.8478 - dense_19_loss: 15.0157 - dense_20_loss: 3.1709\nEpoch 302/500\n446/446 [==============================] - 7s 15ms/step - loss: 62.2697 - dense_19_loss: 14.6412 - dense_20_loss: 3.1486\nEpoch 303/500\n446/446 [==============================] - 7s 15ms/step - loss: 64.2815 - dense_19_loss: 16.4813 - dense_20_loss: 3.3213\nEpoch 304/500\n446/446 [==============================] - 7s 16ms/step - loss: 64.9966 - dense_19_loss: 17.3202 - dense_20_loss: 3.2680\nEpoch 305/500\n446/446 [==============================] - 7s 15ms/step - loss: 63.5303 - dense_19_loss: 15.8936 - dense_20_loss: 3.3010\nEpoch 306/500\n446/446 [==============================] - 7s 15ms/step - loss: 63.1704 - dense_19_loss: 15.7427 - dense_20_loss: 3.2395\nEpoch 307/500\n446/446 [==============================] - 7s 15ms/step - loss: 61.9214 - dense_19_loss: 14.9073 - dense_20_loss: 3.0271\nEpoch 308/500\n446/446 [==============================] - 7s 15ms/step - loss: 60.7824 - dense_19_loss: 13.9967 - dense_20_loss: 3.0165\nEpoch 309/500\n446/446 [==============================] - 7s 16ms/step - loss: 60.3440 - dense_19_loss: 13.8340 - dense_20_loss: 2.9347\nEpoch 310/500\n446/446 [==============================] - 7s 15ms/step - loss: 60.4962 - dense_19_loss: 13.9921 - dense_20_loss: 3.0232\nEpoch 311/500\n446/446 [==============================] - 7s 15ms/step - loss: 61.4609 - dense_19_loss: 14.8917 - dense_20_loss: 3.1379\nEpoch 312/500\n446/446 [==============================] - 7s 15ms/step - loss: 62.9085 - dense_19_loss: 16.3438 - dense_20_loss: 3.2271\nEpoch 313/500\n446/446 [==============================] - 7s 16ms/step - loss: 61.8093 - dense_19_loss: 15.5265 - dense_20_loss: 3.0582\nEpoch 314/500\n446/446 [==============================] - 7s 15ms/step - loss: 62.4014 - dense_19_loss: 16.0509 - dense_20_loss: 3.2421\nEpoch 315/500\n446/446 [==============================] - 7s 15ms/step - loss: 61.9775 - dense_19_loss: 15.7907 - dense_20_loss: 3.1148\nEpoch 316/500\n446/446 [==============================] - 7s 15ms/step - loss: 64.9730 - dense_19_loss: 18.3666 - dense_20_loss: 3.4333\nEpoch 317/500\n446/446 [==============================] - 7s 16ms/step - loss: 59.7909 - dense_19_loss: 14.0087 - dense_20_loss: 2.8981\nEpoch 318/500\n446/446 [==============================] - 7s 16ms/step - loss: 63.9496 - dense_19_loss: 17.7908 - dense_20_loss: 3.3172\nEpoch 319/500\n446/446 [==============================] - 7s 16ms/step - loss: 64.6158 - dense_19_loss: 17.7759 - dense_20_loss: 3.5720\nEpoch 320/500\n446/446 [==============================] - 7s 15ms/step - loss: 59.1565 - dense_19_loss: 13.4708 - dense_20_loss: 2.9170\nEpoch 321/500\n446/446 [==============================] - 7s 16ms/step - loss: 57.7836 - dense_19_loss: 12.5183 - dense_20_loss: 2.8025\nEpoch 322/500\n446/446 [==============================] - 7s 15ms/step - loss: 58.5069 - dense_19_loss: 13.4204 - dense_20_loss: 2.8279\nEpoch 323/500\n446/446 [==============================] - 7s 16ms/step - loss: 58.5042 - dense_19_loss: 13.4445 - dense_20_loss: 2.9353\nEpoch 324/500\n446/446 [==============================] - 7s 15ms/step - loss: 58.0627 - dense_19_loss: 13.2777 - dense_20_loss: 2.8634\nEpoch 325/500\n446/446 [==============================] - 7s 16ms/step - loss: 58.3280 - dense_19_loss: 13.6072 - dense_20_loss: 2.9033\nEpoch 326/500\n446/446 [==============================] - 7s 15ms/step - loss: 58.3434 - dense_19_loss: 13.7682 - dense_20_loss: 2.9030\nEpoch 327/500\n446/446 [==============================] - 7s 17ms/step - loss: 58.5034 - dense_19_loss: 14.1172 - dense_20_loss: 2.8793\nEpoch 328/500\n446/446 [==============================] - 7s 15ms/step - loss: 58.2591 - dense_19_loss: 13.7925 - dense_20_loss: 3.0374\nEpoch 329/500\n446/446 [==============================] - 7s 15ms/step - loss: 58.8231 - dense_19_loss: 14.3597 - dense_20_loss: 3.0783\nEpoch 330/500\n446/446 [==============================] - 7s 15ms/step - loss: 60.4304 - dense_19_loss: 15.8951 - dense_20_loss: 3.1168\nEpoch 331/500\n446/446 [==============================] - 7s 15ms/step - loss: 61.4066 - dense_19_loss: 16.7389 - dense_20_loss: 3.2039\nEpoch 332/500\n446/446 [==============================] - 7s 17ms/step - loss: 63.2915 - dense_19_loss: 18.3580 - dense_20_loss: 3.3751\nEpoch 333/500\n446/446 [==============================] - 7s 15ms/step - loss: 58.8797 - dense_19_loss: 14.4784 - dense_20_loss: 3.0075\nEpoch 334/500\n446/446 [==============================] - 7s 15ms/step - loss: 56.5520 - dense_19_loss: 12.7195 - dense_20_loss: 2.7809\nEpoch 335/500\n446/446 [==============================] - 7s 15ms/step - loss: 56.5161 - dense_19_loss: 12.9233 - dense_20_loss: 2.7770\nEpoch 336/500\n446/446 [==============================] - 7s 16ms/step - loss: 56.3088 - dense_19_loss: 12.8763 - dense_20_loss: 2.7719\nEpoch 337/500\n446/446 [==============================] - 7s 16ms/step - loss: 55.8725 - dense_19_loss: 12.6915 - dense_20_loss: 2.7092\nEpoch 338/500\n446/446 [==============================] - 7s 15ms/step - loss: 57.3375 - dense_19_loss: 13.9895 - dense_20_loss: 2.8956\nEpoch 339/500\n446/446 [==============================] - 7s 15ms/step - loss: 57.1382 - dense_19_loss: 13.9172 - dense_20_loss: 2.8696\nEpoch 340/500\n446/446 [==============================] - 7s 15ms/step - loss: 57.1412 - dense_19_loss: 14.0738 - dense_20_loss: 2.8544\nEpoch 341/500\n446/446 [==============================] - 7s 16ms/step - loss: 57.3691 - dense_19_loss: 14.3469 - dense_20_loss: 2.9011\nEpoch 342/500\n446/446 [==============================] - 7s 15ms/step - loss: 56.1265 - dense_19_loss: 13.3185 - dense_20_loss: 2.8147\nEpoch 343/500\n446/446 [==============================] - 7s 15ms/step - loss: 55.9943 - dense_19_loss: 13.3914 - dense_20_loss: 2.7970\nEpoch 344/500\n446/446 [==============================] - 7s 15ms/step - loss: 66.8558 - dense_19_loss: 22.1768 - dense_20_loss: 4.1914\nEpoch 346/500\n446/446 [==============================] - 7s 17ms/step - loss: 56.9872 - dense_19_loss: 13.9571 - dense_20_loss: 2.9426\nEpoch 347/500\n446/446 [==============================] - 7s 15ms/step - loss: 56.4259 - dense_19_loss: 13.7285 - dense_20_loss: 2.8653\nEpoch 348/500\n446/446 [==============================] - 7s 16ms/step - loss: 56.8026 - dense_19_loss: 14.2156 - dense_20_loss: 2.9049\nEpoch 349/500\n446/446 [==============================] - 7s 16ms/step - loss: 56.9430 - dense_19_loss: 14.1391 - dense_20_loss: 2.9526\nEpoch 350/500\n446/446 [==============================] - 7s 16ms/step - loss: 54.7740 - dense_19_loss: 12.5584 - dense_20_loss: 2.7919\nEpoch 351/500\n446/446 [==============================] - 7s 15ms/step - loss: 54.6206 - dense_19_loss: 12.6157 - dense_20_loss: 2.7252\nEpoch 352/500\n446/446 [==============================] - 7s 15ms/step - loss: 53.6006 - dense_19_loss: 11.9755 - dense_20_loss: 2.5842\nEpoch 353/500\n446/446 [==============================] - 7s 15ms/step - loss: 54.0713 - dense_19_loss: 12.5732 - dense_20_loss: 2.6378\nEpoch 354/500\n446/446 [==============================] - 7s 15ms/step - loss: 54.6445 - dense_19_loss: 13.1424 - dense_20_loss: 2.7335\nEpoch 355/500\n446/446 [==============================] - 7s 16ms/step - loss: 58.1976 - dense_19_loss: 16.2733 - dense_20_loss: 3.0518\nEpoch 356/500\n446/446 [==============================] - 7s 15ms/step - loss: 58.5318 - dense_19_loss: 16.3667 - dense_20_loss: 3.1382\nEpoch 357/500\n446/446 [==============================] - 7s 15ms/step - loss: 54.9138 - dense_19_loss: 13.3420 - dense_20_loss: 2.7014\nEpoch 358/500\n446/446 [==============================] - 7s 15ms/step - loss: 53.8434 - dense_19_loss: 12.6106 - dense_20_loss: 2.6390\nEpoch 359/500\n446/446 [==============================] - 7s 16ms/step - loss: 54.2172 - dense_19_loss: 12.9939 - dense_20_loss: 2.7880\nEpoch 360/500\n446/446 [==============================] - 7s 16ms/step - loss: 53.8732 - dense_19_loss: 12.7869 - dense_20_loss: 2.6896\nEpoch 361/500\n446/446 [==============================] - 7s 15ms/step - loss: 54.1240 - dense_19_loss: 13.1984 - dense_20_loss: 2.6880\nEpoch 362/500\n446/446 [==============================] - 7s 15ms/step - loss: 53.8613 - dense_19_loss: 13.0830 - dense_20_loss: 2.6575\nEpoch 363/500\n446/446 [==============================] - 7s 15ms/step - loss: 61.6650 - dense_19_loss: 19.5262 - dense_20_loss: 3.5018\nEpoch 364/500\n446/446 [==============================] - 7s 16ms/step - loss: 54.7369 - dense_19_loss: 13.6066 - dense_20_loss: 2.7150\nEpoch 365/500\n446/446 [==============================] - 7s 15ms/step - loss: 52.5456 - dense_19_loss: 11.8927 - dense_20_loss: 2.5423\nEpoch 366/500\n446/446 [==============================] - 7s 15ms/step - loss: 52.6643 - dense_19_loss: 12.1833 - dense_20_loss: 2.6228\nEpoch 367/500\n446/446 [==============================] - 7s 15ms/step - loss: 52.4508 - dense_19_loss: 12.2682 - dense_20_loss: 2.5038\nEpoch 368/500\n446/446 [==============================] - 7s 15ms/step - loss: 53.1219 - dense_19_loss: 12.8854 - dense_20_loss: 2.6409\nEpoch 369/500\n446/446 [==============================] - 7s 16ms/step - loss: 52.2944 - dense_19_loss: 12.2382 - dense_20_loss: 2.6276\nEpoch 370/500\n446/446 [==============================] - 7s 15ms/step - loss: 52.5256 - dense_19_loss: 12.6471 - dense_20_loss: 2.5940\nEpoch 371/500\n446/446 [==============================] - 7s 15ms/step - loss: 52.5471 - dense_19_loss: 12.7817 - dense_20_loss: 2.5748\nEpoch 372/500\n446/446 [==============================] - 7s 15ms/step - loss: 53.1718 - dense_19_loss: 13.2998 - dense_20_loss: 2.7237\nEpoch 373/500\n446/446 [==============================] - 8s 17ms/step - loss: 52.5012 - dense_19_loss: 12.8219 - dense_20_loss: 2.6067\nEpoch 374/500\n446/446 [==============================] - 7s 15ms/step - loss: 52.5066 - dense_19_loss: 12.8580 - dense_20_loss: 2.6905\nEpoch 375/500\n446/446 [==============================] - 7s 15ms/step - loss: 51.8664 - dense_19_loss: 12.4841 - dense_20_loss: 2.5642\nEpoch 376/500\n446/446 [==============================] - 7s 15ms/step - loss: 51.0754 - dense_19_loss: 11.8835 - dense_20_loss: 2.5461\nEpoch 377/500\n446/446 [==============================] - 7s 15ms/step - loss: 51.9374 - dense_19_loss: 12.7407 - dense_20_loss: 2.5978\nEpoch 378/500\n446/446 [==============================] - 7s 17ms/step - loss: 51.7063 - dense_19_loss: 12.6038 - dense_20_loss: 2.6081\nEpoch 379/500\n446/446 [==============================] - 7s 15ms/step - loss: 54.4332 - dense_19_loss: 14.8957 - dense_20_loss: 2.8722\nEpoch 380/500\n446/446 [==============================] - 7s 16ms/step - loss: 51.8456 - dense_19_loss: 12.8396 - dense_20_loss: 2.6011\nEpoch 381/500\n446/446 [==============================] - 7s 16ms/step - loss: 52.3657 - dense_19_loss: 13.3738 - dense_20_loss: 2.6848\nEpoch 382/500\n446/446 [==============================] - 7s 16ms/step - loss: 63.8862 - dense_19_loss: 22.7785 - dense_20_loss: 3.9085\nEpoch 383/500\n446/446 [==============================] - 7s 15ms/step - loss: 52.9016 - dense_19_loss: 13.3678 - dense_20_loss: 2.6373\nEpoch 384/500\n446/446 [==============================] - 7s 15ms/step - loss: 50.6928 - dense_19_loss: 11.6726 - dense_20_loss: 2.4405\nEpoch 385/500\n446/446 [==============================] - 7s 15ms/step - loss: 49.7146 - dense_19_loss: 11.0282 - dense_20_loss: 2.3841\nEpoch 386/500\n446/446 [==============================] - 7s 15ms/step - loss: 50.4143 - dense_19_loss: 11.8297 - dense_20_loss: 2.4494\nEpoch 387/500\n446/446 [==============================] - 7s 16ms/step - loss: 50.0085 - dense_19_loss: 11.6024 - dense_20_loss: 2.4302\nEpoch 388/500\n446/446 [==============================] - 7s 15ms/step - loss: 50.8033 - dense_19_loss: 12.4193 - dense_20_loss: 2.4735\nEpoch 389/500\n446/446 [==============================] - 7s 15ms/step - loss: 49.4093 - dense_19_loss: 11.2183 - dense_20_loss: 2.4658\nEpoch 390/500\n446/446 [==============================] - 7s 15ms/step - loss: 49.5633 - dense_19_loss: 11.5479 - dense_20_loss: 2.4551\nEpoch 391/500\n446/446 [==============================] - 7s 15ms/step - loss: 49.6981 - dense_19_loss: 11.6234 - dense_20_loss: 2.5541\nEpoch 392/500\n446/446 [==============================] - 7s 17ms/step - loss: 49.9361 - dense_19_loss: 12.0130 - dense_20_loss: 2.5546\nEpoch 393/500\n446/446 [==============================] - 7s 15ms/step - loss: 50.6553 - dense_19_loss: 12.7728 - dense_20_loss: 2.5375\nEpoch 394/500\n446/446 [==============================] - 7s 15ms/step - loss: 50.5672 - dense_19_loss: 12.7053 - dense_20_loss: 2.6058\nEpoch 395/500\n446/446 [==============================] - 7s 15ms/step - loss: 50.1786 - dense_19_loss: 12.5816 - dense_20_loss: 2.5087\nEpoch 396/500\n446/446 [==============================] - 7s 16ms/step - loss: 50.7298 - dense_19_loss: 13.0110 - dense_20_loss: 2.5757\nEpoch 397/500\n446/446 [==============================] - 7s 15ms/step - loss: 50.8666 - dense_19_loss: 13.2575 - dense_20_loss: 2.5233\nEpoch 398/500\n446/446 [==============================] - 7s 15ms/step - loss: 49.6398 - dense_19_loss: 12.2059 - dense_20_loss: 2.5062\nEpoch 399/500\n446/446 [==============================] - 7s 15ms/step - loss: 49.5169 - dense_19_loss: 12.2142 - dense_20_loss: 2.4674\nEpoch 400/500\n446/446 [==============================] - 7s 15ms/step - loss: 50.0036 - dense_19_loss: 12.6962 - dense_20_loss: 2.5381\nEpoch 401/500\n446/446 [==============================] - 7s 16ms/step - loss: 48.5095 - dense_19_loss: 11.4999 - dense_20_loss: 2.3852\nEpoch 402/500\n446/446 [==============================] - 7s 15ms/step - loss: 49.2194 - dense_19_loss: 12.1868 - dense_20_loss: 2.4704\nEpoch 403/500\n446/446 [==============================] - 7s 15ms/step - loss: 51.2115 - dense_19_loss: 13.8681 - dense_20_loss: 2.7763\nEpoch 404/500\n446/446 [==============================] - 7s 15ms/step - loss: 50.9033 - dense_19_loss: 13.4901 - dense_20_loss: 2.7652\nEpoch 405/500\n446/446 [==============================] - 7s 16ms/step - loss: 59.8283 - dense_19_loss: 20.9246 - dense_20_loss: 3.5396\nEpoch 406/500\n446/446 [==============================] - 7s 15ms/step - loss: 49.3019 - dense_19_loss: 11.8124 - dense_20_loss: 2.4760\nEpoch 407/500\n446/446 [==============================] - 7s 15ms/step - loss: 47.4090 - dense_19_loss: 10.4823 - dense_20_loss: 2.2925\nEpoch 408/500\n446/446 [==============================] - 7s 15ms/step - loss: 47.0446 - dense_19_loss: 10.4294 - dense_20_loss: 2.2463\nEpoch 409/500\n446/446 [==============================] - 7s 15ms/step - loss: 46.7337 - dense_19_loss: 10.3006 - dense_20_loss: 2.2752\nEpoch 410/500\n446/446 [==============================] - 7s 16ms/step - loss: 46.9596 - dense_19_loss: 10.6057 - dense_20_loss: 2.3442\nEpoch 411/500\n446/446 [==============================] - 7s 15ms/step - loss: 47.5207 - dense_19_loss: 11.2815 - dense_20_loss: 2.3593\nEpoch 412/500\n446/446 [==============================] - 7s 15ms/step - loss: 49.4437 - dense_19_loss: 12.9939 - dense_20_loss: 2.4994\nEpoch 413/500\n446/446 [==============================] - 7s 15ms/step - loss: 49.9257 - dense_19_loss: 13.3576 - dense_20_loss: 2.6426\nEpoch 414/500\n446/446 [==============================] - 7s 15ms/step - loss: 47.7055 - dense_19_loss: 11.4747 - dense_20_loss: 2.4499\nEpoch 415/500\n446/446 [==============================] - 7s 16ms/step - loss: 47.5310 - dense_19_loss: 11.5176 - dense_20_loss: 2.3605\nEpoch 416/500\n446/446 [==============================] - 7s 15ms/step - loss: 47.3144 - dense_19_loss: 11.3773 - dense_20_loss: 2.4021\nEpoch 417/500\n446/446 [==============================] - 7s 15ms/step - loss: 47.1640 - dense_19_loss: 11.2698 - dense_20_loss: 2.4349\nEpoch 418/500\n446/446 [==============================] - 7s 15ms/step - loss: 48.2180 - dense_19_loss: 12.3288 - dense_20_loss: 2.4448\nEpoch 419/500\n446/446 [==============================] - 7s 16ms/step - loss: 47.1778 - dense_19_loss: 11.4708 - dense_20_loss: 2.4052\nEpoch 420/500\n446/446 [==============================] - 7s 15ms/step - loss: 47.0719 - dense_19_loss: 11.5160 - dense_20_loss: 2.3907\nEpoch 421/500\n446/446 [==============================] - 7s 15ms/step - loss: 46.7367 - dense_19_loss: 11.3150 - dense_20_loss: 2.3910\nEpoch 422/500\n446/446 [==============================] - 7s 15ms/step - loss: 48.7958 - dense_19_loss: 12.9779 - dense_20_loss: 2.6688\nEpoch 423/500\n446/446 [==============================] - 7s 15ms/step - loss: 46.8600 - dense_19_loss: 11.4984 - dense_20_loss: 2.3324\nEpoch 424/500\n446/446 [==============================] - 7s 16ms/step - loss: 46.2249 - dense_19_loss: 11.0378 - dense_20_loss: 2.3253\nEpoch 425/500\n446/446 [==============================] - 7s 15ms/step - loss: 46.7712 - dense_19_loss: 11.5749 - dense_20_loss: 2.3828\nEpoch 426/500\n446/446 [==============================] - 7s 15ms/step - loss: 49.2216 - dense_19_loss: 13.7109 - dense_20_loss: 2.6378\nEpoch 427/500\n446/446 [==============================] - 7s 15ms/step - loss: 48.3986 - dense_19_loss: 12.9133 - dense_20_loss: 2.5802\nEpoch 428/500\n446/446 [==============================] - 7s 15ms/step - loss: 46.7250 - dense_19_loss: 11.5494 - dense_20_loss: 2.4121\nEpoch 429/500\n446/446 [==============================] - 7s 16ms/step - loss: 46.1791 - dense_19_loss: 11.2090 - dense_20_loss: 2.3408\nEpoch 430/500\n446/446 [==============================] - 7s 15ms/step - loss: 45.2707 - dense_19_loss: 10.6138 - dense_20_loss: 2.2021\nEpoch 431/500\n446/446 [==============================] - 7s 16ms/step - loss: 46.1157 - dense_19_loss: 11.3803 - dense_20_loss: 2.3404\nEpoch 432/500\n446/446 [==============================] - 7s 15ms/step - loss: 46.6760 - dense_19_loss: 11.7629 - dense_20_loss: 2.4539\nEpoch 433/500\n446/446 [==============================] - 7s 16ms/step - loss: 46.3120 - dense_19_loss: 11.6926 - dense_20_loss: 2.3425\nEpoch 434/500\n446/446 [==============================] - 7s 15ms/step - loss: 45.5076 - dense_19_loss: 11.0277 - dense_20_loss: 2.3249\nEpoch 435/500\n446/446 [==============================] - 7s 16ms/step - loss: 44.9875 - dense_19_loss: 10.6351 - dense_20_loss: 2.3534\nEpoch 436/500\n446/446 [==============================] - 7s 16ms/step - loss: 46.3115 - dense_19_loss: 11.8395 - dense_20_loss: 2.4151\nEpoch 437/500\n446/446 [==============================] - 7s 15ms/step - loss: 45.9773 - dense_19_loss: 11.7685 - dense_20_loss: 2.3082\nEpoch 438/500\n446/446 [==============================] - 7s 16ms/step - loss: 47.8082 - dense_19_loss: 13.3009 - dense_20_loss: 2.4993\nEpoch 439/500\n446/446 [==============================] - 7s 16ms/step - loss: 45.3101 - dense_19_loss: 11.0168 - dense_20_loss: 2.3297\nEpoch 440/500\n446/446 [==============================] - 7s 16ms/step - loss: 45.5173 - dense_19_loss: 11.4835 - dense_20_loss: 2.2405\nEpoch 441/500\n446/446 [==============================] - 7s 16ms/step - loss: 46.7187 - dense_19_loss: 12.3711 - dense_20_loss: 2.4011\nEpoch 442/500\n446/446 [==============================] - 7s 16ms/step - loss: 53.0442 - dense_19_loss: 17.9428 - dense_20_loss: 2.9511\nEpoch 443/500\n446/446 [==============================] - 7s 15ms/step - loss: 58.5344 - dense_19_loss: 22.1339 - dense_20_loss: 3.4616\nEpoch 444/500\n446/446 [==============================] - 7s 15ms/step - loss: 48.5244 - dense_19_loss: 13.1190 - dense_20_loss: 2.6005\nEpoch 445/500\n446/446 [==============================] - 7s 15ms/step - loss: 45.1719 - dense_19_loss: 10.5694 - dense_20_loss: 2.1910\nEpoch 446/500\n446/446 [==============================] - 7s 15ms/step - loss: 45.9135 - dense_19_loss: 11.3720 - dense_20_loss: 2.2709\nEpoch 447/500\n446/446 [==============================] - 7s 16ms/step - loss: 43.4799 - dense_19_loss: 9.4065 - dense_20_loss: 2.0926\nEpoch 448/500\n446/446 [==============================] - 7s 15ms/step - loss: 43.4350 - dense_19_loss: 9.6033 - dense_20_loss: 2.0971\nEpoch 449/500\n446/446 [==============================] - 7s 15ms/step - loss: 43.2547 - dense_19_loss: 9.6926 - dense_20_loss: 2.0440\nEpoch 450/500\n446/446 [==============================] - 7s 15ms/step - loss: 43.5815 - dense_19_loss: 10.0537 - dense_20_loss: 2.1339\nEpoch 451/500\n446/446 [==============================] - 7s 16ms/step - loss: 43.8353 - dense_19_loss: 10.3814 - dense_20_loss: 2.1789\nEpoch 452/500\n446/446 [==============================] - 7s 15ms/step - loss: 44.5324 - dense_19_loss: 11.0492 - dense_20_loss: 2.2486\nEpoch 453/500\n446/446 [==============================] - 7s 15ms/step - loss: 44.7991 - dense_19_loss: 11.2186 - dense_20_loss: 2.3824\nEpoch 454/500\n446/446 [==============================] - 7s 15ms/step - loss: 48.0241 - dense_19_loss: 13.9671 - dense_20_loss: 2.6256\nEpoch 455/500\n446/446 [==============================] - 7s 15ms/step - loss: 44.5757 - dense_19_loss: 10.9157 - dense_20_loss: 2.2827\nEpoch 456/500\n446/446 [==============================] - 7s 16ms/step - loss: 42.9527 - dense_19_loss: 9.7577 - dense_20_loss: 2.1537\nEpoch 457/500\n446/446 [==============================] - 7s 15ms/step - loss: 43.4552 - dense_19_loss: 10.3773 - dense_20_loss: 2.1730\nEpoch 458/500\n446/446 [==============================] - 7s 15ms/step - loss: 43.4364 - dense_19_loss: 10.5048 - dense_20_loss: 2.1453\nEpoch 459/500\n446/446 [==============================] - 7s 15ms/step - loss: 43.8351 - dense_19_loss: 10.9178 - dense_20_loss: 2.1958\nEpoch 460/500\n446/446 [==============================] - 7s 15ms/step - loss: 43.4266 - dense_19_loss: 10.6443 - dense_20_loss: 2.1628\nEpoch 461/500\n446/446 [==============================] - 7s 16ms/step - loss: 43.6353 - dense_19_loss: 10.8754 - dense_20_loss: 2.2250\nEpoch 462/500\n446/446 [==============================] - 7s 15ms/step - loss: 43.9173 - dense_19_loss: 11.2555 - dense_20_loss: 2.1795\nEpoch 463/500\n446/446 [==============================] - 7s 15ms/step - loss: 43.8483 - dense_19_loss: 11.1498 - dense_20_loss: 2.1937\nEpoch 464/500\n446/446 [==============================] - 7s 15ms/step - loss: 47.4452 - dense_19_loss: 14.2793 - dense_20_loss: 2.5283\nEpoch 465/500\n446/446 [==============================] - 7s 16ms/step - loss: 49.4994 - dense_19_loss: 15.6100 - dense_20_loss: 2.8948\nEpoch 466/500\n446/446 [==============================] - 7s 15ms/step - loss: 46.8441 - dense_19_loss: 13.2588 - dense_20_loss: 2.5462\nEpoch 467/500\n446/446 [==============================] - 7s 15ms/step - loss: 43.2176 - dense_19_loss: 10.2774 - dense_20_loss: 2.2016\nEpoch 468/500\n446/446 [==============================] - 7s 15ms/step - loss: 42.7738 - dense_19_loss: 10.2289 - dense_20_loss: 2.0545\nEpoch 469/500\n446/446 [==============================] - 7s 15ms/step - loss: 42.6172 - dense_19_loss: 10.1721 - dense_20_loss: 2.1049\nEpoch 470/500\n446/446 [==============================] - 7s 16ms/step - loss: 45.2420 - dense_19_loss: 12.3620 - dense_20_loss: 2.3962\nEpoch 471/500\n446/446 [==============================] - 7s 15ms/step - loss: 42.3165 - dense_19_loss: 9.9695 - dense_20_loss: 2.0889\nEpoch 472/500\n446/446 [==============================] - 7s 15ms/step - loss: 41.7967 - dense_19_loss: 9.6816 - dense_20_loss: 2.0521\nEpoch 473/500\n446/446 [==============================] - 7s 15ms/step - loss: 42.1722 - dense_19_loss: 10.0277 - dense_20_loss: 2.1492\nEpoch 474/500\n446/446 [==============================] - 7s 16ms/step - loss: 42.3767 - dense_19_loss: 10.3898 - dense_20_loss: 2.1233\nEpoch 475/500\n446/446 [==============================] - 7s 16ms/step - loss: 42.9194 - dense_19_loss: 10.9276 - dense_20_loss: 2.1840\nEpoch 476/500\n446/446 [==============================] - 7s 15ms/step - loss: 43.9059 - dense_19_loss: 11.7707 - dense_20_loss: 2.2664\nEpoch 477/500\n446/446 [==============================] - 7s 15ms/step - loss: 49.2681 - dense_19_loss: 16.4333 - dense_20_loss: 2.7251\nEpoch 478/500\n446/446 [==============================] - 7s 15ms/step - loss: 48.1463 - dense_19_loss: 14.6908 - dense_20_loss: 2.7131\nEpoch 479/500\n446/446 [==============================] - 7s 17ms/step - loss: 43.1960 - dense_19_loss: 10.6562 - dense_20_loss: 2.2334\nEpoch 480/500\n446/446 [==============================] - 7s 15ms/step - loss: 41.3572 - dense_19_loss: 9.3601 - dense_20_loss: 2.0063\nEpoch 481/500\n446/446 [==============================] - 7s 15ms/step - loss: 40.6552 - dense_19_loss: 9.0023 - dense_20_loss: 1.9277\nEpoch 482/500\n446/446 [==============================] - 7s 15ms/step - loss: 41.7630 - dense_19_loss: 9.9687 - dense_20_loss: 2.1468\nEpoch 483/500\n446/446 [==============================] - 7s 16ms/step - loss: 41.4466 - dense_19_loss: 9.8877 - dense_20_loss: 2.0577\nEpoch 484/500\n446/446 [==============================] - 7s 16ms/step - loss: 41.7167 - dense_19_loss: 10.2429 - dense_20_loss: 2.0683\nEpoch 485/500\n446/446 [==============================] - 7s 15ms/step - loss: 41.4001 - dense_19_loss: 10.0448 - dense_20_loss: 2.0507\nEpoch 486/500\n446/446 [==============================] - 7s 15ms/step - loss: 41.1096 - dense_19_loss: 9.8253 - dense_20_loss: 2.0889\nEpoch 487/500\n446/446 [==============================] - 7s 15ms/step - loss: 41.8964 - dense_19_loss: 10.5250 - dense_20_loss: 2.1158\nEpoch 488/500\n446/446 [==============================] - 7s 16ms/step - loss: 43.2242 - dense_19_loss: 11.7540 - dense_20_loss: 2.2463\nEpoch 489/500\n446/446 [==============================] - 7s 15ms/step - loss: 42.4532 - dense_19_loss: 11.0219 - dense_20_loss: 2.2126\nEpoch 490/500\n446/446 [==============================] - 7s 16ms/step - loss: 41.2416 - dense_19_loss: 10.1449 - dense_20_loss: 2.0573\nEpoch 491/500\n446/446 [==============================] - 7s 15ms/step - loss: 40.4713 - dense_19_loss: 9.5773 - dense_20_loss: 2.0110\nEpoch 492/500\n446/446 [==============================] - 7s 15ms/step - loss: 41.2708 - dense_19_loss: 10.3184 - dense_20_loss: 2.1405\nEpoch 493/500\n446/446 [==============================] - 7s 17ms/step - loss: 41.4182 - dense_19_loss: 10.4922 - dense_20_loss: 2.1109\nEpoch 494/500\n446/446 [==============================] - 7s 15ms/step - loss: 41.1858 - dense_19_loss: 10.4135 - dense_20_loss: 2.0697\nEpoch 495/500\n446/446 [==============================] - 7s 15ms/step - loss: 43.8525 - dense_19_loss: 12.6961 - dense_20_loss: 2.2896\nEpoch 496/500\n446/446 [==============================] - 7s 15ms/step - loss: 41.0565 - dense_19_loss: 10.3318 - dense_20_loss: 2.0382\nEpoch 497/500\n446/446 [==============================] - 7s 15ms/step - loss: 41.8719 - dense_19_loss: 11.0967 - dense_20_loss: 2.1373\nEpoch 498/500\n446/446 [==============================] - 7s 16ms/step - loss: 40.4729 - dense_19_loss: 9.8486 - dense_20_loss: 2.0542\nEpoch 499/500\n446/446 [==============================] - 7s 15ms/step - loss: 40.1081 - dense_19_loss: 9.7192 - dense_20_loss: 1.9957\nEpoch 500/500\n446/446 [==============================] - 7s 15ms/step - loss: 39.5359 - dense_19_loss: 9.3566 - dense_20_loss: 1.9516\nmsynergy_mean_squared_error 202.74678084359118\nmsenstivity_mean_squared_error 13.355789918261111\nmsynergy_mean_absolute_error 9.454906100417872\nmsenstivity_mean_absolute_error 2.590778897992515\nmsynergy_r2_score 0.6466560144031828\nmsenstivity_r2_score 0.9140031077328885\n114/114 [==============================] - 2s 8ms/step - loss: 244.3194 - dense_19_loss: 202.7467 - dense_20_loss: 13.3558\n[244.31944274902344, 202.7467498779297, 13.355790138244629]\nmsynergy_pear (array([0.8043234538174125], dtype=object), 0.0)\nmsynergy_spear SpearmanrResult(correlation=0.7502485219540712, pvalue=0.0)\nmsenstivity_pear (array([0.9563708321101344], dtype=object), 0.0)\nmsenstivity_spear SpearmanrResult(correlation=0.9569299623823122, pvalue=0.0)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(array([[-16.30978 ],\n        [ 12.797248],\n        [-11.640114],\n        ...,\n        [  9.536682],\n        [ 11.272397],\n        [ 18.919746]], dtype=float32),\n array([[28.838797],\n        [ 5.856005],\n        [34.61385 ],\n        ...,\n        [12.211086],\n        [36.028965],\n        [19.641037]], dtype=float32))"},"metadata":{}}]},{"cell_type":"code","source":"\nfrom IPython.display import FileLink\nnp.savetxt('pred_syn4.csv', ap111 ,delimiter=',')\nFileLink(r'pred_syn4.csv')\n\nnp.savetxt('pred_sen4.csv', ap221 ,delimiter=',')\nFileLink(r'pred_sen4.csv')\n\nnp.savetxt('test_syn4.csv', test_synergy ,delimiter=',')\nFileLink(r'test_syn4.csv')\n\nnp.savetxt('test_sen4.csv', test_senstivity ,delimiter=',')\nFileLink(r'test_sen4.csv')","metadata":{"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/test_sen4.csv","text/html":"<a href='test_sen4.csv' target='_blank'>test_sen4.csv</a><br>"},"metadata":{}}]}]}