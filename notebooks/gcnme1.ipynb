{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install deepchem\n!pip install pysmiles\n!pip install openpyxl\n# !pip install rdkit\n\n# !pip install MolGraphConvFeaturizer\n!pip install PubChemPy\n!pip install PyDrive\n!pip install xlrd==2.0.1\n!pip install Pandas==1.3.5\n# !pip install Pandas==1.1.5\n!pip install  gdown\nimport deepchem as dc\nfrom rdkit import Chem\nprint(\"hjjbjh\")\nfrom pysmiles import read_smiles\nimport networkx as nx\nfrom deepchem.feat import MolGraphConvFeaturizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n#   gauth = GoogleAuth()\n#   gauth.credentials = GoogleCredentials.get_application_default()\n#   drivea = GoogleDrive(gauth)\n#   drive.mount('/content/drive')\ndef normalize1(X, means1=None, std1=None, means2=None, std2=None, feat_filt=None, norm='tanh_norm'):\n    if std1 is None:\n        std1 = np.nanstd(X, axis=0)\n    if feat_filt is None:\n        feat_filt = std1!=0\n    X = X[:,feat_filt]\n    X = np.ascontiguousarray(X)\n    if means1 is None:\n        means1 = np.mean(X, axis=0)\n    X = (X-means1)/std1[feat_filt]\n    if norm == 'norm':\n        return(X, means1, std1, feat_filt)\n    elif norm == 'tanh':\n        return(np.tanh(X), means1, std1, feat_filt)\n    elif norm == 'tanh_norm':\n        X = np.tanh(X)\n        if means2 is None:\n            means2 = np.mean(X, axis=0)\n        if std2 is None:\n            std2 = np.std(X, axis=0)\n        X = (X-means2)/std2\n        X[:,std2==0]=0\n        return(X, means1, std1, means2, std2, feat_filt) \n    \n    \ndef get_data():\n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  smiles=pd.read_excel('pubchem.xls', header=None)\n  smiles=np.array(smiles)\n\n#   !gdown https://drive.google.com/uc?id=15bNKK7tacCJIFzvt5y4WfU6uKbPdYtA6\n  !gdown --id 1z1sWG4E9BqipP0iczLrbR4qxSa4KG95_\n  data_to_repeat=pd.read_excel('pcbi.1006752.s004.xls', header=None)\n  data_to_repeat=np.array(data_to_repeat)\n  \n#   !gdown --id 1TThHsLyORlcHuEBgOad20SQUYH88O9mm\n#   data_to_repeat=pd.read_excel('labels1.xlsx', header=None)\n#   data_to_repeat=np.array(data_to_repeat)\n    \n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  unique_drugs=pd.read_excel('pubchem.xls', header=None)\n  unique_drugs=np.array(unique_drugs)         \n\n#   !gdown https://drive.google.com/uc?id=1IMr5zMLRAXC5iE2MAHJbKmHf2-0DCZNd\n# #   !gdown --id 1bBJUFBA4Tm9YdE5OxA1mbUfBI8B7wqcr\n#   feature_cell=pd.read_excel('final_feature_cell.xlsx', header=None)\n#   feature_cell=np.array(feature_cell)\n\n  !gdown --id 1NoUKCRZ0CkNVFP1-578x_L0otPUMkz6T\n  feature_cell=pd.read_excel('unique934_cell.xlsx',header=None)\n  feature_cell=np.array(feature_cell)\n  a=np.zeros((1,934))\n  feature_cell[22,1:]=a\n  feature_cell[36,1:]=a\n#   !gdown --id 109nyFVOO_P9DdyrhWzbNg0FBB_Y8gD58\n#   feature_cell=pd.read_excel('final_deep_cell.xlsx',header=None)\n#   feature_cell=np.array(feature_cell)\n\n#   !gdown https://drive.google.com/uc?id=1YTe0v5PzwjlgPqo3OTU4FMghOFAHgeeC\n  !gdown --id 1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\n  deleted_index=pd.read_excel('deleted_index.xls', header=None)\n  deleted_index=np.array(deleted_index)  \n\n  return smiles[1:,1],data_to_repeat[1:,:],unique_drugs[1:,0],feature_cell,deleted_index\n\n\ndef atom_features(atom):\n    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n                                          ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As',\n                                           'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se',\n                                           'Ti', 'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n                                           'Pt', 'Hg', 'Pb', 'Unknown']) +\n                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    [atom.GetIsAromatic()])\n\n\ndef one_of_k_encoding(x, allowable_set):\n    if x not in allowable_set:\n        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))\n\n\ndef one_of_k_encoding_unk(x, allowable_set):\n    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\n\n\ndef smile_to_graph(smile):\n    mol = Chem.MolFromSmiles(smile)\n\n    c_size = mol.GetNumAtoms()\n\n    features = []\n    for atom in mol.GetAtoms():\n        feature = atom_features(atom)\n        features.append(feature / sum(feature))\n\n    edges = []\n    for bond in mol.GetBonds():\n        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n    g = nx.Graph(edges).to_directed()\n    edge_index = []\n    for e1, e2 in g.edges:\n        edge_index.append([e1, e2])\n\n    return c_size, features, edge_index\n\n\ndef graph_node(smiles):\n  node=[]\n  adj=[]\n  max=0\n  final_feature=[]\n  final_adjacency=[]\n  for i  in range(len(smiles)):\n     m=smiles[i]\n    #  aa=m#.item((0))\n     featurizer = MolGraphConvFeaturizer(use_edges=True,use_chirality=True)\n     out = featurizer.featurize(m)\n     feature=out[0].node_features\n        \n#      featurizer = DMPNNFeaturizer()\n#      out = featurizer.featurize(m)\n#      feature=out[0].node_features\n    \n     ma=feature.shape[0]\n     n=feature.shape[1]\n     if(ma>max):\n       max=ma\n    # out[0].edge_featuwres.shape\n     molecules=read_smiles(m)\n     adjacency1=nx.to_numpy_array(molecules)\n     node.append(feature)\n     adj.append(adjacency1)\n#   f_n=np.zeros((max,n))\n#   f_a=np.zeros((max,max))\n  feature=[]\n  adjacency=[]\n  for i  in range(len(smiles)):\n    n1=node[i]\n    a=adj[i]\n    f_n=np.zeros((max,n))\n    f_a=np.zeros((max,max))\n    f_n[0:n1.shape[0],]=n1\n    f_a[0:n1.shape[0],0:n1.shape[0]]=a\n    final_feature.append(f_n)\n    final_adjacency.append(f_a)\n\n\n  return final_feature,final_adjacency \n\n\ndef graph_node_edge(smiles):\n  node=[]\n  adj=[]\n  max=0\n  max1=0\n  final_feature=[]\n  final_adjacency=[]\n  for i  in range(len(smiles)):\n     m=smiles[i]\n    #  aa=m#.item((0))\n    #  featurizer = MolGraphConvFeaturizer(use_edges=True,use_chirality=True)\n    #  out = featurizer.featurize(m)\n    #  feature=out[0].node_features\n     num_atom,feature,edge_index=smile_to_graph(m)  \n#      featurizer = DMPNNFeaturizer()\n#      out = featurizer.featurize(m)\n#      feature=out[0].node_features\n     feature=np.stack( feature, axis=0)\n     edge_index=np.stack( edge_index, axis=0)\n    \n     ma=num_atom\n     n=feature.shape[1]\n     if(ma>max):\n       max=ma\n    # out[0].edge_featuwres.shape\n     node.append(feature)\n\n     \n     ne=edge_index.shape[0]\n     if(ne>max1):\n       max1=ne\n    # out[0].edge_featuwres.shape\n     adj.append(edge_index)\n#   f_n=np.zeros((max,n))\n#   f_a=np.zeros((max,max))\n  feature=[]\n  adjacency=[]\n  for i  in range(len(smiles)):\n    n1=node[i]\n    a=adj[i]\n    f_n=np.zeros((max,n))\n    f_n[0:n1.shape[0],]=n1\n    f_a=np.zeros((max1,2))\n    f_a[0:a.shape[0],]=a\n    final_feature.append(f_n)\n    final_adjacency.append(f_a)\n\n  return final_feature,final_adjacency \n\n\ndef repeat_smiles(data_to_repeat,unique_drugs,feature,adjacency,deleted_index,synergy,senstivity):\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  synergy1=[]\n  senstivity1=[]\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    if(i not in deleted_index):\n      f_drug1.append(feature[k1[0]])\n      f_drug2.append(feature[k2[0]])\n      a_drug1.append(adjacency[k1[0]])\n      a_drug2.append(adjacency[k2[0]])\n      synergy1.append(synergy[i,])\n      senstivity1.append(senstivity[i,])\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,synergy1,senstivity1\n\ndef repeat_smiles1(data_to_repeat,unique_drugs,feature,adjacency,unique_cell):\n  unique_feature=unique_cell[:,1:]\n  unique_name=unique_cell[:,0]\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  c=data_to_repeat[:,2]\n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  feature_cell=[]\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    cc=c[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    cc1=[m for m, v in enumerate(unique_name) if cc in v]\n    f_drug1.append(feature[k1[0]])\n    f_drug2.append(feature[k2[0]])\n    a_drug1.append(adjacency[k1[0]])\n    a_drug2.append(adjacency[k2[0]])\n    feature_cell.append(unique_feature[cc1[0]])\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,feature_cell \n\ndef repeat_smiles_deep(data_to_repeat,unique_drugs,feature,adjacency,unique_cell):\n  unique_feature=unique_cell[:,1:]\n  unique_name=unique_cell[:,0]\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  c=data_to_repeat[:,2]\n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  feature_cell=[]\n  f=np.zeros((feature[0].shape[0],feature[0].shape[1]))\n  a=np.zeros((adjacency[0].shape[0],adjacency[0].shape[1]))\n  cell=np.zeros((unique_cell[0].shape[0]))\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    cc=c[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    cc1=[m for m, v in enumerate(unique_name) if cc in v]\n    if(cc1):\n        f_drug1.append(feature[k1[0]])\n        f_drug2.append(feature[k2[0]])\n        a_drug1.append(adjacency[k1[0]])\n        a_drug2.append(adjacency[k2[0]])\n        feature_cell.append(unique_feature[cc1[0]])\n    else:\n        f_drug1.append(f)\n        f_drug2.append(f)\n        a_drug1.append(a)\n        a_drug2.append(a)\n        feature_cell.append(cell)\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,feature_cell\n\ndef split_data(feature_cell,synergy,senstivity):\n  row=feature_cell.shape[0]\n  col=feature_cell.shape[1]\n  input1=np.zeros((row,col+1))\n  # a=np.array('i')\n  for i in range(len(feature_cell)):\n    input1[i,0:col]=feature_cell[i,:]\n    input1[i,col]=i\n\n\n  index_train=[]\n  index_test=[]\n  output = np.c_[synergy,senstivity ]\n#   dataset1=np.c_[input11,output1]\n#   dataset=np.random.shuffle(dataset1)\n#   input1=dataset[:,0:len(dataset[0])-2]\n#   output=dataset[:,-2:]\n  x_train, x_test, y_train, y_test = train_test_split(input1, output,test_size=.20)\n  train_synergy=y_train[:,0]\n  train_senstivity=y_train[:,1]\n  test_synergy=y_test[:,0]\n  test_senstivity=y_test[:,1]\n  for i in range(len(x_train)):#.shape[0]):\n    for j in range(len(input1)):#.shape[0]):\n      if(all(x_train[i]==input1[j])):\n        index_train.append(j)\n        \n\n  for ii in range(len(x_test)):#.shape[0]):\n    for jj in range(len(input1)):#.shape[0]):\n      if(all(x_test[ii]==input1[jj])):\n        index_test.append(jj)\n        \n\n  return  train_synergy, train_senstivity,test_synergy, test_senstivity,index_train,index_test\n\n\ndef split_data1(feature_cell,synergy,senstivity):\n  row=feature_cell.shape[0]\n  col=feature_cell.shape[1]\n  input1=np.zeros((row,col+1))\n  # a=np.array('i')\n  k=0\n  for i in range(len(feature_cell)):\n    if((senstivity[i])):\n        input1[k,0:col]=feature_cell[i,:]\n        input1[k,col]=i\n        k=k+1\n\n\n  index_train=[]\n  index_test=[]\n  output=[]\n  for i in range(len(feature_cell)):\n    if((senstivity[i])):\n      output.append (np.c_[synergy[i],senstivity[i] ])\n#   dataset1=np.c_[input11,output1]\n#   dataset=np.random.shuffle(dataset1)\n#   input1=dataset[:,0:len(dataset[0])-2]\n#   output=dataset[:,-2:]\n  output= np.array(output)\n  x_train, x_test, y_train, y_test = train_test_split(input1, output,test_size=.20)\n  train_synergy=y_train[:,0]\n  train_senstivity=y_train[:,1]\n  test_synergy=y_test[:,0]\n  test_senstivity=y_test[:,1]\n  for i in range(len(x_train)):#.shape[0]):\n    for j in range(len(input1)):#.shape[0]):\n      if(all(x_train[i]==input1[j])):\n        index_train.append(j)\n        \n\n  for ii in range(len(x_test)):#.shape[0]):\n    for jj in range(len(input1)):#.shape[0]):\n      if(all(x_test[ii]==input1[jj])):\n        index_test.append(jj)\n        \n\n  return  index_train,index_test\n\n\ndef index(m):\n    if m==1:\n      !gdown --id 1b82ry7sSPqPIcJSr-i4rmC9XYp7Z1CpH\n      test_ind=pd.read_excel('ind1_test.xls', header=None)\n      test_ind=np.array(test_ind)\n    \n      !gdown --id 1aAaYK1EG5AVlWtwHlsajVJo4Qx-u2ig3\n      train_ind=pd.read_excel('ind1_train.xls', header=None)\n      train_ind=np.array(train_ind)\n\n    \n    if m==2:\n        !gdown --id 1EXKurPZ5ScjiZM0OG-bSv7IPXyKKW6B5\n        test_ind=pd.read_excel('ind2_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1-rayr-tZdwX4DDtSHChkoSdgEK1P3jWN\n        train_ind=pd.read_excel('ind2_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    if m==3:\n        !gdown --id 19O4yKBUCAyPrvdUne2IhwOY73A4y7pzT\n        test_ind=pd.read_excel('ind3_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1YNZBGdL--Ww9ZSGWUKmeC26DdJDywrR4\n        train_ind=pd.read_excel('ind3_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==4:\n        !gdown --id 1sJ3ksFbMOQoBTqNw5Ddv7bGgD9YESRGn\n        test_ind=pd.read_excel('ind4_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1-P14BxLrpnYS-9TbsvyATKtFTFG7QsLO\n        train_ind=pd.read_excel('ind4_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==5:\n        !gdown --id 1iZo5wJgiUOGBAVRy0Hw6vXngFHe9ciFH\n        test_ind=pd.read_excel('ind5_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1xlFp8g1gDGPa_Sf0zgrPWm3hnRCSkAxp\n        train_ind=pd.read_excel('ind5_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    return test_ind,train_ind\n\n\ndef index1(m):\n    if m==1:\n      !gdown --id 10a4cV1tOqomBo7RzWqnbs6j5-RGcBNjS\n      test_ind=pd.read_excel('another_ind1_test.xls', header=None)\n      test_ind=np.array(test_ind)\n    \n      !gdown --id 1Kn8Y2iGCH7DLwq72s_QmouA0Db1WBfVR\n      train_ind=pd.read_excel('another_ind1_train.xls', header=None)\n      train_ind=np.array(train_ind)\n\n    \n    if m==2:\n        !gdown --id 1WR4kDyNO6Xo-P-z3O2q6M_tHr6Soj6NZ\n        test_ind=pd.read_excel('another_ind2_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1aegtCzmVg9znXPTUvpzf2zLkmcjEeQ22\n        train_ind=pd.read_excel('another_ind2_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    if m==3:\n        !gdown --id 1W29vAF6mOoKmacbibxzTNQCfp42VVYYN\n        test_ind=pd.read_excel('another_ind3_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1B8h0ewSo4XJCvSWBdS5oiQiwtAL3H72T\n        train_ind=pd.read_excel('another_ind3_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==4:\n        !gdown --id 1tUXsGGT_-JdJTpT5sYEBB3zNFnh60tJR\n        test_ind=pd.read_excel('another_ind4_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1T9H0O6bzr2d01cPnSyE-vYk_9doKKBHZ\n        train_ind=pd.read_excel('another_ind4_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==5:\n        !gdown --id 1p1kOBJQRhsavnOwgeb_3z-gBqLSS08sE\n        test_ind=pd.read_excel('another_ind5_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1hhw-ITuxUnKwTlLveS1MJER_KM580SXa\n        train_ind=pd.read_excel('another_ind5_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    return test_ind,train_ind\n\n\ndef train_test_input(f_drug1,a_drug1,f_drug2,a_drug2,cell_line,index_train,index_test,synery,senstivity):\n  train_f_drug1=[]\n  train_a_drug1=[]\n  train_f_drug2=[]\n  train_a_drug2=[]\n  train_cell_line=[]\n  train_synergy=[]\n  train_senstivity=[]\n  test_f_drug1=[]\n  test_a_drug1=[]\n  test_f_drug2=[]\n  test_a_drug2=[]\n  test_cell_line=[]\n  test_synergy=[]\n  test_senstivity=[]\n  for i in range(len(index_train)):\n      train_a_drug1.append(a_drug1[index_train[i]])\n      train_a_drug2.append(a_drug2[index_train[i]])\n      train_f_drug1.append(f_drug1[index_train[i]])\n      train_f_drug2.append(f_drug2[index_train[i]])\n      train_cell_line.append(cell_line[index_train[i]])\n      train_synergy.append(synergy[index_train[i]])\n      train_senstivity.append(senstivity[index_train[i]])\n  for ii in range(len(index_test)):\n      test_a_drug1.append(a_drug1[index_test[ii]])\n      test_a_drug2.append(a_drug2[index_test[ii]])\n      test_f_drug1.append(f_drug1[index_test[ii]])\n      test_f_drug2.append(f_drug2[index_test[ii]])\n      test_cell_line.append(cell_line[index_test[ii]])\n      test_synergy.append(synergy[index_test[ii]])\n      test_senstivity.append(senstivity[index_test[ii]])\n\n  return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity\n\ndef train_test_input1(f_drug1,a_drug1,f_drug2,a_drug2,cell_line,index_test,synergy,senstivity):\n  train_f_drug1=[]\n  train_a_drug1=[]\n  train_f_drug2=[]\n  train_a_drug2=[]\n  train_cell_line=[]\n  train_synergy=[]\n  train_senstivity=[]\n  test_f_drug1=[]\n  test_a_drug1=[]\n  test_f_drug2=[]\n  test_a_drug2=[]\n  test_cell_line=[]\n  test_synergy=[]\n  test_senstivity=[]\n    \n  for i in range(len(f_drug1)):\n     ind=i+1\n     if(ind not in index_test):\n          train_a_drug1.append(a_drug1[i])\n          train_a_drug2.append(a_drug2[i])\n          train_f_drug1.append(f_drug1[i])\n          train_f_drug2.append(f_drug2[i])\n          train_cell_line.append(cell_line[i])\n          train_synergy.append(synergy[i])\n          train_senstivity.append(senstivity[i])\n  for ii in range(len(index_test)):\n#       ind1=ii-1\n      x=index_test[ii]-1\n      n=x[0]\n      test_a_drug1.append(a_drug1[n])\n      test_a_drug2.append(a_drug2[n])\n      test_f_drug1.append(f_drug1[n])\n      test_f_drug2.append(f_drug2[n])\n      test_cell_line.append(cell_line[n])\n      test_synergy.append(synergy[n])\n      test_senstivity.append(senstivity[n])\n    \n  return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity\n\n\ndef preprocess(index_train,index_test):\n    index_train1=[]\n    index_test1=[]\n    for i in range(len(index_train)):\n        index_train1.append((index_train[i])[0])\n        \n    for ii in range(len(index_test)):\n        index_test1.append((index_test[ii])[0])\n        \n    return index_train1,index_test1\n\n\n\ndef train_test_input2(f_drug1,a_drug1,f_drug2,a_drug2,cell_line,index_test,synergy,senstivity):\n  \n \n  train_a_drug1=a_drug1[3581:]\n  train_a_drug2=a_drug2[3581:]\n  train_f_drug1=f_drug1[3581:]\n  train_f_drug2=f_drug2[3581:]\n  train_cell_line=cell_line[3581:]\n  train_synergy=synergy[3581:]\n  train_senstivity=senstivity[3581:]\n  test_a_drug1=a_drug1[0:3581]\n  test_a_drug2=a_drug2[0:3581]\n  test_f_drug1=f_drug1[0:3581]\n  test_f_drug2=f_drug2[0:3581]\n  test_cell_line=cell_line[0:3581]\n  test_synergy=synergy[0:3581]\n  test_senstivity=senstivity[0:3581]\n    \n#   train_synergy=np.array(train_synergy)\n#   train_senstivity=np.array(train_senstivity)\n#   test_synergy=np.array(test_synergy)\n#   test_senstivity=np.array(test_senstivity)\n\n  return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity\n\n\ndef get_data_me1(s):\n    \n\n    !gdown --id 1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\n    dele=pd.read_excel('deleted_index.xls',header=None)\n    dele=np.array(dele)\n    dele=dele-1\n    \n    \n#     !gdown http://www.bioinf.jku.at/software/DeepSynergy/data_test_fold0_tanh.p.gz\n#     file = gzip.open(data_file, 'rb')\n#     X_tr, X_val, X_train, X_test, y_tr, y_val, y_train, y_test = pickle.load(file)\n#     file.close()\n    \n\n    !gdown 1SvLGjU---dvwsuCTqRMehtV-Gmf2Hk52\n    labels = pd.read_csv('pcbi.1006752.s004.csv', index_col=0) \n    \n    h=len(np.array(labels))\n    #labels are duplicated for the two different ways of ordering in the data\n#     labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['Fold']!=test_fold)\n    \n#     h=len(idx_train[0])\n    xx=np.where(idx_train == dele)[1]\n    idx_train=np.delete(idx_train,xx)\n    xx1=np.where(idx_train == (dele+h))[1]\n    idx_train=np.delete(idx_train,xx1)\n#     idx_train=idx_train[!dele]\n    #indices of test data for model testing: fold 0\n    idx_test = np.where(labels['Fold']==test_fold)\n#     h1=len(idx_test[0])\n    yy=np.where(idx_test == dele)[1]\n    idx_test=np.delete(idx_test,yy)\n    yy1=np.where(idx_test == (dele+h))[1]\n    idx_test=np.delete(idx_test,yy1)\n    \n#     X_train = X[idx_train]\n#     X_test = X[idx_test]\n    idx_train1=idx_train + (idx_train.shape)\n    idx_test1=idx_test + (idx_test.shape)\n    \n    idx_train1=idx_train + (h)\n    idx_test1=idx_test + (h)\n    idx_train1=np.r_[idx_train,idx_train1]\n    idx_test1=np.r_[idx_test,idx_test1]\n    #choose idx_train1 and idx_test1 if you want to duplicate data\n    return idx_train1,idx_test1\n\ndef get_data_me(s):\n   \n    !gdown --id 1R_E1txnkHrwMQlBKpG7a4BHcZM4kRSkj\n    dele=pd.read_excel('deleted_deep.xlsx',header=None)\n\n    dele=np.array(dele)\n    dele=dele-1\n \n#     !gdown http://www.bioinf.jku.at/software/DeepSynergy/data_test_fold0_tanh.p.gz\n#     file = gzip.open(data_file, 'rb')\n#     X_tr, X_val, X_train, X_test, y_tr, y_val, y_train, y_test = pickle.load(file)\n#     file.close()\n    \n    !gdown 1HNEch5czfqjThnZpFpP-Dcy0Qi-DQdc1\n    labels = pd.read_csv('labels1.csv', index_col=0)\n\n\n    h=len(np.array(labels))\n    #labels are duplicated for the two different ways of ordering in the data\n#     labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['fold']!=test_fold)\n    \n#     h=len(idx_train[0])\n    xx=np.where(idx_train == dele)[1]\n    idx_train=np.delete(idx_train,xx)\n    xx1=np.where(idx_train == (dele+h))[1]\n    idx_train=np.delete(idx_train,xx1)\n#     idx_train=idx_train[!dele]\n    #indices of test data for model testing: fold 0\n    idx_test = np.where(labels['fold']==test_fold)\n#     h1=len(idx_test[0])\n    yy=np.where(idx_test == dele)[1]\n    idx_test=np.delete(idx_test,yy)\n    yy1=np.where(idx_test == (dele+h))[1]\n    idx_test=np.delete(idx_test,yy1)\n    \n#     X_train = X[idx_train]\n#     X_test = X[idx_test]\n\n    y_train = labels.iloc[idx_train]['synergy'].values\n    y_test = labels.iloc[idx_test]['synergy'].values\n    y1_train = labels.iloc[idx_train]['senstivity'].values\n    y1_test = labels.iloc[idx_test]['senstivity'].values\n \n    return y_train,y_test,idx_train,idx_test,y1_train,y1_test\n\n\n\nsmiles,data_to_repeat,unique_drugs,unique_cell,deleted_index=get_data()\n# smiles,data_to_repeat,unique_drugs,unique_cell,deleted_index=get_data()\ndeleted_index=deleted_index-1\n\nfeature,adjacency=graph_node(smiles)\n# feature,edge_index=graph_node_edge(smiles)\ndata_to_repeat=np.r_[data_to_repeat,data_to_repeat]\nl=int((data_to_repeat.shape[0])/2)\ndata_to_repeat[l:,0]=data_to_repeat[0:l,1]\ndata_to_repeat[l:,1]=data_to_repeat[0:l,0]\n# np.random.shuffle(data_to_repeat)\nsynergy=data_to_repeat[:,13]\nsenstivity=data_to_repeat[:,5]\n# synergy=data_to_repeat[:,3]\n# senstivity=data_to_repeat[:,5]\n\n# f_drug1,f_drug2,a_drug1,a_drug2,synergy,senstivity=repeat_smiles(data_to_repeat,unique_drugs,feature,adjacency,deleted_index,synergy,senstivity)\n\nf_drug1,f_drug2,a_drug1,a_drug2,feature_cell=repeat_smiles1(data_to_repeat,unique_drugs,feature,adjacency,unique_cell)\n# f_drug1,f_drug2,a_drug1,a_drug2,feature_cell=repeat_smiles_deep(data_to_repeat,unique_drugs,feature,adjacency,unique_cell)\n# y_train,y_test,index_train,index_test,y1_train,y1_test=get_data_me(3)\nindex_train,index_test=get_data_me1(1)\n# train_synergy1, train_senstivity1,test_synergy1, test_senstivity1,index_train,index_test=split_data(feature_cell,synergy,senstivity)\n# feature_cell=np.array(feature_cell).astype(float)\n# index_train,index_test=split_data1(feature_cell,synergy,senstivity)\n# index_test,index_train=index1(1)\n# index_train,index_test=preprocess(index_train,index_test)\n# train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy, train_senstivity,test_synergy, test_senstivity=train_test_input1(f_drug1,a_drug1,f_drug2,a_drug2,feature_cell,index_test,synergy,senstivity)\ntrain_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity=train_test_input(f_drug1,a_drug1,f_drug2,a_drug2,feature_cell,index_train,index_test,synergy,senstivity)\ntrain_cell_line=np.array(train_cell_line).astype(float)\ntest_cell_line=np.array(test_cell_line).astype(float)  \nnorm = \"tanh_norm\"\nif norm == \"tanh_norm\":\n    train_cell_line, mean, std, mean2, std2, feat_filt = normalize1(train_cell_line, norm=norm)\n    test_cell_line, mean, std, mean2, std2, feat_filt = normalize1(test_cell_line, mean, std, mean2, std2, \n                                                          feat_filt=feat_filt, norm=norm)\nelse:\n    train_cell_line, mean, std, feat_filt = normalize1(train_cell_line, norm=norm)\n    test_cell_line, mean, std, feat_filt = normalize1(test_cell_line, mean, std, feat_filt=feat_filt, norm=norm)\n  \nprint(synergy.shape)\n# print(np.random.shuffle(dataset1))\n# cc\n# cc1=[m for m, v in enumerate(unique_name) if cc in v]\n# # int((l))ik2\n# unique_cell[0].shape[1]\n# unique_name=unique_cell[:,0]\nprint(train_cell_line.shape)  \nprint(test_cell_line.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T14:42:20.324609Z","iopub.execute_input":"2022-12-12T14:42:20.325033Z","iopub.status.idle":"2022-12-12T14:44:32.717067Z","shell.execute_reply.started":"2022-12-12T14:42:20.324950Z","shell.execute_reply":"2022-12-12T14:44:32.715683Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting deepchem\n  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.3.5)\nRequirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.21.6)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.0.1)\nRequirement already satisfied: scipy<1.9 in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.7.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.0.2)\nCollecting rdkit\n  Downloading rdkit-2022.9.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/29.1 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->deepchem) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->deepchem) (2022.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from rdkit->deepchem) (9.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->deepchem) (3.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\nInstalling collected packages: rdkit, deepchem\nSuccessfully installed deepchem-2.7.1 rdkit-2022.9.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pysmiles\n  Downloading pysmiles-1.0.2-py2.py3-none-any.whl (22 kB)\nRequirement already satisfied: networkx~=2.0 in /opt/conda/lib/python3.7/site-packages (from pysmiles) (2.5)\nRequirement already satisfied: pbr in /opt/conda/lib/python3.7/site-packages (from pysmiles) (5.10.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx~=2.0->pysmiles) (5.1.1)\nInstalling collected packages: pysmiles\nSuccessfully installed pysmiles-1.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: openpyxl in /opt/conda/lib/python3.7/site-packages (3.0.10)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.7/site-packages (from openpyxl) (1.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting PubChemPy\n  Downloading PubChemPy-1.0.4.tar.gz (29 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: PubChemPy\n  Building wheel for PubChemPy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PubChemPy: filename=PubChemPy-1.0.4-py3-none-any.whl size=13834 sha256=860c4d21ad66fa907a717716de60dbbc9ccb9a454693011f0d357963a8a52063\n  Stored in directory: /root/.cache/pip/wheels/7c/3d/8c/8192697412e9899dc55bbbb08bbc1197bef333caaa2a71c448\nSuccessfully built PubChemPy\nInstalling collected packages: PubChemPy\nSuccessfully installed PubChemPy-1.0.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting PyDrive\n  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (1.12.11)\nRequirement already satisfied: oauth2client>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (4.1.3)\nRequirement already satisfied: PyYAML>=3.0 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (6.0)\nRequirement already satisfied: six<2dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.15.0)\nRequirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (0.1.0)\nRequirement already satisfied: google-api-core<3dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.33.2)\nRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\nRequirement already satisfied: google-auth<3dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.35.0)\nRequirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (0.20.4)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (4.8)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (0.2.7)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.56.3)\nCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5\n  Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.28.1)\nRequirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (59.8.0)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.2.4)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.2->PyDrive) (3.0.9)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2022.9.24)\nBuilding wheels for collected packages: PyDrive\n  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PyDrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27453 sha256=e6d6b2b240533da29b31bfcd98e3cf9d075543c60a329888f94205de7f92ab2a\n  Stored in directory: /root/.cache/pip/wheels/57/cc/07/6aac75f5395a224650905accd38c868c2276782a56f1046b7b\nSuccessfully built PyDrive\nInstalling collected packages: protobuf, PyDrive\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Uninstalling protobuf-3.19.4:\n      Successfully uninstalled protobuf-3.19.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\ntensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\ntensorflow 2.6.4 requires tensorboard<2.7,>=2.6.0, but you have tensorboard 2.10.1 which is incompatible.\ntensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\ntensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\ntensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.12.0 requires protobuf<=3.20.1,>=3.12.2, but you have protobuf 3.20.3 which is incompatible.\nnnabla 1.31.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.47.0 requires grpcio>=1.47.0, but you have grpcio 1.43.0 which is incompatible.\ngcsfs 2022.5.0 requires fsspec==2022.5.0, but you have fsspec 2022.8.2 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed PyDrive-1.3.1 protobuf-3.20.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting xlrd==2.0.1\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: xlrd\nSuccessfully installed xlrd-2.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: Pandas==1.3.5 in /opt/conda/lib/python3.7/site-packages (1.3.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from Pandas==1.3.5) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from Pandas==1.3.5) (2022.1)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from Pandas==1.3.5) (1.21.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->Pandas==1.3.5) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting gdown\n  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.64.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.15.0)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.28.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.7.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.3)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","output_type":"stream"},{"name":"stdout","text":"hjjbjh\nDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 35.1MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1z1sWG4E9BqipP0iczLrbR4qxSa4KG95_\nTo: /kaggle/working/pcbi.1006752.s004.xls\n100%|███████████████████████████████████████| 5.24M/5.24M [00:00<00:00, 230MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 33.8MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1NoUKCRZ0CkNVFP1-578x_L0otPUMkz6T\nTo: /kaggle/working/unique934_cell.xlsx\n100%|████████████████████████████████████████| 255k/255k [00:00<00:00, 99.1MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\nTo: /kaggle/working/deleted_index.xls\n100%|██████████████████████████████████████| 58.9k/58.9k [00:00<00:00, 45.3MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\nTo: /kaggle/working/deleted_index.xls\n100%|██████████████████████████████████████| 58.9k/58.9k [00:00<00:00, 59.3MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1SvLGjU---dvwsuCTqRMehtV-Gmf2Hk52\nTo: /kaggle/working/pcbi.1006752.s004.csv\n100%|███████████████████████████████████████| 2.57M/2.57M [00:00<00:00, 182MB/s]\n(37810,)\n(28852, 934)\n(6950, 934)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow\nfrom networkx.readwrite.graph6 import data_to_n\nfrom tensorflow.python.training.tracking import data_structures\n!pip install spektral\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import add,PReLU, Flatten,Reshape,Dropout, Input,Dense,Add,concatenate,BatchNormalization, Activation#,MultiHeadAttention,AdditiveAttention\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n# from tensorflow.random import set_seed\nfrom spektral.data.loaders import SingleLoader\nfrom spektral.datasets.citation import Citation\nfrom spektral.layers import GATConv, GlobalAvgPool,GlobalMaxPool, GCNConv,GlobalAttentionPool\nfrom spektral.transforms import LayerPreprocess\n# import tensorflow.compat.v1.keras.backend as K\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score#,AUC\nfrom scipy.stats import pearsonr,spearmanr\nfrom sklearn.metrics import roc_curve,auc\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras import regularizers\ndef generate_network1(x_in1,a_in1,x_in2,a_in2,cell, inDrop, drop):\n    # fill the architecture params from dict\n    gcn_layers = [78,156,312]\n    \n    gan_layers = [128,128]\n    cell_layers = [512,265,128]#for gcn\n#     cell_layers = [2048,512,128*2] #for gan\n    snp_layers = [512,128]#for gcn\n#     snp_layers = [2048,512,256]# for gan\n    dsn1_layers = [1024,2048,1024]\n    dsn2_layers = [1024,2048,1024]\n    N = x_in1[0].shape[0]\n    F = x_in1[0].shape[1]\n    \n    channels = 8  # Number of channels in each head of the first GAT layer\n    n_attn_heads = 10  # Number of attention heads in first GAT layer\n    dropout = 0.2 # Dropout rate for the features and adjacency matrix\n    drop=0.2\n    l2_reg = 1e-3  # L2 regularization rate\n    # contruct two parallel networks\n    x_in1 = Input(shape=(N,F,),name='x_in1')\n    a_in1 = Input((N,N,),name='a_in1')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d1 = Dropout(dropout)(x_in1)\n             middle_layer_d1  = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activty_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d1, a_in1])\n#              middle_layer_d1=BatchNormalization()(middle_layer_d1)\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n             gcn_output1=GlobalMaxPool(name='a1')(middle_layer_d1)\n             \n#              gcn_output1=Flatten()(x_in1)\n#              gcn_output1=GlobalAttentionPool(1000)(x_in1)#(middle_layer_d1)\n             \n             \n        else:\n             middle_layer_d1 = Dropout(dropout)(middle_layer_d1)\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n#              middle_layer_d1=BatchNormalization()(middle_layer_d1)\n#     print(x_in1.shape)\n    # print(a_in1.shape)\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n#             middle_layer1  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in1, a_in1])\n            \n#         elif l == (len(gan_layers)-1):\n#             middle_layer1 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n            \n#             gan_output1=GlobalMaxPool()(middle_layer1)\n# #             gan_output1 = Dense(int(gan_layers[l]), activation='tanh')(gan_output1)\n#         else:\n#             middle_layer1 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n            \n            \n    \n#   156  concatModel1 = concatenate([gcn_output1, gan_output1])\n    gcn_output1 = Dense(256, activation='relu')(gcn_output1)\n    gcn_output1 = Dropout(dropout)(gcn_output1)\n    gcn_output1 = Dense(128, activation='linear')(gcn_output1)\n    \n    gan_output1 = Dense(128, activation='relu')(gan_output1)\n   \n    \n    concatModel1 =  gan_output1\n    # # addtModel = Add([gcn_output, gan_output])\n    x_in2 = Input(shape=(N,F,),name='x_in2')\n    a_in2 = Input((N,N,),name='a_in2')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d2 = Dropout(dropout)(x_in2)\n             middle_layer_d2 =  GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d2, a_in2])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n             gcn_output2=GlobalMaxPool(name='a2')(middle_layer_d2)\n#             gcn_output2=Flatten()(x_in2)\n#             gcn_output2=GlobalAttentionPool(1000)(x_in2)#(middle_layer_d2)\n             \n        else:\n             middle_layer_d2 = Dropout(dropout)(middle_layer_d2)\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n#             middle_layer2  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in2, a_in2])\n           \n#         elif l == (len(gan_layers)-1):\n#             middle_layer2 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n#             gan_output2=GlobalMaxPool()(middle_layer2)\n# #             gan_output2 = Dense(int(gan_layers[l]), activation='tanh')(gan_output2)\n#         else:\n#             middle_layer2 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n    # print(gcn_output2.shape)\n    # print(gan_output2.shape)        \n#   156  concatModel2 = concatenate([gcn_output2, gan_output2])\n    gcn_output2 = Dense(256, activation='relu')(gcn_output2)\n    gcn_output2 = Dropout(dropout)(gcn_output2)\n    gcn_output2 = Dense(128, activation='linear')(gcn_output2)\n    \n    gan_output2 = Dense(128, activation='relu')(gan_output2)\n    \n    concatModel2 = gcn_output2\n    # concatModel2 = Add()([gcn_output2, gan_output2])\n    for cell_layer in range(len(cell_layers)):\n      if cell_layer == 0:\n        input_cell    = Input(shape=(cell[0].shape[0],))\n#         input_cell1 = BatchNormalization()(input_cell)\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,)(input_cell)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      elif cell_layer == (len(cell_layers)-1):\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,)(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      #  snp_output = Dense(1, activation='linear')(snpFC)\n      else:\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,)(cellFC)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n    # print(concatModel1.shape)\n    # print(cellFC.shape)\n    \n    concatModel=concatenate([concatModel2,concatModel1,cellFC])\n    concatModel=BatchNormalization()(concatModel)\n#     input2=concatenate([concatModel2,cellFC])\n    # input1=concatenate([gan_output1,cellFC])\n    # input2=concatenate([gan_output2,cellFC])\n    # # addtModel = Add([concatModel1, concatModel2])\n    # dsn1_layers = [2048,4096,2048]\n    # dsn2_layers = [2048,4096,2048]\n    # snp_layers = [2048,1024]\n    # # contruct two parallel networks\n    \n       \n#     for snp_layer in range(len(snp_layers)):\n#        if snp_layer == 0:\n#          snpFC1 = Dense(int(snp_layers[snp_layer]), activation='linear',use_bias=True,)(concatModel)\n#          snpFC1=PReLU()(snpFC1)\n#          snpFC = Dropout(float(drop))(snpFC1)\n# #          snpFC=BatchNormalization()(snpFC)\n#        elif snp_layer == (len(snp_layers)-1):\n#          snpFC = Dense(int(snp_layers[snp_layer]), activation='linear',use_bias=True,)(snpFC)\n#          snpFC=PReLU()(snpFC)\n# #          layer = MultiHeadAttention(num_heads=2, key_dim=2)\n# #          rr_task1=Reshape([1,snpFC.shape[1]])(snpFC)\n# #          a_task1=layer(rr_task1,rr_task1)\n#          snp_output1 = Dense(1, activation='linear')(snpFC)\n#        else:\n#           snpFC = Dense(int(snp_layers[snp_layer]), activation='linear',use_bias=True,)(snpFC)\n#           snpFC=PReLU()(snpFC)\n#           snpFC = Dropout(float(drop))(snpFC)\n\n\n    for snp_layer in range(len(snp_layers)):\n       if snp_layer == 0:\n         snpFC1 = Dense(int(snp_layers[snp_layer]), activation='relu',use_bias=True,)(concatModel)\n#          snpFC1=PReLU()(snpFC1)\n         snpFC = Dropout(float(drop))(snpFC1)\n#          snpFC=BatchNormalization()(snpFC)\n       elif snp_layer == (len(snp_layers)-1):\n         snpFC = Dense(int(snp_layers[snp_layer]), activation='relu',use_bias=True,)(snpFC)\n#          snpFC=PReLU()(snpFC)\n#          layer = MultiHeadAttention(num_heads=2, key_dim=2)\n#          rr_task1=Reshape([1,snpFC.shape[1]])(snpFC)\n#          a_task1=layer(rr_task1,rr_task1)\n         snp_output1 = Dense(1, activation='linear')(snpFC)\n       else:\n          snpFC = Dense(int(snp_layers[snp_layer]), activation='relu',use_bias=True,)(snpFC)\n#           snpFC=PReLU()(snpFC)\n          snpFC = Dropout(float(drop))(snpFC)\n#     model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [f_task1,f_task2])\n    model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [snp_output1])\n    # tf.keras.utils.plot_model(model.build_graph(), to_file=\"model.png\",\n    #        expand_nested=True, show_shapes=True)\n    print(model.summary())\n    return model\n\ndef trainer1(model, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity, epo, batch_size, earlyStop):\n#     model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=float(l_rate), momentum=0.9,nesterov=True, clipvalue=0.3))\n    model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=float(l_rate)))#, beta_1=0.9, beta_2=0.999, amsgrad=True))\n    model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line], [train_synergy], shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 )#,restore_best_weights=True,validation_split=0.1) \n                  #  validation_data=([val_input1,val_input2], [val_synergy,val_senstivity]))\n    # model.fit({\"input1\":train_drug1, \"input2\":train_drug2}, {\"synergy\":train_synergy,\"senstivity\":train_senstivity}, epochs=epo, shuffle=True, batch_size=batch_size,verbose=1) \n    #                validation_data=({\"input1\":val_input1,\"input2\":val_input2}, {\"synergy\":val_synergy,\"senstivity\":val_senstivity}))\n\n    return model\ntrain_f_drug1=np.array(train_f_drug1)\ntrain_a_drug1=np.array(train_a_drug1)\ntrain_f_drug2=np.array(train_f_drug2)\ntrain_a_drug2=np.array(train_a_drug2)\ntrain_cell_line=np.array(train_cell_line)\ntest_f_drug1=np.array(test_f_drug1)\ntest_a_drug1=np.array(test_a_drug1)\ntest_f_drug2=np.array(test_f_drug2)\ntest_a_drug2=np.array(test_a_drug2)\ntest_cell_line=np.array(test_cell_line)\ntrain_synergy=np.array(train_synergy)\ntrain_senstivity=np.array(train_senstivity)\ntest_synergy=np.array(test_synergy)\ntest_senstivity=np.array(test_senstivity)\n\n\n# train_synergy\n# test_synergy\n# train_synergy.shape\n# test_synergy.shape\n# train_senstivity","metadata":{"execution":{"iopub.status.busy":"2022-12-12T14:44:32.720164Z","iopub.execute_input":"2022-12-12T14:44:32.720572Z","iopub.status.idle":"2022-12-12T14:45:02.205901Z","shell.execute_reply.started":"2022-12-12T14:44:32.720533Z","shell.execute_reply":"2022-12-12T14:45:02.204735Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.6.4)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nCollecting tensorboard<2.7,>=2.6.0\n  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting numpy~=1.19.2\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.43.0)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.15.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.37.1)\nCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (5.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.8.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.6.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.4.6)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.35.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (3.3.7)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.2.2)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (59.8.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.28.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.2.4)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (4.13.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2.1.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (3.2.0)\nInstalling collected packages: typing-extensions, numpy, h5py, tensorboard\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.1.1\n    Uninstalling typing_extensions-4.1.1:\n      Successfully uninstalled typing_extensions-4.1.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.7.0\n    Uninstalling h5py-3.7.0:\n      Successfully uninstalled h5py-3.7.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.10.1\n    Uninstalling tensorboard-2.10.1:\n      Successfully uninstalled tensorboard-2.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nxarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\ntensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\nrich 12.6.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.7.7 requires tensorboard>=2.9.1, but you have tensorboard 2.6.0 which is incompatible.\npytorch-lightning 1.7.7 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.12 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nonnx 1.12.0 requires protobuf<=3.20.1,>=3.12.2, but you have protobuf 3.20.3 which is incompatible.\nnnabla 1.31.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\nnnabla 1.31.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\njaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\njax 0.3.23 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\nflax 0.6.1 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\nfeaturetools 1.11.1 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndeepchem 2.7.1 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\ncmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\nallennlp 2.10.1 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\nallennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\naioitertools 0.11.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.93 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed h5py-3.1.0 numpy-1.19.5 tensorboard-2.6.0 typing-extensions-3.10.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting spektral\n  Downloading spektral-1.2.0-py3-none-any.whl (140 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from spektral) (1.19.5)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from spektral) (2.5)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from spektral) (1.3.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from spektral) (1.0.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from spektral) (4.64.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from spektral) (4.9.1)\nRequirement already satisfied: tensorflow>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from spektral) (2.6.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from spektral) (1.7.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from spektral) (1.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from spektral) (2.28.1)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.15.0)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.15.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.1.2)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.43.0)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.4.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.37.1)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (2.6.0)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.12.1)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (2.6.0)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (5.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.12)\nRequirement already satisfied: typing-extensions<3.11,>=3.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.10.0.2)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.1.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\nRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (2.6.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\nRequirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.1.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.20.3)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->spektral) (5.1.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->spektral) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->spektral) (2022.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (2.1.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->spektral) (3.1.0)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=2.2.0->spektral) (1.5.2)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (1.8.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (2.2.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.4.6)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (3.3.7)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (59.8.0)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (1.35.0)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.6.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (4.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (4.13.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (3.2.0)\nInstalling collected packages: spektral\nSuccessfully installed spektral-1.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"### from sklearn.metrics import roc_curve,auc\nl_rate = 0.0001\ninDrop = 0.2\ndrop = 0.2\nmax_epoch = 500\n# batch_size = 128 #gcn\nbatch_size = 64 #gan\nearlyStop_patience = 20#np.ceil(train_f_drug1.shape[0]/batch_size)#1000\n\n# model1= generate_network1(train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line, inDrop, drop)\n# plot_model(model1, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n# model1=trainer1(model1, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity,max_epoch, batch_size,\n#                                 earlyStop_patience)\n# # p1,p2= predict(model, [test_input1,test_input2])\n# p1= model1.predict( [test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line])\n# # s1=np.zeros(len(p1))\n# # s2=np.zeros(len(p1))\n# # for i in range(len(p1)):\n# #     n1=p1[0]\n# #     h1=n1[0]\n# #     x1=h1[0]\n# #     s1[i]=x1\n# #     n2=p2[i]\n# #     h2=n2[0]\n# #     x2=h2[0]\n\n# synergy_error=mean_squared_error(test_synergy, p1)\n# # senstivity_error=mean_squared_error(test_senstivity, p2)\n# # p1,p2\n\n# # senstivity_error=mean_squared_error(test_senstivity, p2)\n# synergy_error1=mean_absolute_error(test_synergy, p1)\n# # senstivity_error1=mean_absolute_error(test_senstivity, p2)\n# p12=[]\n# for i in range(len(p1)):\n#   x=p1[i]\n#   p12.append(x[0])\n# p12=np.array(p12)\n# synergy_error2=r2_score(test_synergy, p12)\n# # senstivity_error2=r2_score(test_senstivity, p2)\n# print(\"synergy_mean_squared_error\",synergy_error)\n# # print(\"senstivity_mean_squared_error\",senstivity_error)\n# print(\"synergy_mean_absolute_error\",synergy_error1)\n# # print(\"senstivity_mean_absolute_error\",senstivity_error1)\n# print(\"synergy_r2_score\",synergy_error2)\n# # print(\"senstivity_r2_score\",senstivity_error2)\n# losses = model1.evaluate([test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line],[test_synergy],batch_size =64)\n# print(losses)\n\n\n# synergy_pear= pearsonr(test_synergy, p12)\n# synergy_spear= spearmanr(test_synergy, p12)\n# print(\"synergy_pear\",synergy_pear)\n# print(\"synergy_spear\",synergy_spear)\n# p1\n# # senstivity_pear= pearsonr(test_senstivity, p2)\n# # senstivity_spear= spearmanr(test_senstivity, p2)\n# # print(\"senstivity_pear\",senstivity_pear)\n# # print(\"senstivity_spear\",senstivity_spear)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T14:45:02.207483Z","iopub.execute_input":"2022-12-12T14:45:02.208841Z","iopub.status.idle":"2022-12-12T14:45:02.218807Z","shell.execute_reply.started":"2022-12-12T14:45:02.208790Z","shell.execute_reply":"2022-12-12T14:45:02.215435Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# import tensorflow_datasets as tfds\nimport tensorflow as tf\ndef scaled_dot_product_attention(q, k, v, mask=None):\n  \"\"\"Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead)\n  but it must be broadcastable for addition.\n\n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable\n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n  Returns:\n    output, attention_weights\n  \"\"\"\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)\n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits)#, axis=-1)#tf.nn.linear()#, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output,attention_weights \n\n#multi-head\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self,*, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n\n    assert d_model % self.num_heads == 0\n\n    self.depth = d_model // self.num_heads\n\n    self.wq = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wk = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wv = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n\n    self.dense = tf.keras.layers.Dense(d_model)#,activation='relu',use_bias='true')\n\n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n\n  def call(self, v, k, q, mask=None):\n    batch_size = tf.shape(q)[0]\n\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n#     q=PReLU()(q)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n#     k=PReLU()(k)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n#     v=PReLU()(v)\n\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention,\n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n    return output#, attention_weights\n\n\n\nclass CrossStitch(tf.keras.layers.Layer):\n\n    \"\"\"Cross-Stitch implementation according to arXiv:1604.03539\n    Implementation adapted from https://github.com/helloyide/Cross-stitch-Networks-for-Multi-task-Learning\"\"\"\n\n    def __init__(self, num_tasks, *args, **kwargs):\n        \"\"\"initialize class variables\"\"\"\n        self.num_tasks = num_tasks\n        super(CrossStitch, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        \"\"\"initialize the kernel and set the instance to 'built'\"\"\"\n        self.kernel = self.add_weight(name=\"kernel\",\n                                      shape=(self.num_tasks,\n                                             self.num_tasks),\n                                      initializer='identity',\n                                      trainable=True)\n        super(CrossStitch, self).build(input_shape)\n\n    def call(self, xl):\n        \"\"\"\n        called by TensorFlow when the model gets build. \n        Returns a stacked tensor with num_tasks channels in the 0 dimension, \n        which need to be unstacked.\n        \"\"\"\n        if (len(xl) != self.num_tasks):\n            # should not happen\n            raise ValueError()\n\n        out_values = []\n        for this_task in range(self.num_tasks):\n            this_weight = self.kernel[this_task, this_task]\n            out = tf.math.scalar_mul(this_weight, xl[this_task])\n            for other_task in range(self.num_tasks):\n                if this_task == other_task:\n                    continue  # already weighted!\n                other_weight = self.kernel[this_task, other_task]\n                out += tf.math.scalar_mul(other_weight, xl[other_task])\n            out_values.append(out)\n        # HACK!\n        # unless we stack, and then unstack the tensors, TF (2.0.0) can't follow\n        # the graph, so it aborts during model initialization.\n        # return tf.stack(out_values, axis=0)\n        return out_values[0],out_values[1]\n\n    def compute_output_shape(self, input_shape):\n        return [self.num_tasks] + input_shape\n\n    def get_config(self):\n        \"\"\"implemented so keras can save the model to json/yml\"\"\"\n        config = {\n            \"num_tasks\": self.num_tasks\n        }\n        base_config = super(CrossStitch, self).get_config()\n        return dict(list(config.items()) + list(base_config.items()))\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T14:45:02.222679Z","iopub.execute_input":"2022-12-12T14:45:02.223776Z","iopub.status.idle":"2022-12-12T14:45:02.315422Z","shell.execute_reply.started":"2022-12-12T14:45:02.223696Z","shell.execute_reply":"2022-12-12T14:45:02.314442Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\ndef generate_network_att1(x_in1,a_in1,x_in2,a_in2,cell, inDrop, drop):\n    # fill the architecture params from dict\n#     gcn_layers = [1024,512,156]\n#     gan_layers = [1024,512]\n#     cell_layers = [2048,512]#,2048]\n#     snp_layers = [1024,512,128]#,2048]\n#     dsn1_layers = [1024,2048,1024]\n#     dsn2_layers = [1024,2048,1024]\n    # fill the architecture params from dict\n    gcn_layers = [78,156,312]\n#     gan_layers = [128,128]\n#     gcn_layers = [32,64,128]\n    cell_layers = [512,265,128]#for gcn\n#     cell_layers = [2048,512,256] #for gan\n    snp_layers = [512,128]#for gcn\n#     snp_layers = [2048,512,256]# for gan\n    dsn1_layers = [1024,2048,1024]\n    dsn2_layers = [1024,2048,1024]\n    N = x_in1[0].shape[0]\n    F = x_in1[0].shape[1]\n    \n    channels = 8  # Number of channels in each head of the first GAT layer\n    n_attn_heads = 10  # Number of attention heads in first GAT layer\n    dropout = 0.2 # Dropout rate for the features and adjacency matrix\n    drop=0.2\n    l2_reg = 1e-3\n    N = x_in1[0].shape[0]\n    F = x_in1[0].shape[1]\n    \n    channels = 8  # Number of channels in each head of the first GAT layer\n    n_attn_heads = 8  # Number of attention heads in first GAT layer\n    dropout = 0.2 # Dropout rate for the features and adjacency matrix\n    drop=0.2\n    l2_reg = 1e-3  # L2 regularization rate\n    # contruct two parallel networks\n    x_in1 = Input(shape=(N,F,),name='x_in1')\n    a_in1 = Input((N,N,),name='a_in1')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d1 = Dropout(dropout)(x_in1)\n             middle_layer_d1  = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activty_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d1, a_in1])\n#              middle_layer_d1=concatenate([middle_layer_d1,do_1_d1])\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n             gcn_output1=GlobalMaxPool()(middle_layer_d1)\n#              gcn_output1=GlobalAttentionPool(gcn_layers[l],name='a1')(middle_layer_d1)\n             \n#              gcn_output1=Flatten()(x_in1)\n#              gcn_output1=GlobalAttentionPool(1000)(x_in1)#(middle_layer_d1)\n             \n             \n        else:\n             middle_layer_d1 = Dropout(dropout)(middle_layer_d1)\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n#              middle_layer_d1=concatenate([middle_layer_d1,middle_layer_d11])\n#              middle_layer_d1=BatchNormalization()(middle_layer_d1)\n#     print(x_in1.shape)\n    # print(a_in1.shape)\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n#             middle_layer1  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in1, a_in1])\n#             middle_layer1=concatenate([middle_layer1,x_in1]) \n#         elif l == (len(gan_layers)-1):\n#             middle_layer1 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n#             gan_output1=GlobalAttentionPool(512)(middle_layer1)\n# #             gan_output1 = Dense(int(gan_layers[l]), activation='elu')(gan_output1)\n#         else:\n#             middle_layer11 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n#             middle_layer1=concatenate([middle_layer1,middle_layer11])\n    \n#  156   concatModel1 = concatenate([gcn_output1, gan_output1])\n    gcn_output1 = Dense(256, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output1)\n    gcn_output1 = Dropout(dropout)(gcn_output1)\n    gcn_output1 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output1)\n    \n#     gan_output1 = Dense(128, activation='relu')(gan_output1)\n    \n    concatModel1 =  gcn_output1\n    # # addtModel = Add([gcn_output, gan_output])\n    x_in2 = Input(shape=(N,F,),name='x_in2')\n    a_in2 = Input((N,N,),name='a_in2')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d2 = Dropout(dropout)(x_in2)\n             middle_layer_d2 =  GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d2, a_in2])\n#              middle_layer_d2=concatenate([middle_layer_d2,do_1_d2])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n#              gcn_output2=GlobalAttentionPool(gcn_layers[l],name='a2')(middle_layer_d2)\n             gcn_output2=GlobalMaxPool()(middle_layer_d2)\n#             gcn_output2=Flatten()(x_in2)\n#             gcn_output2=GlobalAttentionPool(1000)(x_in2)#(middle_layer_d2)\n             \n        else:\n             middle_layer_d2 = Dropout(dropout)(middle_layer_d2)\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n#              middle_layer_d2=concatenate([middle_layer_d2,middle_layer_d21])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n            \n#             middle_layer2  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in2, a_in2])\n#             middle_layer2=concatenate([middle_layer2,x_in2])\n#         elif l == (len(gan_layers)-1):\n#             middle_layer2 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n#             gan_output2=GlobalAttentionPool(512)(middle_layer2)\n# #             gan_output2 = Dense(int(gan_layers[l]), activation='tanh')(gan_output2)\n#         else:\n#             middle_layer21 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n#             middle_layer2=concatenate([middle_layer2,middle_layer_d21])\n    # print(gcn_output2.shape)\n    # print(gan_output2.shape)        \n#  156   concatModel2 = concatenate([gcn_output2, gan_output2])\n    gcn_output2 = Dense(256, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output2)\n    gcn_output2 = Dropout(dropout)(gcn_output2)\n    gcn_output2 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output2)\n    \n#     gan_output2 = Dense(128, activation='relu')(gan_output2)\n        \n    concatModel2 = gcn_output2\n    # concatModel2 = Add()([gcn_output2, gan_output2])\n    for cell_layer in range(len(cell_layers)):\n      if cell_layer == 0:\n        input_cell    = Input(shape=(cell[0].shape[0],))\n#         input_cell1 = BatchNormalization()(input_cell)\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(input_cell)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      elif cell_layer == (len(cell_layers)-1):\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      #  snp_output = Dense(1, activation='linear')(snpFC)\n      else:\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n    # print(concatModel1.shape)\n    # print(cellFC.shape)\n    \n    concatModel=concatenate([concatModel2,concatModel1,cellFC])\n    concatModel=BatchNormalization()(concatModel)\n#     input2=concatenate([concatModel2,cellFC])\n    # input1=concatenate([gan_output1,cellFC])\n    # input2=concatenate([gan_output2,cellFC])\n    # # addtModel = Add([concatModel1, concatModel2])\n    # dsn1_layers = [2048,4096,2048]\n    # dsn2_layers = [2048,4096,2048]\n    # snp_layers = [2048,1024]\n    # # contruct two parallel networks\n  \n    \n#     task1=Dense(512, activation='relu',use_bias=True)(concatModel)\n# #     task1=PReLU()(task1)\n#     task2=Dense(512, activation='relu',use_bias=True)(concatModel)\n    \n    layer1 =MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task1= layer1(concatModel,concatModel,concatModel)\n    layer2 = MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task2= layer2(concatModel,concatModel,concatModel)\n    task11 = Reshape([a_task1.shape[2]])(a_task1)\n    task22 = Reshape([a_task2.shape[2]])(a_task2)\n    task1=concatenate([task11,concatModel])\n    task2=concatenate([task22,concatModel])\n#     task2=PReLU()(task2)\n#     task1=BatchNormalization()(task1)\n#     task2=BatchNormalization()(task2)\n    r_task1,r_task2 = CrossStitch(2)([task1,task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = apply_cross_stitch([task1,task2])\n#     r_task1=PReLU()(r_task1)\n#     r_task2=PReLU()(r_task2)\n\n#     r_task1=concatenate([r_task1,task1])\n#     r_task2=concatenate([r_task2,task2])\n#     r_task1=BatchNormalization()(r_task1)\n#     r_task2=BatchNormalization()(r_task2)\n#     r_task1=concatenate([r_task1,task1])\n#     r_task2=concatenate([r_task2,task2])\n    \n#     r_task1=Dense(256, activation='relu',use_bias=True)(r_task1)\n    \n#     r_task2=Dense(256, activation='relu',use_bias=True)(r_task2)\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n    \n    \n    r_task1=Dense(1024,activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    \n    r_task2=Dense(1024, activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n    r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n    \n#     layer1 =MultiHeadAttention(d_model=r_task1.shape[1], num_heads=4)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=4)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     task11 = Reshape([a_task1.shape[2]])(a_task1)\n#     task22 = Reshape([a_task2.shape[2]])(a_task2)\n#     r_task1=concatenate([task11,r_task1])\n#     r_task2=concatenate([task22,r_task2])\n    \n    r_task1=concatenate([r_task1,task1])\n    r_task2=concatenate([r_task2,task2])\n    \n#     r_task1=PReLU()(r_task1)\n#     r_task2=PReLU()(r_task2)\n\n \n   \n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = apply_cross_stitch([r_task1,r_task2])\n\n    \n#     r_task1=BatchNormalization()(r_task1)\n#     r_task2=BatchNormalization()(r_task2)\n    \n#     r_task1=Dense(128, activation='linear',use_bias=True)(r_task1)\n    \n#     r_task2=Dense(128, activation='linear',use_bias=True)(r_task2)\n#     r_task1=PReLU()(r_task1)\n#     r_task2=PReLU()(r_task2)\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = apply_cross_stitch(r_task1,r_task2)\n\n    \n#     r_task1=BatchNormalization()(r_task1)\n#     r_task2=BatchNormalization()(r_task2)\n    \n     \n#     layer1 =MultiHeadAttention(d_model=r_task1.shape[1], num_heads=4)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=4)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task1=BatchNormalization()(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n#     a_task2=BatchNormalization()(a_task2)\n    \n    \n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n# #  \n#     rr_task2=Reshape([1,r_task2.shape[1]])(r_task2)\n#     a_task1=layer(rr_task1,rr_task1)\n#     a_task2=layer(rr_task2,rr_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n    \n#     layer1 = MultiHeadAttention(d_model=r_task1.shape[1], num_heads=8)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=8)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n    \n#     r_task1 = Dense(64, activation='linear',name=\"synergy\")(a_task1)\n#     r_task2 = Dense(64, activation='linear',name=\"senstivity\")(a_task2)\n    \n#     layer1 = MultiHeadAttention(d_model=r_task1.shape[1], num_heads=8)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=8)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task1=BatchNormalization()(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n#     a_task2=BatchNormalization()(a_task2)\n   \n    r_task1 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(128, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n    r_task2=PReLU()(r_task2)\n    r_task1 = Dense(64, activation='linear',name=\"synergy\",kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(64, activation='relu',name=\"senstivity\",kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n# \n#     r_task1 = Dense(512, activation='relu')(r_task1)\n# #   \n#     r_task2 = Dense(512, activation='relu')(r_task2)\n\n#     r_task1 = Dense(256, activation='relu',name=\"synergy\")(r_task1)\n#     r_task2 = Dense(256, activation='relu',name=\"senstivity\")(r_task2)\n    \n    snp_output1 = Dense(1, activation='linear')(r_task1)\n#     snp_output1=PReLU()(snp_output1)\n    snp_output2 = Dense(1, activation='relu')(r_task2)\n#     model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [f_task1,f_task2])\n    model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [snp_output1,snp_output2])\n    # tf.keras.utils.plot_model(model.build_graph(), to_file=\"model.png\",\n    #        expand_nested=True, show_shapes=True)\n    print(model.summary())\n    return model\n\ndef trainer_att1(model, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity, epo, batch_size, earlyStop):\n    model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=float(l_rate)))#, beta_1=0.9, beta_2=0.999, amsgrad=True))\n#     model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=float(l_rate), momentum=0.9, nesterov=True, clipnorm=1.))\n    model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line], [train_synergy,train_senstivity], shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 )#,restore_best_weights=True,validation_split=0.1) \n                  #  validation_data=([val_input1,val_input2], [val_synergy,val_senstivity]))\n    # model.fit({\"input1\":train_drug1, \"input2\":train_drug2}, {\"synergy\":train_synergy,\"senstivity\":train_senstivity}, epochs=epo, shuffle=True, batch_size=batch_size,verbose=1) \n    #                validation_data=({\"input1\":val_input1,\"input2\":val_input2}, {\"synergy\":val_synergy,\"senstivity\":val_senstivity}))\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-12-12T14:45:02.317315Z","iopub.execute_input":"2022-12-12T14:45:02.317687Z","iopub.status.idle":"2022-12-12T14:45:02.359534Z","shell.execute_reply.started":"2022-12-12T14:45:02.317653Z","shell.execute_reply":"2022-12-12T14:45:02.358604Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# l_rate = 0.0001\n# inDrop = 0.2\n# drop = 0.2\n# max_epoch = 500\n# batch_size =16\n\n\n# earlyStop_patience = np.ceil(train_f_drug1.shape[0]/batch_size)#1000\nmodel_att1= generate_network_att1(train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line, inDrop, drop)\nmodel_att1=trainer_att1(model_att1, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity,max_epoch, batch_size,\n                                earlyStop_patience)\n\n# p1,p2= predict(model, [test_input1,test_input2])\nap111,ap221= model_att1.predict( [test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line])\nasynergy_error1=mean_squared_error(test_synergy, ap111)\nasenstivity_error1=mean_squared_error(test_senstivity, ap221)\nasynergy_error11=mean_absolute_error(test_synergy, ap111)\nasenstivity_error11=mean_absolute_error(test_senstivity, ap221)\nasynergy_error21=r2_score(test_synergy, ap111)\nasenstivity_error21=r2_score(test_senstivity, ap221)\nprint(\"msynergy_mean_squared_error\",asynergy_error1)\nprint(\"msenstivity_mean_squared_error\",asenstivity_error1)\nprint(\"msynergy_mean_absolute_error\",asynergy_error11)\nprint(\"msenstivity_mean_absolute_error\",asenstivity_error11)\nprint(\"msynergy_r2_score\",asynergy_error21)\nprint(\"msenstivity_r2_score\",asenstivity_error21)\ncross_att1 = model_att1.evaluate([test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line],[test_synergy,test_senstivity],batch_size =64)\nprint(cross_att1)\nasynergy_pear1= pearsonr(test_synergy, ap111)\nasynergy_spear1= spearmanr(test_synergy, ap111)\nprint(\"msynergy_pear\",asynergy_pear1)\nprint(\"msynergy_spear\",asynergy_spear1)\nasenstivity_pear1= pearsonr(test_senstivity, ap221)\nasenstivity_spear1= spearmanr(test_senstivity, ap221)\nprint(\"msenstivity_pear\",asenstivity_pear1)\nprint(\"msenstivity_spear\",asenstivity_spear1)\nap111,ap221\n# yourTerminal:prompt> jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10 \n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T14:45:02.361696Z","iopub.execute_input":"2022-12-12T14:45:02.362466Z","iopub.status.idle":"2022-12-12T15:43:45.608718Z","shell.execute_reply.started":"2022-12-12T14:45:02.362430Z","shell.execute_reply":"2022-12-12T15:43:45.607670Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nx_in2 (InputLayer)              [(None, 69, 32)]     0                                            \n__________________________________________________________________________________________________\nx_in1 (InputLayer)              [(None, 69, 32)]     0                                            \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 69, 32)       0           x_in2[0][0]                      \n__________________________________________________________________________________________________\na_in2 (InputLayer)              [(None, 69, 69)]     0                                            \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 69, 32)       0           x_in1[0][0]                      \n__________________________________________________________________________________________________\na_in1 (InputLayer)              [(None, 69, 69)]     0                                            \n__________________________________________________________________________________________________\ngcn_conv_3 (GCNConv)            (None, 69, 78)       2574        dropout_3[0][0]                  \n                                                                 a_in2[0][0]                      \n__________________________________________________________________________________________________\ngcn_conv (GCNConv)              (None, 69, 78)       2574        dropout[0][0]                    \n                                                                 a_in1[0][0]                      \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 69, 78)       0           gcn_conv_3[0][0]                 \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 69, 78)       0           gcn_conv[0][0]                   \n__________________________________________________________________________________________________\ngcn_conv_4 (GCNConv)            (None, 69, 156)      12324       dropout_4[0][0]                  \n                                                                 a_in2[0][0]                      \n__________________________________________________________________________________________________\ngcn_conv_1 (GCNConv)            (None, 69, 156)      12324       dropout_1[0][0]                  \n                                                                 a_in1[0][0]                      \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            [(None, 934)]        0                                            \n__________________________________________________________________________________________________\ngcn_conv_5 (GCNConv)            (None, 69, 312)      48984       gcn_conv_4[0][0]                 \n                                                                 a_in2[0][0]                      \n__________________________________________________________________________________________________\ngcn_conv_2 (GCNConv)            (None, 69, 312)      48984       gcn_conv_1[0][0]                 \n                                                                 a_in1[0][0]                      \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 512)          478720      input_1[0][0]                    \n__________________________________________________________________________________________________\nglobal_max_pool_1 (GlobalMaxPoo (None, 312)          0           gcn_conv_5[0][0]                 \n__________________________________________________________________________________________________\nglobal_max_pool (GlobalMaxPool) (None, 312)          0           gcn_conv_2[0][0]                 \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 256)          80128       global_max_pool_1[0][0]          \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 256)          80128       global_max_pool[0][0]            \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 265)          135945      dropout_6[0][0]                  \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 256)          0           dense_2[0][0]                    \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 256)          0           dense[0][0]                      \n__________________________________________________________________________________________________\ndropout_7 (Dropout)             (None, 265)          0           dense_5[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 128)          32896       dropout_5[0][0]                  \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 128)          32896       dropout_2[0][0]                  \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 128)          34048       dropout_7[0][0]                  \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 384)          0           dense_3[0][0]                    \n                                                                 dense_1[0][0]                    \n                                                                 dense_6[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 384)          1536        concatenate[0][0]                \n__________________________________________________________________________________________________\nmulti_head_attention (MultiHead (None, None, 384)    591360      batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\nmulti_head_attention_1 (MultiHe (None, None, 384)    591360      batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 384)          0           multi_head_attention[0][0]       \n__________________________________________________________________________________________________\nreshape_1 (Reshape)             (None, 384)          0           multi_head_attention_1[0][0]     \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 768)          0           reshape[0][0]                    \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 768)          0           reshape_1[0][0]                  \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\ncross_stitch (CrossStitch)      ((None, 768), (None, 4           concatenate_1[0][0]              \n                                                                 concatenate_2[0][0]              \n__________________________________________________________________________________________________\ndense_15 (Dense)                (None, 1024)         787456      cross_stitch[0][0]               \n__________________________________________________________________________________________________\ndense_16 (Dense)                (None, 1024)         787456      cross_stitch[0][1]               \n__________________________________________________________________________________________________\ncross_stitch_1 (CrossStitch)    ((None, 1024), (None 4           dense_15[0][0]                   \n                                                                 dense_16[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 1792)         0           cross_stitch_1[0][0]             \n                                                                 concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndense_17 (Dense)                (None, 128)          229504      concatenate_3[0][0]              \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 1792)         0           cross_stitch_1[0][1]             \n                                                                 concatenate_2[0][0]              \n__________________________________________________________________________________________________\np_re_lu (PReLU)                 (None, 128)          128         dense_17[0][0]                   \n__________________________________________________________________________________________________\ndense_18 (Dense)                (None, 128)          229504      concatenate_4[0][0]              \n__________________________________________________________________________________________________\nsynergy (Dense)                 (None, 64)           8256        p_re_lu[0][0]                    \n__________________________________________________________________________________________________\np_re_lu_1 (PReLU)               (None, 128)          128         dense_18[0][0]                   \n__________________________________________________________________________________________________\np_re_lu_2 (PReLU)               (None, 64)           64          synergy[0][0]                    \n__________________________________________________________________________________________________\nsenstivity (Dense)              (None, 64)           8256        p_re_lu_1[0][0]                  \n__________________________________________________________________________________________________\ndense_19 (Dense)                (None, 1)            65          p_re_lu_2[0][0]                  \n__________________________________________________________________________________________________\ndense_20 (Dense)                (None, 1)            65          senstivity[0][0]                 \n==================================================================================================\nTotal params: 4,237,671\nTrainable params: 4,236,903\nNon-trainable params: 768\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n2022-12-12 14:45:09.896377: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 254820864 exceeds 10% of free system memory.\n2022-12-12 14:45:10.375408: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 549457488 exceeds 10% of free system memory.\n2022-12-12 14:45:11.058207: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 254820864 exceeds 10% of free system memory.\n2022-12-12 14:45:11.612306: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 549457488 exceeds 10% of free system memory.\n2022-12-12 14:45:12.450550: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 254820864 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n451/451 [==============================] - 12s 15ms/step - loss: 778.7999 - dense_19_loss: 517.5131 - dense_20_loss: 132.2431\nEpoch 2/500\n451/451 [==============================] - 7s 16ms/step - loss: 700.0775 - dense_19_loss: 482.5902 - dense_20_loss: 91.9822\nEpoch 3/500\n451/451 [==============================] - 7s 16ms/step - loss: 675.2711 - dense_19_loss: 466.8318 - dense_20_loss: 84.9942\nEpoch 4/500\n451/451 [==============================] - 7s 15ms/step - loss: 655.4475 - dense_19_loss: 453.9673 - dense_20_loss: 79.6841\nEpoch 5/500\n451/451 [==============================] - 7s 15ms/step - loss: 635.5797 - dense_19_loss: 439.9186 - dense_20_loss: 75.1273\nEpoch 6/500\n451/451 [==============================] - 7s 16ms/step - loss: 618.0560 - dense_19_loss: 426.1363 - dense_20_loss: 72.5509\nEpoch 7/500\n451/451 [==============================] - 7s 15ms/step - loss: 601.2322 - dense_19_loss: 413.4291 - dense_20_loss: 69.4450\nEpoch 8/500\n451/451 [==============================] - 7s 15ms/step - loss: 589.1182 - dense_19_loss: 404.5870 - dense_20_loss: 67.2576\nEpoch 9/500\n451/451 [==============================] - 7s 15ms/step - loss: 575.8300 - dense_19_loss: 395.1833 - dense_20_loss: 64.3711\nEpoch 10/500\n451/451 [==============================] - 7s 15ms/step - loss: 563.7088 - dense_19_loss: 386.1325 - dense_20_loss: 62.2357\nEpoch 11/500\n451/451 [==============================] - 7s 16ms/step - loss: 548.7762 - dense_19_loss: 373.9984 - dense_20_loss: 60.2405\nEpoch 12/500\n451/451 [==============================] - 7s 15ms/step - loss: 535.2712 - dense_19_loss: 363.3110 - dense_20_loss: 58.2847\nEpoch 13/500\n451/451 [==============================] - 7s 15ms/step - loss: 526.6603 - dense_19_loss: 357.4527 - dense_20_loss: 56.3072\nEpoch 14/500\n451/451 [==============================] - 7s 15ms/step - loss: 516.3674 - dense_19_loss: 349.0515 - dense_20_loss: 55.1447\nEpoch 15/500\n451/451 [==============================] - 7s 16ms/step - loss: 503.3195 - dense_19_loss: 338.7990 - dense_20_loss: 52.9622\nEpoch 16/500\n451/451 [==============================] - 7s 15ms/step - loss: 497.9075 - dense_19_loss: 335.1331 - dense_20_loss: 51.8723\nEpoch 17/500\n451/451 [==============================] - 7s 15ms/step - loss: 486.3879 - dense_19_loss: 327.3302 - dense_20_loss: 48.7779\nEpoch 18/500\n451/451 [==============================] - 7s 15ms/step - loss: 480.5648 - dense_19_loss: 322.9696 - dense_20_loss: 48.0022\nEpoch 19/500\n451/451 [==============================] - 7s 15ms/step - loss: 474.1883 - dense_19_loss: 318.6362 - dense_20_loss: 46.5027\nEpoch 20/500\n451/451 [==============================] - 7s 16ms/step - loss: 457.9035 - dense_19_loss: 305.4379 - dense_20_loss: 43.9249\nEpoch 21/500\n451/451 [==============================] - 7s 16ms/step - loss: 459.8150 - dense_19_loss: 308.5891 - dense_20_loss: 43.1308\nEpoch 22/500\n451/451 [==============================] - 7s 15ms/step - loss: 442.4820 - dense_19_loss: 293.1100 - dense_20_loss: 41.7614\nEpoch 23/500\n451/451 [==============================] - 7s 15ms/step - loss: 441.2994 - dense_19_loss: 293.7741 - dense_20_loss: 40.4135\nEpoch 24/500\n451/451 [==============================] - 7s 16ms/step - loss: 435.8591 - dense_19_loss: 289.8328 - dense_20_loss: 39.3148\nEpoch 25/500\n451/451 [==============================] - 7s 15ms/step - loss: 434.1988 - dense_19_loss: 289.6995 - dense_20_loss: 38.2283\nEpoch 26/500\n451/451 [==============================] - 7s 15ms/step - loss: 423.0251 - dense_19_loss: 280.5098 - dense_20_loss: 36.6794\nEpoch 27/500\n451/451 [==============================] - 7s 15ms/step - loss: 410.1344 - dense_19_loss: 268.7266 - dense_20_loss: 35.8850\nEpoch 28/500\n451/451 [==============================] - 7s 15ms/step - loss: 403.5346 - dense_19_loss: 264.1298 - dense_20_loss: 34.2403\nEpoch 29/500\n451/451 [==============================] - 7s 16ms/step - loss: 398.2416 - dense_19_loss: 259.7458 - dense_20_loss: 33.7291\nEpoch 30/500\n451/451 [==============================] - 7s 16ms/step - loss: 386.4681 - dense_19_loss: 249.6167 - dense_20_loss: 32.4465\nEpoch 31/500\n451/451 [==============================] - 7s 15ms/step - loss: 386.8222 - dense_19_loss: 250.5075 - dense_20_loss: 32.2797\nEpoch 32/500\n451/451 [==============================] - 7s 15ms/step - loss: 376.5195 - dense_19_loss: 241.9915 - dense_20_loss: 30.7352\nEpoch 33/500\n451/451 [==============================] - 7s 16ms/step - loss: 363.3919 - dense_19_loss: 230.0815 - dense_20_loss: 29.8459\nEpoch 34/500\n451/451 [==============================] - 7s 15ms/step - loss: 360.7050 - dense_19_loss: 228.3562 - dense_20_loss: 29.2173\nEpoch 35/500\n451/451 [==============================] - 7s 15ms/step - loss: 351.1131 - dense_19_loss: 219.3360 - dense_20_loss: 28.9483\nEpoch 36/500\n451/451 [==============================] - 7s 16ms/step - loss: 352.2888 - dense_19_loss: 221.5047 - dense_20_loss: 28.2364\nEpoch 37/500\n451/451 [==============================] - 7s 15ms/step - loss: 339.0783 - dense_19_loss: 209.3556 - dense_20_loss: 27.3763\nEpoch 38/500\n451/451 [==============================] - 7s 16ms/step - loss: 338.5290 - dense_19_loss: 209.7385 - dense_20_loss: 26.8121\nEpoch 39/500\n451/451 [==============================] - 7s 15ms/step - loss: 335.7507 - dense_19_loss: 207.4698 - dense_20_loss: 26.4945\nEpoch 40/500\n451/451 [==============================] - 7s 15ms/step - loss: 333.7596 - dense_19_loss: 205.9624 - dense_20_loss: 26.2016\nEpoch 41/500\n451/451 [==============================] - 7s 16ms/step - loss: 326.6510 - dense_19_loss: 199.6368 - dense_20_loss: 25.6360\nEpoch 42/500\n451/451 [==============================] - 7s 16ms/step - loss: 319.6621 - dense_19_loss: 193.8811 - dense_20_loss: 24.7151\nEpoch 43/500\n451/451 [==============================] - 7s 15ms/step - loss: 315.9238 - dense_19_loss: 190.8882 - dense_20_loss: 24.3268\nEpoch 44/500\n451/451 [==============================] - 7s 16ms/step - loss: 308.7906 - dense_19_loss: 184.9486 - dense_20_loss: 23.4064\nEpoch 45/500\n451/451 [==============================] - 7s 15ms/step - loss: 297.6861 - dense_19_loss: 174.7852 - dense_20_loss: 22.7562\nEpoch 46/500\n451/451 [==============================] - 7s 16ms/step - loss: 298.5728 - dense_19_loss: 176.9021 - dense_20_loss: 21.7756\nEpoch 47/500\n451/451 [==============================] - 7s 15ms/step - loss: 294.2317 - dense_19_loss: 172.5140 - dense_20_loss: 22.0764\nEpoch 48/500\n451/451 [==============================] - 7s 15ms/step - loss: 295.2981 - dense_19_loss: 174.0762 - dense_20_loss: 21.7519\nEpoch 49/500\n451/451 [==============================] - 7s 16ms/step - loss: 288.5116 - dense_19_loss: 167.8148 - dense_20_loss: 21.4555\nEpoch 50/500\n451/451 [==============================] - 7s 15ms/step - loss: 290.6621 - dense_19_loss: 170.7152 - dense_20_loss: 20.9400\nEpoch 51/500\n451/451 [==============================] - 7s 16ms/step - loss: 279.4435 - dense_19_loss: 160.2437 - dense_20_loss: 20.3815\nEpoch 52/500\n451/451 [==============================] - 7s 15ms/step - loss: 287.4392 - dense_19_loss: 168.4477 - dense_20_loss: 20.3778\nEpoch 53/500\n451/451 [==============================] - 7s 15ms/step - loss: 274.4279 - dense_19_loss: 156.4912 - dense_20_loss: 19.5617\nEpoch 54/500\n451/451 [==============================] - 7s 15ms/step - loss: 271.2326 - dense_19_loss: 154.3799 - dense_20_loss: 18.7421\nEpoch 55/500\n451/451 [==============================] - 7s 16ms/step - loss: 267.4945 - dense_19_loss: 150.9998 - dense_20_loss: 18.5490\nEpoch 56/500\n451/451 [==============================] - 7s 15ms/step - loss: 259.3391 - dense_19_loss: 143.3556 - dense_20_loss: 18.3215\nEpoch 57/500\n451/451 [==============================] - 7s 15ms/step - loss: 258.6569 - dense_19_loss: 143.4618 - dense_20_loss: 17.7117\nEpoch 58/500\n451/451 [==============================] - 7s 15ms/step - loss: 257.2684 - dense_19_loss: 142.1874 - dense_20_loss: 17.7962\nEpoch 59/500\n451/451 [==============================] - 7s 15ms/step - loss: 249.7645 - dense_19_loss: 135.8296 - dense_20_loss: 16.8579\nEpoch 60/500\n451/451 [==============================] - 8s 17ms/step - loss: 249.1352 - dense_19_loss: 135.1368 - dense_20_loss: 17.1223\nEpoch 61/500\n451/451 [==============================] - 7s 15ms/step - loss: 245.4805 - dense_19_loss: 132.0518 - dense_20_loss: 16.8001\nEpoch 62/500\n451/451 [==============================] - 7s 15ms/step - loss: 243.1955 - dense_19_loss: 129.9141 - dense_20_loss: 16.7698\nEpoch 63/500\n451/451 [==============================] - 7s 15ms/step - loss: 240.1976 - dense_19_loss: 128.0576 - dense_20_loss: 15.9385\nEpoch 64/500\n451/451 [==============================] - 7s 16ms/step - loss: 239.8232 - dense_19_loss: 127.9720 - dense_20_loss: 15.8012\nEpoch 65/500\n451/451 [==============================] - 7s 15ms/step - loss: 237.0715 - dense_19_loss: 125.7124 - dense_20_loss: 15.5934\nEpoch 66/500\n451/451 [==============================] - 7s 16ms/step - loss: 243.3348 - dense_19_loss: 131.9602 - dense_20_loss: 15.6339\nEpoch 67/500\n451/451 [==============================] - 7s 15ms/step - loss: 236.0168 - dense_19_loss: 124.7908 - dense_20_loss: 15.6853\nEpoch 68/500\n451/451 [==============================] - 7s 16ms/step - loss: 223.8667 - dense_19_loss: 114.0137 - dense_20_loss: 14.6122\nEpoch 69/500\n451/451 [==============================] - 7s 16ms/step - loss: 218.4533 - dense_19_loss: 109.4512 - dense_20_loss: 13.9904\nEpoch 70/500\n451/451 [==============================] - 7s 15ms/step - loss: 214.2707 - dense_19_loss: 105.4503 - dense_20_loss: 14.0371\nEpoch 71/500\n451/451 [==============================] - 7s 15ms/step - loss: 218.7428 - dense_19_loss: 109.8721 - dense_20_loss: 14.1784\nEpoch 72/500\n451/451 [==============================] - 7s 15ms/step - loss: 216.8164 - dense_19_loss: 108.6007 - dense_20_loss: 13.7345\nEpoch 73/500\n451/451 [==============================] - 8s 17ms/step - loss: 218.2693 - dense_19_loss: 109.9639 - dense_20_loss: 13.9697\nEpoch 74/500\n451/451 [==============================] - 7s 15ms/step - loss: 211.7281 - dense_19_loss: 104.1351 - dense_20_loss: 13.4379\nEpoch 75/500\n451/451 [==============================] - 7s 15ms/step - loss: 210.1880 - dense_19_loss: 103.0371 - dense_20_loss: 13.2831\nEpoch 76/500\n451/451 [==============================] - 7s 15ms/step - loss: 210.2154 - dense_19_loss: 103.2530 - dense_20_loss: 13.2381\nEpoch 77/500\n451/451 [==============================] - 7s 15ms/step - loss: 205.5777 - dense_19_loss: 98.9958 - dense_20_loss: 13.0328\nEpoch 78/500\n451/451 [==============================] - 7s 16ms/step - loss: 210.6416 - dense_19_loss: 103.8863 - dense_20_loss: 13.2697\nEpoch 79/500\n451/451 [==============================] - 7s 15ms/step - loss: 198.1882 - dense_19_loss: 92.7483 - dense_20_loss: 12.1680\nEpoch 80/500\n451/451 [==============================] - 7s 15ms/step - loss: 200.7802 - dense_19_loss: 95.3005 - dense_20_loss: 12.4247\nEpoch 81/500\n451/451 [==============================] - 7s 15ms/step - loss: 200.0026 - dense_19_loss: 94.5727 - dense_20_loss: 12.4377\nEpoch 82/500\n451/451 [==============================] - 7s 16ms/step - loss: 196.8155 - dense_19_loss: 91.9010 - dense_20_loss: 12.2404\nEpoch 83/500\n451/451 [==============================] - 7s 15ms/step - loss: 191.8624 - dense_19_loss: 87.4919 - dense_20_loss: 11.8825\nEpoch 84/500\n451/451 [==============================] - 7s 15ms/step - loss: 190.3537 - dense_19_loss: 86.4834 - dense_20_loss: 11.6043\nEpoch 85/500\n451/451 [==============================] - 7s 16ms/step - loss: 188.0607 - dense_19_loss: 84.4317 - dense_20_loss: 11.5873\nEpoch 86/500\n451/451 [==============================] - 7s 15ms/step - loss: 185.3013 - dense_19_loss: 82.1955 - dense_20_loss: 11.2847\nEpoch 87/500\n451/451 [==============================] - 7s 16ms/step - loss: 185.1399 - dense_19_loss: 82.1171 - dense_20_loss: 11.3137\nEpoch 88/500\n451/451 [==============================] - 7s 15ms/step - loss: 183.0056 - dense_19_loss: 80.5410 - dense_20_loss: 11.0383\nEpoch 89/500\n451/451 [==============================] - 7s 15ms/step - loss: 181.2049 - dense_19_loss: 79.0657 - dense_20_loss: 10.8768\nEpoch 90/500\n451/451 [==============================] - 7s 16ms/step - loss: 177.2839 - dense_19_loss: 75.7327 - dense_20_loss: 10.5672\nEpoch 91/500\n451/451 [==============================] - 7s 16ms/step - loss: 180.3232 - dense_19_loss: 78.8516 - dense_20_loss: 10.6626\nEpoch 92/500\n451/451 [==============================] - 7s 15ms/step - loss: 176.2835 - dense_19_loss: 75.0614 - dense_20_loss: 10.5695\nEpoch 93/500\n451/451 [==============================] - 7s 15ms/step - loss: 173.6175 - dense_19_loss: 72.6978 - dense_20_loss: 10.5201\nEpoch 94/500\n451/451 [==============================] - 7s 15ms/step - loss: 173.4468 - dense_19_loss: 72.9783 - dense_20_loss: 10.2059\nEpoch 95/500\n451/451 [==============================] - 7s 15ms/step - loss: 171.0844 - dense_19_loss: 70.7857 - dense_20_loss: 10.2356\nEpoch 96/500\n451/451 [==============================] - 7s 16ms/step - loss: 166.2194 - dense_19_loss: 66.7033 - dense_20_loss: 9.7709\nEpoch 97/500\n451/451 [==============================] - 7s 15ms/step - loss: 167.9256 - dense_19_loss: 68.5889 - dense_20_loss: 9.8445\nEpoch 98/500\n451/451 [==============================] - 7s 15ms/step - loss: 164.5014 - dense_19_loss: 65.3654 - dense_20_loss: 9.8794\nEpoch 99/500\n451/451 [==============================] - 7s 15ms/step - loss: 172.2277 - dense_19_loss: 72.8402 - dense_20_loss: 10.1823\nEpoch 100/500\n451/451 [==============================] - 7s 16ms/step - loss: 164.5567 - dense_19_loss: 66.2005 - dense_20_loss: 9.4082\nEpoch 101/500\n451/451 [==============================] - 7s 16ms/step - loss: 166.1058 - dense_19_loss: 67.4659 - dense_20_loss: 9.8094\nEpoch 102/500\n451/451 [==============================] - 7s 15ms/step - loss: 160.8064 - dense_19_loss: 62.8454 - dense_20_loss: 9.3362\nEpoch 103/500\n451/451 [==============================] - 7s 16ms/step - loss: 158.7242 - dense_19_loss: 61.3264 - dense_20_loss: 9.0181\nEpoch 104/500\n451/451 [==============================] - 7s 15ms/step - loss: 157.6550 - dense_19_loss: 60.2385 - dense_20_loss: 9.2349\nEpoch 105/500\n451/451 [==============================] - 7s 16ms/step - loss: 162.4349 - dense_19_loss: 64.9439 - dense_20_loss: 9.4478\nEpoch 106/500\n451/451 [==============================] - 7s 15ms/step - loss: 154.9837 - dense_19_loss: 58.2973 - dense_20_loss: 8.9011\nEpoch 107/500\n451/451 [==============================] - 7s 16ms/step - loss: 153.7992 - dense_19_loss: 57.5579 - dense_20_loss: 8.7404\nEpoch 108/500\n451/451 [==============================] - 7s 15ms/step - loss: 151.8095 - dense_19_loss: 55.7065 - dense_20_loss: 8.8691\nEpoch 109/500\n451/451 [==============================] - 7s 16ms/step - loss: 151.7891 - dense_19_loss: 56.1883 - dense_20_loss: 8.6050\nEpoch 110/500\n451/451 [==============================] - 7s 15ms/step - loss: 153.0774 - dense_19_loss: 57.6913 - dense_20_loss: 8.6107\nEpoch 111/500\n451/451 [==============================] - 7s 15ms/step - loss: 151.4972 - dense_19_loss: 56.3855 - dense_20_loss: 8.5240\nEpoch 112/500\n451/451 [==============================] - 7s 15ms/step - loss: 157.2338 - dense_19_loss: 61.9041 - dense_20_loss: 8.8674\nEpoch 113/500\n451/451 [==============================] - 7s 15ms/step - loss: 151.7286 - dense_19_loss: 56.6972 - dense_20_loss: 8.7856\nEpoch 114/500\n451/451 [==============================] - 7s 16ms/step - loss: 148.3889 - dense_19_loss: 54.1877 - dense_20_loss: 8.2099\nEpoch 115/500\n451/451 [==============================] - 7s 15ms/step - loss: 144.7652 - dense_19_loss: 50.9057 - dense_20_loss: 8.0489\nEpoch 116/500\n451/451 [==============================] - 7s 15ms/step - loss: 146.0394 - dense_19_loss: 52.4830 - dense_20_loss: 8.0691\nEpoch 117/500\n451/451 [==============================] - 7s 15ms/step - loss: 142.4210 - dense_19_loss: 49.2317 - dense_20_loss: 7.9317\nEpoch 118/500\n451/451 [==============================] - 7s 16ms/step - loss: 147.5907 - dense_19_loss: 54.2921 - dense_20_loss: 8.2971\nEpoch 119/500\n451/451 [==============================] - 7s 15ms/step - loss: 140.8942 - dense_19_loss: 48.1489 - dense_20_loss: 7.8288\nEpoch 120/500\n451/451 [==============================] - 7s 15ms/step - loss: 142.0235 - dense_19_loss: 49.4456 - dense_20_loss: 8.0006\nEpoch 121/500\n451/451 [==============================] - 7s 15ms/step - loss: 146.7120 - dense_19_loss: 53.9239 - dense_20_loss: 8.1927\nEpoch 122/500\n451/451 [==============================] - 7s 15ms/step - loss: 136.3591 - dense_19_loss: 44.7287 - dense_20_loss: 7.4591\nEpoch 123/500\n451/451 [==============================] - 7s 16ms/step - loss: 136.3986 - dense_19_loss: 45.0713 - dense_20_loss: 7.5052\nEpoch 124/500\n451/451 [==============================] - 7s 15ms/step - loss: 141.7570 - dense_19_loss: 50.3312 - dense_20_loss: 7.7851\nEpoch 125/500\n451/451 [==============================] - 7s 16ms/step - loss: 136.0734 - dense_19_loss: 45.1778 - dense_20_loss: 7.5240\nEpoch 126/500\n451/451 [==============================] - 7s 15ms/step - loss: 133.5848 - dense_19_loss: 43.1045 - dense_20_loss: 7.3526\nEpoch 127/500\n451/451 [==============================] - 7s 16ms/step - loss: 135.2058 - dense_19_loss: 44.7731 - dense_20_loss: 7.5438\nEpoch 128/500\n451/451 [==============================] - 7s 15ms/step - loss: 131.4250 - dense_19_loss: 41.7715 - dense_20_loss: 7.1120\nEpoch 129/500\n451/451 [==============================] - 7s 15ms/step - loss: 133.7266 - dense_19_loss: 44.0003 - dense_20_loss: 7.4197\nEpoch 130/500\n451/451 [==============================] - 7s 15ms/step - loss: 132.7553 - dense_19_loss: 43.4706 - dense_20_loss: 7.2119\nEpoch 131/500\n451/451 [==============================] - 7s 15ms/step - loss: 129.4667 - dense_19_loss: 40.6679 - dense_20_loss: 7.0137\nEpoch 132/500\n451/451 [==============================] - 7s 16ms/step - loss: 130.4548 - dense_19_loss: 41.8312 - dense_20_loss: 7.1087\nEpoch 133/500\n451/451 [==============================] - 7s 15ms/step - loss: 129.8827 - dense_19_loss: 41.7575 - dense_20_loss: 6.9080\nEpoch 134/500\n451/451 [==============================] - 7s 15ms/step - loss: 128.4380 - dense_19_loss: 40.7003 - dense_20_loss: 6.8562\nEpoch 135/500\n451/451 [==============================] - 7s 15ms/step - loss: 130.3177 - dense_19_loss: 42.2600 - dense_20_loss: 7.2996\nEpoch 136/500\n451/451 [==============================] - 7s 16ms/step - loss: 128.4059 - dense_19_loss: 40.8194 - dense_20_loss: 7.0888\nEpoch 137/500\n451/451 [==============================] - 7s 15ms/step - loss: 129.7112 - dense_19_loss: 42.7593 - dense_20_loss: 6.7932\nEpoch 138/500\n451/451 [==============================] - 7s 15ms/step - loss: 128.7023 - dense_19_loss: 41.6866 - dense_20_loss: 7.0009\nEpoch 139/500\n451/451 [==============================] - 7s 15ms/step - loss: 130.9488 - dense_19_loss: 43.6416 - dense_20_loss: 7.3137\nEpoch 140/500\n451/451 [==============================] - 7s 15ms/step - loss: 122.2794 - dense_19_loss: 36.1665 - dense_20_loss: 6.5883\nEpoch 141/500\n451/451 [==============================] - 7s 16ms/step - loss: 123.6745 - dense_19_loss: 37.8228 - dense_20_loss: 6.6323\nEpoch 142/500\n451/451 [==============================] - 7s 16ms/step - loss: 120.6998 - dense_19_loss: 35.3090 - dense_20_loss: 6.4062\nEpoch 143/500\n451/451 [==============================] - 7s 15ms/step - loss: 124.0165 - dense_19_loss: 38.6193 - dense_20_loss: 6.6310\nEpoch 144/500\n451/451 [==============================] - 7s 15ms/step - loss: 123.1565 - dense_19_loss: 38.0476 - dense_20_loss: 6.5654\nEpoch 145/500\n451/451 [==============================] - 7s 16ms/step - loss: 124.1079 - dense_19_loss: 39.0086 - dense_20_loss: 6.8380\nEpoch 146/500\n451/451 [==============================] - 7s 15ms/step - loss: 119.1562 - dense_19_loss: 34.8930 - dense_20_loss: 6.3251\nEpoch 147/500\n451/451 [==============================] - 7s 15ms/step - loss: 124.7791 - dense_19_loss: 40.1408 - dense_20_loss: 6.7749\nEpoch 148/500\n451/451 [==============================] - 7s 15ms/step - loss: 117.6678 - dense_19_loss: 33.9090 - dense_20_loss: 6.1712\nEpoch 149/500\n451/451 [==============================] - 7s 15ms/step - loss: 117.9498 - dense_19_loss: 34.6380 - dense_20_loss: 6.1488\nEpoch 150/500\n451/451 [==============================] - 7s 16ms/step - loss: 116.4777 - dense_19_loss: 33.4539 - dense_20_loss: 6.1484\nEpoch 151/500\n451/451 [==============================] - 7s 16ms/step - loss: 123.8566 - dense_19_loss: 40.3221 - dense_20_loss: 6.7420\nEpoch 152/500\n451/451 [==============================] - 7s 15ms/step - loss: 127.5894 - dense_19_loss: 43.5498 - dense_20_loss: 7.0316\nEpoch 153/500\n451/451 [==============================] - 7s 15ms/step - loss: 115.1674 - dense_19_loss: 32.7576 - dense_20_loss: 5.9490\nEpoch 154/500\n451/451 [==============================] - 7s 16ms/step - loss: 112.7928 - dense_19_loss: 31.0792 - dense_20_loss: 5.6724\nEpoch 155/500\n451/451 [==============================] - 7s 15ms/step - loss: 112.6035 - dense_19_loss: 31.0421 - dense_20_loss: 5.8456\nEpoch 156/500\n451/451 [==============================] - 7s 15ms/step - loss: 111.3318 - dense_19_loss: 30.0946 - dense_20_loss: 5.8644\nEpoch 157/500\n451/451 [==============================] - 7s 15ms/step - loss: 111.9767 - dense_19_loss: 31.1260 - dense_20_loss: 5.7983\nEpoch 158/500\n451/451 [==============================] - 7s 15ms/step - loss: 119.9332 - dense_19_loss: 38.1152 - dense_20_loss: 6.5201\nEpoch 159/500\n451/451 [==============================] - 8s 17ms/step - loss: 112.2131 - dense_19_loss: 31.7846 - dense_20_loss: 5.6120\nEpoch 160/500\n451/451 [==============================] - 7s 15ms/step - loss: 110.8546 - dense_19_loss: 30.7261 - dense_20_loss: 5.7175\nEpoch 161/500\n451/451 [==============================] - 7s 15ms/step - loss: 109.7373 - dense_19_loss: 29.9625 - dense_20_loss: 5.7141\nEpoch 162/500\n451/451 [==============================] - 7s 15ms/step - loss: 109.4277 - dense_19_loss: 29.9992 - dense_20_loss: 5.5911\nEpoch 163/500\n451/451 [==============================] - 7s 16ms/step - loss: 110.8475 - dense_19_loss: 31.5556 - dense_20_loss: 5.7318\nEpoch 164/500\n451/451 [==============================] - 7s 15ms/step - loss: 108.9409 - dense_19_loss: 30.0349 - dense_20_loss: 5.6816\nEpoch 165/500\n451/451 [==============================] - 7s 15ms/step - loss: 114.7591 - dense_19_loss: 35.1924 - dense_20_loss: 6.1775\nEpoch 166/500\n451/451 [==============================] - 7s 16ms/step - loss: 114.2644 - dense_19_loss: 34.8000 - dense_20_loss: 5.9084\nEpoch 167/500\n451/451 [==============================] - 7s 15ms/step - loss: 107.1329 - dense_19_loss: 28.6098 - dense_20_loss: 5.5213\nEpoch 168/500\n451/451 [==============================] - 7s 17ms/step - loss: 105.2762 - dense_19_loss: 27.4213 - dense_20_loss: 5.3520\nEpoch 169/500\n451/451 [==============================] - 7s 16ms/step - loss: 105.0063 - dense_19_loss: 27.5810 - dense_20_loss: 5.2647\nEpoch 170/500\n451/451 [==============================] - 7s 16ms/step - loss: 105.4657 - dense_19_loss: 28.3247 - dense_20_loss: 5.2943\nEpoch 171/500\n451/451 [==============================] - 7s 15ms/step - loss: 105.1566 - dense_19_loss: 28.3685 - dense_20_loss: 5.2983\nEpoch 172/500\n451/451 [==============================] - 8s 17ms/step - loss: 104.9311 - dense_19_loss: 28.2376 - dense_20_loss: 5.4170\nEpoch 173/500\n451/451 [==============================] - 7s 16ms/step - loss: 108.9176 - dense_19_loss: 32.3559 - dense_20_loss: 5.5437\nEpoch 174/500\n451/451 [==============================] - 7s 15ms/step - loss: 105.1922 - dense_19_loss: 29.0466 - dense_20_loss: 5.3176\nEpoch 175/500\n451/451 [==============================] - 7s 16ms/step - loss: 103.1721 - dense_19_loss: 27.4138 - dense_20_loss: 5.2442\nEpoch 176/500\n451/451 [==============================] - 7s 16ms/step - loss: 101.7053 - dense_19_loss: 26.4781 - dense_20_loss: 5.0892\nEpoch 177/500\n451/451 [==============================] - 7s 16ms/step - loss: 100.4597 - dense_19_loss: 25.5723 - dense_20_loss: 5.0457\nEpoch 178/500\n451/451 [==============================] - 7s 16ms/step - loss: 102.4222 - dense_19_loss: 27.4326 - dense_20_loss: 5.3598\nEpoch 179/500\n451/451 [==============================] - 7s 15ms/step - loss: 103.6649 - dense_19_loss: 28.9003 - dense_20_loss: 5.4020\nEpoch 180/500\n451/451 [==============================] - 7s 16ms/step - loss: 104.8526 - dense_19_loss: 29.9801 - dense_20_loss: 5.5960\nEpoch 181/500\n451/451 [==============================] - 8s 18ms/step - loss: 105.4770 - dense_19_loss: 30.8296 - dense_20_loss: 5.5661\nEpoch 182/500\n451/451 [==============================] - 7s 15ms/step - loss: 101.3408 - dense_19_loss: 27.5204 - dense_20_loss: 5.0562\nEpoch 183/500\n451/451 [==============================] - 7s 15ms/step - loss: 103.5209 - dense_19_loss: 29.5217 - dense_20_loss: 5.3754\nEpoch 184/500\n451/451 [==============================] - 7s 15ms/step - loss: 102.6345 - dense_19_loss: 28.9014 - dense_20_loss: 5.2680\nEpoch 185/500\n451/451 [==============================] - 7s 16ms/step - loss: 100.2225 - dense_19_loss: 26.9474 - dense_20_loss: 5.1551\nEpoch 186/500\n451/451 [==============================] - 7s 15ms/step - loss: 97.1551 - dense_19_loss: 24.5093 - dense_20_loss: 4.8694\nEpoch 187/500\n451/451 [==============================] - 7s 16ms/step - loss: 95.8729 - dense_19_loss: 23.6233 - dense_20_loss: 4.7848\nEpoch 188/500\n451/451 [==============================] - 7s 15ms/step - loss: 96.4481 - dense_19_loss: 24.6121 - dense_20_loss: 4.6713\nEpoch 189/500\n451/451 [==============================] - 7s 15ms/step - loss: 96.2041 - dense_19_loss: 24.4094 - dense_20_loss: 4.9381\nEpoch 190/500\n451/451 [==============================] - 7s 16ms/step - loss: 96.6021 - dense_19_loss: 25.0665 - dense_20_loss: 4.9144\nEpoch 191/500\n451/451 [==============================] - 7s 15ms/step - loss: 100.3994 - dense_19_loss: 28.5586 - dense_20_loss: 5.3442\nEpoch 192/500\n451/451 [==============================] - 7s 15ms/step - loss: 98.5563 - dense_19_loss: 27.0251 - dense_20_loss: 5.1464\nEpoch 193/500\n451/451 [==============================] - 7s 15ms/step - loss: 102.0235 - dense_19_loss: 30.3329 - dense_20_loss: 5.4325\nEpoch 194/500\n451/451 [==============================] - 7s 16ms/step - loss: 100.9233 - dense_19_loss: 29.3765 - dense_20_loss: 5.2985\nEpoch 195/500\n451/451 [==============================] - 7s 15ms/step - loss: 95.5568 - dense_19_loss: 24.8539 - dense_20_loss: 4.7972\nEpoch 196/500\n451/451 [==============================] - 7s 15ms/step - loss: 94.3616 - dense_19_loss: 24.2038 - dense_20_loss: 4.6147\nEpoch 197/500\n451/451 [==============================] - 7s 15ms/step - loss: 93.5935 - dense_19_loss: 23.8710 - dense_20_loss: 4.4906\nEpoch 198/500\n451/451 [==============================] - 7s 15ms/step - loss: 91.3380 - dense_19_loss: 21.8456 - dense_20_loss: 4.5332\nEpoch 199/500\n451/451 [==============================] - 7s 16ms/step - loss: 92.2980 - dense_19_loss: 23.1478 - dense_20_loss: 4.5524\nEpoch 200/500\n451/451 [==============================] - 7s 15ms/step - loss: 94.3472 - dense_19_loss: 25.2210 - dense_20_loss: 4.7300\nEpoch 201/500\n451/451 [==============================] - 7s 15ms/step - loss: 99.3208 - dense_19_loss: 29.0837 - dense_20_loss: 5.4239\nEpoch 202/500\n451/451 [==============================] - 7s 15ms/step - loss: 91.5264 - dense_19_loss: 22.8534 - dense_20_loss: 4.4606\nEpoch 203/500\n451/451 [==============================] - 8s 17ms/step - loss: 92.8236 - dense_19_loss: 24.1012 - dense_20_loss: 4.8080\nEpoch 204/500\n451/451 [==============================] - 7s 15ms/step - loss: 96.6923 - dense_19_loss: 27.8171 - dense_20_loss: 5.0068\nEpoch 205/500\n451/451 [==============================] - 7s 16ms/step - loss: 102.4704 - dense_19_loss: 32.6400 - dense_20_loss: 5.5755\nEpoch 206/500\n451/451 [==============================] - 7s 15ms/step - loss: 90.4449 - dense_19_loss: 22.3579 - dense_20_loss: 4.4078\nEpoch 207/500\n451/451 [==============================] - 7s 15ms/step - loss: 89.2678 - dense_19_loss: 21.6312 - dense_20_loss: 4.3453\nEpoch 208/500\n451/451 [==============================] - 7s 16ms/step - loss: 88.5125 - dense_19_loss: 21.1981 - dense_20_loss: 4.3529\nEpoch 209/500\n451/451 [==============================] - 7s 15ms/step - loss: 89.4693 - dense_19_loss: 22.3018 - dense_20_loss: 4.4622\nEpoch 210/500\n451/451 [==============================] - 7s 15ms/step - loss: 88.0900 - dense_19_loss: 21.3161 - dense_20_loss: 4.3462\nEpoch 211/500\n451/451 [==============================] - 7s 16ms/step - loss: 87.7819 - dense_19_loss: 21.3712 - dense_20_loss: 4.2426\nEpoch 212/500\n451/451 [==============================] - 7s 16ms/step - loss: 89.9294 - dense_19_loss: 23.4368 - dense_20_loss: 4.5317\nEpoch 213/500\n451/451 [==============================] - 7s 16ms/step - loss: 94.6348 - dense_19_loss: 27.4311 - dense_20_loss: 5.0313\nEpoch 214/500\n451/451 [==============================] - 7s 15ms/step - loss: 90.4349 - dense_19_loss: 24.1701 - dense_20_loss: 4.5031\nEpoch 215/500\n451/451 [==============================] - 7s 15ms/step - loss: 90.0938 - dense_19_loss: 23.7667 - dense_20_loss: 4.6150\nEpoch 216/500\n451/451 [==============================] - 7s 15ms/step - loss: 85.7989 - dense_19_loss: 20.3830 - dense_20_loss: 4.1835\nEpoch 217/500\n451/451 [==============================] - 7s 16ms/step - loss: 91.3843 - dense_19_loss: 26.0236 - dense_20_loss: 4.3950\nEpoch 218/500\n451/451 [==============================] - 7s 15ms/step - loss: 92.5054 - dense_19_loss: 26.6055 - dense_20_loss: 4.7747\nEpoch 219/500\n451/451 [==============================] - 7s 15ms/step - loss: 92.3932 - dense_19_loss: 26.4631 - dense_20_loss: 4.7967\nEpoch 220/500\n451/451 [==============================] - 7s 15ms/step - loss: 85.7672 - dense_19_loss: 20.8956 - dense_20_loss: 4.1977\nEpoch 221/500\n451/451 [==============================] - 7s 16ms/step - loss: 84.4062 - dense_19_loss: 20.0526 - dense_20_loss: 4.0423\nEpoch 222/500\n451/451 [==============================] - 7s 16ms/step - loss: 83.1584 - dense_19_loss: 19.1060 - dense_20_loss: 4.0411\nEpoch 223/500\n451/451 [==============================] - 7s 15ms/step - loss: 84.2361 - dense_19_loss: 20.4325 - dense_20_loss: 4.0357\nEpoch 224/500\n451/451 [==============================] - 7s 16ms/step - loss: 83.2580 - dense_19_loss: 19.7538 - dense_20_loss: 4.0500\nEpoch 225/500\n451/451 [==============================] - 7s 15ms/step - loss: 83.9187 - dense_19_loss: 20.5408 - dense_20_loss: 4.1730\nEpoch 226/500\n451/451 [==============================] - 7s 16ms/step - loss: 82.8466 - dense_19_loss: 19.8355 - dense_20_loss: 4.0745\nEpoch 227/500\n451/451 [==============================] - 7s 16ms/step - loss: 83.7235 - dense_19_loss: 20.8683 - dense_20_loss: 4.1557\nEpoch 228/500\n451/451 [==============================] - 7s 16ms/step - loss: 89.5299 - dense_19_loss: 26.5402 - dense_20_loss: 4.3733\nEpoch 229/500\n451/451 [==============================] - 7s 15ms/step - loss: 96.4896 - dense_19_loss: 31.8228 - dense_20_loss: 5.4131\nEpoch 230/500\n451/451 [==============================] - 8s 17ms/step - loss: 84.3152 - dense_19_loss: 21.4523 - dense_20_loss: 4.1418\nEpoch 231/500\n451/451 [==============================] - 7s 15ms/step - loss: 80.9621 - dense_19_loss: 18.7685 - dense_20_loss: 3.8508\nEpoch 232/500\n451/451 [==============================] - 7s 15ms/step - loss: 80.1117 - dense_19_loss: 18.3285 - dense_20_loss: 3.7753\nEpoch 233/500\n451/451 [==============================] - 7s 16ms/step - loss: 81.2458 - dense_19_loss: 19.5886 - dense_20_loss: 3.8969\nEpoch 234/500\n451/451 [==============================] - 7s 15ms/step - loss: 81.4960 - dense_19_loss: 19.9078 - dense_20_loss: 4.0126\nEpoch 235/500\n451/451 [==============================] - 7s 16ms/step - loss: 81.2394 - dense_19_loss: 19.8372 - dense_20_loss: 4.0465\nEpoch 236/500\n451/451 [==============================] - 7s 16ms/step - loss: 89.2758 - dense_19_loss: 27.3135 - dense_20_loss: 4.5292\nEpoch 237/500\n451/451 [==============================] - 7s 15ms/step - loss: 84.1347 - dense_19_loss: 22.5759 - dense_20_loss: 4.2889\nEpoch 238/500\n451/451 [==============================] - 7s 15ms/step - loss: 80.3434 - dense_19_loss: 19.4505 - dense_20_loss: 3.9338\nEpoch 239/500\n451/451 [==============================] - 7s 16ms/step - loss: 79.1527 - dense_19_loss: 18.7308 - dense_20_loss: 3.8044\nEpoch 240/500\n451/451 [==============================] - 7s 15ms/step - loss: 78.2456 - dense_19_loss: 18.1554 - dense_20_loss: 3.7757\nEpoch 241/500\n451/451 [==============================] - 7s 15ms/step - loss: 81.1074 - dense_19_loss: 20.9589 - dense_20_loss: 3.9585\nEpoch 242/500\n451/451 [==============================] - 7s 15ms/step - loss: 78.2801 - dense_19_loss: 18.5329 - dense_20_loss: 3.8163\nEpoch 243/500\n451/451 [==============================] - 7s 15ms/step - loss: 79.1510 - dense_19_loss: 19.5648 - dense_20_loss: 3.8645\nEpoch 244/500\n451/451 [==============================] - 7s 16ms/step - loss: 79.9715 - dense_19_loss: 20.1043 - dense_20_loss: 4.1312\nEpoch 245/500\n451/451 [==============================] - 7s 15ms/step - loss: 77.1692 - dense_19_loss: 18.1192 - dense_20_loss: 3.7506\nEpoch 246/500\n451/451 [==============================] - 7s 16ms/step - loss: 81.8784 - dense_19_loss: 22.5239 - dense_20_loss: 4.1197\nEpoch 247/500\n451/451 [==============================] - 7s 15ms/step - loss: 79.6395 - dense_19_loss: 20.6955 - dense_20_loss: 3.9377\nEpoch 248/500\n451/451 [==============================] - 7s 16ms/step - loss: 79.7949 - dense_19_loss: 20.7987 - dense_20_loss: 4.0219\nEpoch 249/500\n451/451 [==============================] - 7s 15ms/step - loss: 78.8384 - dense_19_loss: 20.1349 - dense_20_loss: 3.8796\nEpoch 250/500\n451/451 [==============================] - 7s 15ms/step - loss: 76.3551 - dense_19_loss: 18.2469 - dense_20_loss: 3.6660\nEpoch 251/500\n451/451 [==============================] - 7s 15ms/step - loss: 75.7367 - dense_19_loss: 17.7986 - dense_20_loss: 3.7105\nEpoch 252/500\n451/451 [==============================] - 7s 15ms/step - loss: 76.0506 - dense_19_loss: 18.2783 - dense_20_loss: 3.7775\nEpoch 253/500\n451/451 [==============================] - 7s 16ms/step - loss: 75.1652 - dense_19_loss: 17.9051 - dense_20_loss: 3.4991\nEpoch 254/500\n451/451 [==============================] - 7s 15ms/step - loss: 75.0900 - dense_19_loss: 17.8201 - dense_20_loss: 3.7058\nEpoch 255/500\n451/451 [==============================] - 7s 15ms/step - loss: 74.9405 - dense_19_loss: 18.0214 - dense_20_loss: 3.6026\nEpoch 256/500\n451/451 [==============================] - 7s 15ms/step - loss: 74.0618 - dense_19_loss: 17.2941 - dense_20_loss: 3.6622\nEpoch 257/500\n451/451 [==============================] - 7s 16ms/step - loss: 75.8663 - dense_19_loss: 19.2568 - dense_20_loss: 3.7187\nEpoch 258/500\n451/451 [==============================] - 7s 15ms/step - loss: 76.8926 - dense_19_loss: 20.2556 - dense_20_loss: 3.8164\nEpoch 259/500\n451/451 [==============================] - 7s 15ms/step - loss: 77.8542 - dense_19_loss: 20.8609 - dense_20_loss: 4.1564\nEpoch 260/500\n451/451 [==============================] - 7s 15ms/step - loss: 73.4965 - dense_19_loss: 17.3785 - dense_20_loss: 3.5831\nEpoch 261/500\n451/451 [==============================] - 7s 15ms/step - loss: 72.0721 - dense_19_loss: 16.4694 - dense_20_loss: 3.4132\nEpoch 262/500\n451/451 [==============================] - 8s 17ms/step - loss: 71.8508 - dense_19_loss: 16.3555 - dense_20_loss: 3.5616\nEpoch 263/500\n451/451 [==============================] - 7s 15ms/step - loss: 73.6272 - dense_19_loss: 18.1268 - dense_20_loss: 3.6693\nEpoch 264/500\n451/451 [==============================] - 7s 15ms/step - loss: 74.7803 - dense_19_loss: 19.2192 - dense_20_loss: 3.6985\nEpoch 265/500\n451/451 [==============================] - 7s 15ms/step - loss: 72.7722 - dense_19_loss: 17.6757 - dense_20_loss: 3.5401\nEpoch 266/500\n451/451 [==============================] - 7s 16ms/step - loss: 71.8109 - dense_19_loss: 17.0339 - dense_20_loss: 3.5278\nEpoch 267/500\n451/451 [==============================] - 7s 15ms/step - loss: 70.9027 - dense_19_loss: 16.4187 - dense_20_loss: 3.4839\nEpoch 268/500\n451/451 [==============================] - 7s 15ms/step - loss: 71.3655 - dense_19_loss: 17.0490 - dense_20_loss: 3.5068\nEpoch 269/500\n451/451 [==============================] - 7s 15ms/step - loss: 71.7427 - dense_19_loss: 17.5380 - dense_20_loss: 3.6032\nEpoch 270/500\n451/451 [==============================] - 7s 15ms/step - loss: 74.0632 - dense_19_loss: 19.6780 - dense_20_loss: 3.7079\nEpoch 271/500\n451/451 [==============================] - 7s 16ms/step - loss: 71.4290 - dense_19_loss: 17.5769 - dense_20_loss: 3.4109\nEpoch 272/500\n451/451 [==============================] - 7s 15ms/step - loss: 69.7679 - dense_19_loss: 16.2016 - dense_20_loss: 3.4219\nEpoch 273/500\n451/451 [==============================] - 7s 15ms/step - loss: 69.3910 - dense_19_loss: 16.0658 - dense_20_loss: 3.4012\nEpoch 274/500\n451/451 [==============================] - 7s 15ms/step - loss: 69.5106 - dense_19_loss: 16.3121 - dense_20_loss: 3.4463\nEpoch 275/500\n451/451 [==============================] - 7s 16ms/step - loss: 68.5854 - dense_19_loss: 15.7061 - dense_20_loss: 3.3451\nEpoch 276/500\n451/451 [==============================] - 7s 15ms/step - loss: 72.3031 - dense_19_loss: 19.2324 - dense_20_loss: 3.6205\nEpoch 277/500\n451/451 [==============================] - 7s 15ms/step - loss: 73.6248 - dense_19_loss: 20.1652 - dense_20_loss: 3.7948\nEpoch 278/500\n451/451 [==============================] - 7s 15ms/step - loss: 69.6465 - dense_19_loss: 16.9009 - dense_20_loss: 3.4392\nEpoch 279/500\n451/451 [==============================] - 7s 15ms/step - loss: 71.7366 - dense_19_loss: 18.8511 - dense_20_loss: 3.6781\nEpoch 280/500\n451/451 [==============================] - 8s 17ms/step - loss: 69.5870 - dense_19_loss: 16.9481 - dense_20_loss: 3.5225\nEpoch 281/500\n451/451 [==============================] - 7s 15ms/step - loss: 67.5580 - dense_19_loss: 15.5391 - dense_20_loss: 3.2461\nEpoch 282/500\n451/451 [==============================] - 7s 15ms/step - loss: 67.3898 - dense_19_loss: 15.6118 - dense_20_loss: 3.2480\nEpoch 283/500\n451/451 [==============================] - 7s 15ms/step - loss: 67.5912 - dense_19_loss: 15.9663 - dense_20_loss: 3.2560\nEpoch 284/500\n451/451 [==============================] - 7s 16ms/step - loss: 68.1456 - dense_19_loss: 16.5672 - dense_20_loss: 3.3008\nEpoch 285/500\n451/451 [==============================] - 7s 15ms/step - loss: 68.2542 - dense_19_loss: 16.7921 - dense_20_loss: 3.3809\nEpoch 286/500\n451/451 [==============================] - 7s 15ms/step - loss: 84.1314 - dense_19_loss: 30.3071 - dense_20_loss: 4.8829\nEpoch 287/500\n451/451 [==============================] - 7s 16ms/step - loss: 77.0237 - dense_19_loss: 23.4910 - dense_20_loss: 4.2808\nEpoch 288/500\n451/451 [==============================] - 7s 15ms/step - loss: 68.5029 - dense_19_loss: 16.3869 - dense_20_loss: 3.3771\nEpoch 289/500\n451/451 [==============================] - 7s 16ms/step - loss: 66.3913 - dense_19_loss: 14.9806 - dense_20_loss: 3.1443\nEpoch 290/500\n451/451 [==============================] - 7s 16ms/step - loss: 64.5698 - dense_19_loss: 13.5600 - dense_20_loss: 3.1210\nEpoch 291/500\n451/451 [==============================] - 7s 15ms/step - loss: 65.4328 - dense_19_loss: 14.7838 - dense_20_loss: 3.0447\nEpoch 292/500\n451/451 [==============================] - 7s 16ms/step - loss: 64.7105 - dense_19_loss: 14.3249 - dense_20_loss: 3.0605\nEpoch 293/500\n451/451 [==============================] - 7s 16ms/step - loss: 64.8320 - dense_19_loss: 14.5940 - dense_20_loss: 3.1141\nEpoch 294/500\n451/451 [==============================] - 7s 15ms/step - loss: 67.6288 - dense_19_loss: 17.1970 - dense_20_loss: 3.3288\nEpoch 295/500\n451/451 [==============================] - 7s 15ms/step - loss: 70.4074 - dense_19_loss: 19.5381 - dense_20_loss: 3.6883\nEpoch 296/500\n451/451 [==============================] - 7s 15ms/step - loss: 70.1260 - dense_19_loss: 19.4341 - dense_20_loss: 3.5253\nEpoch 297/500\n451/451 [==============================] - 7s 15ms/step - loss: 80.6690 - dense_19_loss: 28.3375 - dense_20_loss: 4.5606\nEpoch 298/500\n451/451 [==============================] - 7s 16ms/step - loss: 68.3959 - dense_19_loss: 17.5145 - dense_20_loss: 3.3799\nEpoch 299/500\n451/451 [==============================] - 7s 15ms/step - loss: 64.5162 - dense_19_loss: 14.3764 - dense_20_loss: 3.0952\nEpoch 300/500\n451/451 [==============================] - 7s 15ms/step - loss: 63.7122 - dense_19_loss: 14.0637 - dense_20_loss: 2.9460\nEpoch 301/500\n451/451 [==============================] - 7s 15ms/step - loss: 65.8255 - dense_19_loss: 16.0677 - dense_20_loss: 3.1740\nEpoch 302/500\n451/451 [==============================] - 7s 16ms/step - loss: 65.4842 - dense_19_loss: 15.8306 - dense_20_loss: 3.2331\nEpoch 303/500\n451/451 [==============================] - 7s 15ms/step - loss: 64.9230 - dense_19_loss: 15.5352 - dense_20_loss: 3.0771\nEpoch 304/500\n451/451 [==============================] - 7s 15ms/step - loss: 64.0365 - dense_19_loss: 14.8621 - dense_20_loss: 3.1605\nEpoch 305/500\n451/451 [==============================] - 7s 15ms/step - loss: 64.2219 - dense_19_loss: 15.0292 - dense_20_loss: 3.1829\nEpoch 306/500\n451/451 [==============================] - 7s 15ms/step - loss: 72.6264 - dense_19_loss: 22.1233 - dense_20_loss: 4.0975\nEpoch 307/500\n451/451 [==============================] - 7s 16ms/step - loss: 65.5035 - dense_19_loss: 16.0765 - dense_20_loss: 3.2125\nEpoch 308/500\n451/451 [==============================] - 7s 15ms/step - loss: 63.3034 - dense_19_loss: 14.4355 - dense_20_loss: 3.0706\nEpoch 309/500\n451/451 [==============================] - 7s 15ms/step - loss: 63.2552 - dense_19_loss: 14.6574 - dense_20_loss: 3.0080\nEpoch 310/500\n451/451 [==============================] - 7s 15ms/step - loss: 62.0177 - dense_19_loss: 13.8460 - dense_20_loss: 2.8847\nEpoch 311/500\n451/451 [==============================] - 7s 16ms/step - loss: 62.4420 - dense_19_loss: 14.4442 - dense_20_loss: 2.9161\nEpoch 312/500\n451/451 [==============================] - 7s 16ms/step - loss: 64.2951 - dense_19_loss: 16.0350 - dense_20_loss: 3.2198\nEpoch 313/500\n451/451 [==============================] - 7s 15ms/step - loss: 62.5423 - dense_19_loss: 14.7242 - dense_20_loss: 2.9805\nEpoch 314/500\n451/451 [==============================] - 7s 16ms/step - loss: 66.6558 - dense_19_loss: 18.2164 - dense_20_loss: 3.3928\nEpoch 315/500\n451/451 [==============================] - 7s 16ms/step - loss: 64.8771 - dense_19_loss: 16.9096 - dense_20_loss: 3.1820\nEpoch 316/500\n451/451 [==============================] - 7s 16ms/step - loss: 68.6749 - dense_19_loss: 20.2061 - dense_20_loss: 3.4935\nEpoch 317/500\n451/451 [==============================] - 7s 15ms/step - loss: 61.9656 - dense_19_loss: 14.3052 - dense_20_loss: 3.0120\nEpoch 318/500\n451/451 [==============================] - 7s 15ms/step - loss: 60.9647 - dense_19_loss: 13.6574 - dense_20_loss: 2.8766\nEpoch 319/500\n451/451 [==============================] - 7s 15ms/step - loss: 60.6239 - dense_19_loss: 13.5855 - dense_20_loss: 2.9038\nEpoch 320/500\n451/451 [==============================] - 7s 16ms/step - loss: 60.9133 - dense_19_loss: 14.0270 - dense_20_loss: 2.9112\nEpoch 321/500\n451/451 [==============================] - 7s 15ms/step - loss: 64.9036 - dense_19_loss: 17.7674 - dense_20_loss: 3.2477\nEpoch 322/500\n451/451 [==============================] - 7s 15ms/step - loss: 70.9031 - dense_19_loss: 22.0206 - dense_20_loss: 4.0808\nEpoch 323/500\n451/451 [==============================] - 7s 16ms/step - loss: 67.4392 - dense_19_loss: 19.7100 - dense_20_loss: 3.3015\nEpoch 324/500\n451/451 [==============================] - 7s 15ms/step - loss: 64.7199 - dense_19_loss: 17.1692 - dense_20_loss: 3.2198\nEpoch 325/500\n451/451 [==============================] - 7s 16ms/step - loss: 65.2337 - dense_19_loss: 17.6980 - dense_20_loss: 3.2857\nEpoch 326/500\n451/451 [==============================] - 7s 16ms/step - loss: 60.4546 - dense_19_loss: 13.7379 - dense_20_loss: 2.8076\nEpoch 327/500\n451/451 [==============================] - 7s 15ms/step - loss: 59.2203 - dense_19_loss: 12.9138 - dense_20_loss: 2.7386\nEpoch 328/500\n451/451 [==============================] - 7s 16ms/step - loss: 59.0442 - dense_19_loss: 12.9238 - dense_20_loss: 2.8085\nEpoch 329/500\n451/451 [==============================] - 7s 17ms/step - loss: 59.3773 - dense_19_loss: 13.4060 - dense_20_loss: 2.8284\nEpoch 330/500\n451/451 [==============================] - 7s 15ms/step - loss: 59.5678 - dense_19_loss: 13.7822 - dense_20_loss: 2.8630\nEpoch 331/500\n451/451 [==============================] - 7s 16ms/step - loss: 61.7109 - dense_19_loss: 15.6229 - dense_20_loss: 3.1244\nEpoch 332/500\n451/451 [==============================] - 7s 16ms/step - loss: 60.0511 - dense_19_loss: 14.3624 - dense_20_loss: 2.8915\nEpoch 333/500\n451/451 [==============================] - 7s 16ms/step - loss: 60.8074 - dense_19_loss: 15.2901 - dense_20_loss: 2.9133\nEpoch 334/500\n451/451 [==============================] - 7s 15ms/step - loss: 60.7678 - dense_19_loss: 15.2285 - dense_20_loss: 2.9834\nEpoch 335/500\n451/451 [==============================] - 7s 15ms/step - loss: 62.7619 - dense_19_loss: 16.9636 - dense_20_loss: 3.2707\nEpoch 336/500\n451/451 [==============================] - 7s 15ms/step - loss: 67.1151 - dense_19_loss: 20.7548 - dense_20_loss: 3.4213\nEpoch 337/500\n451/451 [==============================] - 7s 16ms/step - loss: 59.1285 - dense_19_loss: 13.7599 - dense_20_loss: 2.8676\nEpoch 338/500\n451/451 [==============================] - 7s 16ms/step - loss: 61.6671 - dense_19_loss: 16.3161 - dense_20_loss: 2.9850\nEpoch 339/500\n451/451 [==============================] - 7s 16ms/step - loss: 58.6826 - dense_19_loss: 13.7001 - dense_20_loss: 2.8641\nEpoch 340/500\n451/451 [==============================] - 7s 15ms/step - loss: 59.8434 - dense_19_loss: 14.6606 - dense_20_loss: 3.0390\nEpoch 341/500\n451/451 [==============================] - 7s 15ms/step - loss: 58.2098 - dense_19_loss: 13.5359 - dense_20_loss: 2.7833\nEpoch 342/500\n451/451 [==============================] - 7s 16ms/step - loss: 57.0373 - dense_19_loss: 12.7440 - dense_20_loss: 2.6681\nEpoch 343/500\n451/451 [==============================] - 7s 15ms/step - loss: 57.4391 - dense_19_loss: 13.2725 - dense_20_loss: 2.7430\nEpoch 344/500\n451/451 [==============================] - 7s 15ms/step - loss: 58.2368 - dense_19_loss: 14.0588 - dense_20_loss: 2.8393\nEpoch 345/500\n451/451 [==============================] - 7s 15ms/step - loss: 57.7063 - dense_19_loss: 13.6986 - dense_20_loss: 2.8379\nEpoch 346/500\n451/451 [==============================] - 7s 15ms/step - loss: 58.8909 - dense_19_loss: 14.8828 - dense_20_loss: 2.9622\nEpoch 347/500\n451/451 [==============================] - 7s 16ms/step - loss: 59.4797 - dense_19_loss: 15.3786 - dense_20_loss: 2.9910\nEpoch 348/500\n451/451 [==============================] - 7s 15ms/step - loss: 59.9054 - dense_19_loss: 15.9852 - dense_20_loss: 2.9487\nEpoch 349/500\n451/451 [==============================] - 7s 15ms/step - loss: 60.2665 - dense_19_loss: 15.7762 - dense_20_loss: 3.1216\nEpoch 350/500\n451/451 [==============================] - 7s 16ms/step - loss: 61.6191 - dense_19_loss: 17.1993 - dense_20_loss: 3.1837\nEpoch 351/500\n451/451 [==============================] - 7s 16ms/step - loss: 57.5762 - dense_19_loss: 13.7233 - dense_20_loss: 2.8615\nEpoch 352/500\n451/451 [==============================] - 7s 15ms/step - loss: 56.0594 - dense_19_loss: 12.6921 - dense_20_loss: 2.6649\nEpoch 353/500\n451/451 [==============================] - 7s 15ms/step - loss: 56.0253 - dense_19_loss: 12.8439 - dense_20_loss: 2.7044\nEpoch 354/500\n451/451 [==============================] - 7s 15ms/step - loss: 55.5476 - dense_19_loss: 12.5964 - dense_20_loss: 2.6817\nEpoch 355/500\n451/451 [==============================] - 7s 15ms/step - loss: 55.8744 - dense_19_loss: 13.0237 - dense_20_loss: 2.7222\nEpoch 356/500\n451/451 [==============================] - 7s 16ms/step - loss: 55.8553 - dense_19_loss: 13.1428 - dense_20_loss: 2.6969\nEpoch 357/500\n451/451 [==============================] - 7s 15ms/step - loss: 56.1853 - dense_19_loss: 13.6145 - dense_20_loss: 2.6729\nEpoch 358/500\n451/451 [==============================] - 7s 15ms/step - loss: 55.8380 - dense_19_loss: 13.3366 - dense_20_loss: 2.7275\nEpoch 359/500\n451/451 [==============================] - 7s 15ms/step - loss: 56.5690 - dense_19_loss: 14.0754 - dense_20_loss: 2.8226\nEpoch 360/500\n451/451 [==============================] - 7s 16ms/step - loss: 68.3074 - dense_19_loss: 24.3216 - dense_20_loss: 3.8816\nEpoch 361/500\n451/451 [==============================] - 7s 16ms/step - loss: 60.4473 - dense_19_loss: 16.9015 - dense_20_loss: 3.1344\nEpoch 362/500\n451/451 [==============================] - 7s 15ms/step - loss: 55.1693 - dense_19_loss: 12.5894 - dense_20_loss: 2.6569\nEpoch 363/500\n451/451 [==============================] - 7s 15ms/step - loss: 54.0999 - dense_19_loss: 11.9589 - dense_20_loss: 2.5439\nEpoch 364/500\n451/451 [==============================] - 7s 15ms/step - loss: 54.1641 - dense_19_loss: 12.2670 - dense_20_loss: 2.5305\nEpoch 365/500\n451/451 [==============================] - 7s 15ms/step - loss: 54.2615 - dense_19_loss: 12.5214 - dense_20_loss: 2.5680\nEpoch 366/500\n451/451 [==============================] - 7s 15ms/step - loss: 55.7055 - dense_19_loss: 13.8350 - dense_20_loss: 2.7115\nEpoch 367/500\n451/451 [==============================] - 7s 15ms/step - loss: 53.8654 - dense_19_loss: 12.3433 - dense_20_loss: 2.5738\nEpoch 368/500\n451/451 [==============================] - 7s 15ms/step - loss: 54.5020 - dense_19_loss: 13.0370 - dense_20_loss: 2.6361\nEpoch 369/500\n451/451 [==============================] - 7s 16ms/step - loss: 56.5903 - dense_19_loss: 14.8417 - dense_20_loss: 2.8315\nEpoch 370/500\n451/451 [==============================] - 7s 15ms/step - loss: 55.4177 - dense_19_loss: 13.9278 - dense_20_loss: 2.6934\nEpoch 371/500\n451/451 [==============================] - 7s 15ms/step - loss: 68.4022 - dense_19_loss: 24.9591 - dense_20_loss: 4.0201\nEpoch 372/500\n451/451 [==============================] - 7s 15ms/step - loss: 69.1371 - dense_19_loss: 25.5411 - dense_20_loss: 3.7514\nEpoch 373/500\n451/451 [==============================] - 7s 15ms/step - loss: 57.5885 - dense_19_loss: 15.1660 - dense_20_loss: 2.8299\nEpoch 374/500\n451/451 [==============================] - 7s 16ms/step - loss: 54.1326 - dense_19_loss: 12.3706 - dense_20_loss: 2.5360\nEpoch 375/500\n451/451 [==============================] - 7s 15ms/step - loss: 52.7788 - dense_19_loss: 11.3795 - dense_20_loss: 2.4589\nEpoch 376/500\n451/451 [==============================] - 7s 15ms/step - loss: 53.0574 - dense_19_loss: 11.8858 - dense_20_loss: 2.4996\nEpoch 377/500\n451/451 [==============================] - 7s 15ms/step - loss: 55.1453 - dense_19_loss: 13.8114 - dense_20_loss: 2.6991\nEpoch 378/500\n451/451 [==============================] - 7s 16ms/step - loss: 54.0132 - dense_19_loss: 12.7940 - dense_20_loss: 2.6328\nEpoch 379/500\n451/451 [==============================] - 7s 15ms/step - loss: 53.2118 - dense_19_loss: 12.2913 - dense_20_loss: 2.5810\nEpoch 380/500\n451/451 [==============================] - 7s 15ms/step - loss: 52.4928 - dense_19_loss: 11.8674 - dense_20_loss: 2.4895\nEpoch 381/500\n451/451 [==============================] - 7s 15ms/step - loss: 52.3300 - dense_19_loss: 11.9485 - dense_20_loss: 2.4809\nEpoch 382/500\n451/451 [==============================] - 7s 15ms/step - loss: 55.2526 - dense_19_loss: 14.4515 - dense_20_loss: 2.7743\nEpoch 383/500\n451/451 [==============================] - 7s 16ms/step - loss: 62.2756 - dense_19_loss: 20.5062 - dense_20_loss: 3.3181\nEpoch 384/500\n451/451 [==============================] - 7s 15ms/step - loss: 56.2477 - dense_19_loss: 15.0224 - dense_20_loss: 2.7358\nEpoch 385/500\n451/451 [==============================] - 7s 16ms/step - loss: 53.3653 - dense_19_loss: 12.6823 - dense_20_loss: 2.5379\nEpoch 386/500\n451/451 [==============================] - 7s 15ms/step - loss: 51.8352 - dense_19_loss: 11.5899 - dense_20_loss: 2.4635\nEpoch 387/500\n451/451 [==============================] - 7s 15ms/step - loss: 50.9865 - dense_19_loss: 11.0959 - dense_20_loss: 2.3767\nEpoch 388/500\n451/451 [==============================] - 7s 16ms/step - loss: 51.5584 - dense_19_loss: 11.8512 - dense_20_loss: 2.3819\nEpoch 389/500\n451/451 [==============================] - 7s 15ms/step - loss: 55.3930 - dense_19_loss: 15.0932 - dense_20_loss: 2.8272\nEpoch 390/500\n451/451 [==============================] - 7s 15ms/step - loss: 53.9366 - dense_19_loss: 13.7822 - dense_20_loss: 2.6524\nEpoch 391/500\n451/451 [==============================] - 7s 15ms/step - loss: 51.8192 - dense_19_loss: 12.1775 - dense_20_loss: 2.4240\nEpoch 392/500\n451/451 [==============================] - 7s 16ms/step - loss: 53.8340 - dense_19_loss: 13.8772 - dense_20_loss: 2.6338\nEpoch 393/500\n451/451 [==============================] - 7s 15ms/step - loss: 51.3595 - dense_19_loss: 11.9307 - dense_20_loss: 2.4127\nEpoch 394/500\n451/451 [==============================] - 7s 15ms/step - loss: 52.5192 - dense_19_loss: 13.0062 - dense_20_loss: 2.5442\nEpoch 395/500\n451/451 [==============================] - 7s 15ms/step - loss: 52.6941 - dense_19_loss: 13.2496 - dense_20_loss: 2.5745\nEpoch 396/500\n451/451 [==============================] - 7s 15ms/step - loss: 51.1045 - dense_19_loss: 12.0293 - dense_20_loss: 2.4241\nEpoch 397/500\n451/451 [==============================] - 7s 16ms/step - loss: 53.9881 - dense_19_loss: 14.5174 - dense_20_loss: 2.7751\nEpoch 398/500\n451/451 [==============================] - 7s 16ms/step - loss: 61.3240 - dense_19_loss: 20.3198 - dense_20_loss: 3.5741\nEpoch 399/500\n451/451 [==============================] - 7s 15ms/step - loss: 55.0401 - dense_19_loss: 14.7066 - dense_20_loss: 2.8645\nEpoch 400/500\n451/451 [==============================] - 7s 15ms/step - loss: 51.2100 - dense_19_loss: 11.8381 - dense_20_loss: 2.3936\nEpoch 401/500\n451/451 [==============================] - 7s 16ms/step - loss: 49.4510 - dense_19_loss: 10.5382 - dense_20_loss: 2.2959\nEpoch 402/500\n451/451 [==============================] - 7s 16ms/step - loss: 50.7143 - dense_19_loss: 11.8701 - dense_20_loss: 2.4169\nEpoch 403/500\n451/451 [==============================] - 7s 15ms/step - loss: 51.1644 - dense_19_loss: 12.4166 - dense_20_loss: 2.4218\nEpoch 404/500\n451/451 [==============================] - 7s 15ms/step - loss: 50.3022 - dense_19_loss: 11.8853 - dense_20_loss: 2.3209\nEpoch 405/500\n451/451 [==============================] - 7s 15ms/step - loss: 50.2305 - dense_19_loss: 11.8190 - dense_20_loss: 2.4213\nEpoch 406/500\n451/451 [==============================] - 7s 16ms/step - loss: 51.9914 - dense_19_loss: 13.5329 - dense_20_loss: 2.5166\nEpoch 407/500\n451/451 [==============================] - 7s 15ms/step - loss: 52.2020 - dense_19_loss: 13.5382 - dense_20_loss: 2.6186\nEpoch 408/500\n451/451 [==============================] - 7s 15ms/step - loss: 55.1743 - dense_19_loss: 15.9296 - dense_20_loss: 2.9223\nEpoch 409/500\n451/451 [==============================] - 7s 15ms/step - loss: 49.9385 - dense_19_loss: 11.5512 - dense_20_loss: 2.3735\nEpoch 410/500\n451/451 [==============================] - 7s 16ms/step - loss: 50.4708 - dense_19_loss: 12.1911 - dense_20_loss: 2.4546\nEpoch 411/500\n451/451 [==============================] - 7s 15ms/step - loss: 49.4390 - dense_19_loss: 11.5039 - dense_20_loss: 2.3578\nEpoch 412/500\n451/451 [==============================] - 7s 15ms/step - loss: 50.5078 - dense_19_loss: 12.6056 - dense_20_loss: 2.4655\nEpoch 413/500\n451/451 [==============================] - 7s 15ms/step - loss: 53.6107 - dense_19_loss: 15.0368 - dense_20_loss: 2.8048\nEpoch 414/500\n451/451 [==============================] - 7s 15ms/step - loss: 51.1432 - dense_19_loss: 12.7970 - dense_20_loss: 2.5493\nEpoch 415/500\n451/451 [==============================] - 7s 16ms/step - loss: 53.2381 - dense_19_loss: 14.6816 - dense_20_loss: 2.8011\nEpoch 416/500\n451/451 [==============================] - 7s 15ms/step - loss: 50.7575 - dense_19_loss: 12.6684 - dense_20_loss: 2.4621\nEpoch 417/500\n451/451 [==============================] - 7s 15ms/step - loss: 48.8822 - dense_19_loss: 11.2164 - dense_20_loss: 2.3164\nEpoch 418/500\n451/451 [==============================] - 7s 15ms/step - loss: 50.2498 - dense_19_loss: 12.6648 - dense_20_loss: 2.4420\nEpoch 419/500\n451/451 [==============================] - 7s 16ms/step - loss: 49.1134 - dense_19_loss: 11.5571 - dense_20_loss: 2.4543\nEpoch 420/500\n451/451 [==============================] - 7s 16ms/step - loss: 48.6321 - dense_19_loss: 11.4660 - dense_20_loss: 2.2713\nEpoch 421/500\n451/451 [==============================] - 7s 15ms/step - loss: 49.3916 - dense_19_loss: 12.1930 - dense_20_loss: 2.3954\nEpoch 422/500\n451/451 [==============================] - 7s 15ms/step - loss: 52.2198 - dense_19_loss: 14.1305 - dense_20_loss: 2.7381\nEpoch 423/500\n451/451 [==============================] - 7s 15ms/step - loss: 49.1156 - dense_19_loss: 11.9030 - dense_20_loss: 2.3370\nEpoch 424/500\n451/451 [==============================] - 7s 16ms/step - loss: 48.3649 - dense_19_loss: 11.4442 - dense_20_loss: 2.3067\nEpoch 425/500\n451/451 [==============================] - 7s 15ms/step - loss: 50.8561 - dense_19_loss: 13.6144 - dense_20_loss: 2.5848\nEpoch 426/500\n451/451 [==============================] - 7s 15ms/step - loss: 49.8999 - dense_19_loss: 12.8933 - dense_20_loss: 2.4149\nEpoch 427/500\n451/451 [==============================] - 7s 15ms/step - loss: 48.1078 - dense_19_loss: 11.4255 - dense_20_loss: 2.2862\nEpoch 428/500\n451/451 [==============================] - 7s 16ms/step - loss: 52.2984 - dense_19_loss: 14.7675 - dense_20_loss: 2.7172\nEpoch 429/500\n451/451 [==============================] - 7s 15ms/step - loss: 47.9573 - dense_19_loss: 11.1612 - dense_20_loss: 2.3032\nEpoch 430/500\n451/451 [==============================] - 7s 15ms/step - loss: 47.7845 - dense_19_loss: 11.3134 - dense_20_loss: 2.2381\nEpoch 431/500\n451/451 [==============================] - 7s 16ms/step - loss: 47.2073 - dense_19_loss: 10.9569 - dense_20_loss: 2.2370\nEpoch 432/500\n451/451 [==============================] - 7s 15ms/step - loss: 48.0679 - dense_19_loss: 11.9364 - dense_20_loss: 2.2652\nEpoch 433/500\n451/451 [==============================] - 7s 16ms/step - loss: 48.2483 - dense_19_loss: 12.0354 - dense_20_loss: 2.3247\nEpoch 434/500\n451/451 [==============================] - 7s 15ms/step - loss: 47.1665 - dense_19_loss: 11.2560 - dense_20_loss: 2.2699\nEpoch 435/500\n451/451 [==============================] - 7s 15ms/step - loss: 51.6193 - dense_19_loss: 15.1174 - dense_20_loss: 2.6375\nEpoch 436/500\n451/451 [==============================] - 7s 15ms/step - loss: 47.5762 - dense_19_loss: 11.5371 - dense_20_loss: 2.3279\nEpoch 437/500\n451/451 [==============================] - 8s 17ms/step - loss: 46.8046 - dense_19_loss: 11.0450 - dense_20_loss: 2.2447\nEpoch 438/500\n451/451 [==============================] - 7s 15ms/step - loss: 46.6012 - dense_19_loss: 10.9760 - dense_20_loss: 2.2359\nEpoch 439/500\n451/451 [==============================] - 7s 15ms/step - loss: 47.4692 - dense_19_loss: 11.9013 - dense_20_loss: 2.2839\nEpoch 440/500\n451/451 [==============================] - 7s 15ms/step - loss: 46.5668 - dense_19_loss: 11.1872 - dense_20_loss: 2.2199\nEpoch 441/500\n451/451 [==============================] - 7s 15ms/step - loss: 46.9782 - dense_19_loss: 11.6854 - dense_20_loss: 2.2376\nEpoch 442/500\n451/451 [==============================] - 7s 16ms/step - loss: 46.2748 - dense_19_loss: 11.0061 - dense_20_loss: 2.2909\nEpoch 443/500\n451/451 [==============================] - 7s 15ms/step - loss: 46.1911 - dense_19_loss: 11.1311 - dense_20_loss: 2.2183\nEpoch 444/500\n451/451 [==============================] - 7s 15ms/step - loss: 47.4650 - dense_19_loss: 12.3133 - dense_20_loss: 2.3564\nEpoch 445/500\n451/451 [==============================] - 7s 15ms/step - loss: 46.1386 - dense_19_loss: 11.2047 - dense_20_loss: 2.2566\nEpoch 446/500\n451/451 [==============================] - 7s 16ms/step - loss: 64.7375 - dense_19_loss: 27.1451 - dense_20_loss: 4.2307\nEpoch 447/500\n451/451 [==============================] - 7s 15ms/step - loss: 55.7464 - dense_19_loss: 18.0152 - dense_20_loss: 3.2085\nEpoch 448/500\n451/451 [==============================] - 7s 15ms/step - loss: 47.5676 - dense_19_loss: 11.2782 - dense_20_loss: 2.2935\nEpoch 449/500\n451/451 [==============================] - 7s 15ms/step - loss: 45.2933 - dense_19_loss: 9.7699 - dense_20_loss: 2.0312\nEpoch 450/500\n451/451 [==============================] - 7s 15ms/step - loss: 44.7554 - dense_19_loss: 9.5649 - dense_20_loss: 2.0792\nEpoch 451/500\n451/451 [==============================] - 7s 16ms/step - loss: 44.7754 - dense_19_loss: 9.9066 - dense_20_loss: 2.0437\nEpoch 452/500\n451/451 [==============================] - 7s 15ms/step - loss: 45.5093 - dense_19_loss: 10.7169 - dense_20_loss: 2.1079\nEpoch 453/500\n451/451 [==============================] - 7s 15ms/step - loss: 44.8053 - dense_19_loss: 10.2408 - dense_20_loss: 2.1232\nEpoch 454/500\n451/451 [==============================] - 7s 15ms/step - loss: 44.8367 - dense_19_loss: 10.4617 - dense_20_loss: 2.1054\nEpoch 455/500\n451/451 [==============================] - 7s 17ms/step - loss: 45.2968 - dense_19_loss: 10.9617 - dense_20_loss: 2.1613\nEpoch 456/500\n451/451 [==============================] - 7s 15ms/step - loss: 46.6020 - dense_19_loss: 12.2962 - dense_20_loss: 2.2175\nEpoch 457/500\n451/451 [==============================] - 7s 15ms/step - loss: 56.3985 - dense_19_loss: 18.8834 - dense_20_loss: 3.9081\nEpoch 458/500\n451/451 [==============================] - 7s 16ms/step - loss: 47.2216 - dense_19_loss: 12.0259 - dense_20_loss: 2.2702\nEpoch 459/500\n451/451 [==============================] - 7s 15ms/step - loss: 45.2228 - dense_19_loss: 10.6332 - dense_20_loss: 2.1020\nEpoch 460/500\n451/451 [==============================] - 7s 16ms/step - loss: 45.6289 - dense_19_loss: 11.2363 - dense_20_loss: 2.1812\nEpoch 461/500\n451/451 [==============================] - 7s 15ms/step - loss: 45.0429 - dense_19_loss: 10.8697 - dense_20_loss: 2.1273\nEpoch 462/500\n451/451 [==============================] - 7s 15ms/step - loss: 43.7540 - dense_19_loss: 9.9140 - dense_20_loss: 2.0384\nEpoch 463/500\n451/451 [==============================] - 7s 15ms/step - loss: 43.6478 - dense_19_loss: 10.0541 - dense_20_loss: 1.9957\nEpoch 464/500\n451/451 [==============================] - 7s 16ms/step - loss: 43.5895 - dense_19_loss: 10.0512 - dense_20_loss: 2.0718\nEpoch 465/500\n451/451 [==============================] - 7s 15ms/step - loss: 44.9765 - dense_19_loss: 11.3617 - dense_20_loss: 2.1819\nEpoch 466/500\n451/451 [==============================] - 7s 15ms/step - loss: 45.2930 - dense_19_loss: 11.6657 - dense_20_loss: 2.1884\nEpoch 467/500\n451/451 [==============================] - 7s 15ms/step - loss: 60.5019 - dense_19_loss: 24.2656 - dense_20_loss: 3.7419\nEpoch 468/500\n451/451 [==============================] - 7s 15ms/step - loss: 51.0197 - dense_19_loss: 15.8481 - dense_20_loss: 2.6773\nEpoch 469/500\n451/451 [==============================] - 7s 16ms/step - loss: 53.2574 - dense_19_loss: 17.4843 - dense_20_loss: 3.0761\nEpoch 470/500\n451/451 [==============================] - 7s 15ms/step - loss: 45.0602 - dense_19_loss: 10.6037 - dense_20_loss: 2.1479\nEpoch 471/500\n451/451 [==============================] - 7s 15ms/step - loss: 44.1287 - dense_19_loss: 10.0546 - dense_20_loss: 2.0686\nEpoch 472/500\n451/451 [==============================] - 7s 16ms/step - loss: 43.4058 - dense_19_loss: 9.7058 - dense_20_loss: 1.9951\nEpoch 473/500\n451/451 [==============================] - 7s 15ms/step - loss: 43.5216 - dense_19_loss: 10.0251 - dense_20_loss: 2.0256\nEpoch 474/500\n451/451 [==============================] - 7s 16ms/step - loss: 57.0901 - dense_19_loss: 21.6112 - dense_20_loss: 3.2896\nEpoch 475/500\n451/451 [==============================] - 7s 15ms/step - loss: 47.1437 - dense_19_loss: 12.5829 - dense_20_loss: 2.3677\nEpoch 476/500\n451/451 [==============================] - 7s 15ms/step - loss: 43.5269 - dense_19_loss: 9.6936 - dense_20_loss: 2.0505\nEpoch 477/500\n451/451 [==============================] - 7s 16ms/step - loss: 43.4949 - dense_19_loss: 9.9539 - dense_20_loss: 2.0241\nEpoch 478/500\n451/451 [==============================] - 7s 16ms/step - loss: 43.0809 - dense_19_loss: 9.7134 - dense_20_loss: 2.0319\nEpoch 479/500\n451/451 [==============================] - 7s 15ms/step - loss: 43.1061 - dense_19_loss: 9.9755 - dense_20_loss: 2.0012\nEpoch 480/500\n451/451 [==============================] - 7s 15ms/step - loss: 43.6311 - dense_19_loss: 10.5556 - dense_20_loss: 2.0817\nEpoch 481/500\n451/451 [==============================] - 7s 15ms/step - loss: 44.6825 - dense_19_loss: 11.4804 - dense_20_loss: 2.1570\nEpoch 482/500\n451/451 [==============================] - 7s 15ms/step - loss: 48.5952 - dense_19_loss: 14.7591 - dense_20_loss: 2.5307\nEpoch 483/500\n451/451 [==============================] - 7s 16ms/step - loss: 43.9505 - dense_19_loss: 10.7588 - dense_20_loss: 2.1110\nEpoch 484/500\n451/451 [==============================] - 7s 15ms/step - loss: 42.9194 - dense_19_loss: 10.1272 - dense_20_loss: 1.9898\nEpoch 485/500\n451/451 [==============================] - 7s 15ms/step - loss: 42.2779 - dense_19_loss: 9.6098 - dense_20_loss: 2.0465\nEpoch 486/500\n451/451 [==============================] - 7s 15ms/step - loss: 42.5815 - dense_19_loss: 10.0405 - dense_20_loss: 2.0615\nEpoch 487/500\n451/451 [==============================] - 7s 16ms/step - loss: 43.2190 - dense_19_loss: 10.6272 - dense_20_loss: 2.1542\nEpoch 488/500\n451/451 [==============================] - 7s 15ms/step - loss: 43.6599 - dense_19_loss: 11.1418 - dense_20_loss: 2.1494\nEpoch 489/500\n451/451 [==============================] - 7s 15ms/step - loss: 43.2942 - dense_19_loss: 10.9115 - dense_20_loss: 2.0981\nEpoch 490/500\n451/451 [==============================] - 7s 16ms/step - loss: 44.2177 - dense_19_loss: 11.6725 - dense_20_loss: 2.2527\nEpoch 491/500\n451/451 [==============================] - 7s 15ms/step - loss: 45.2408 - dense_19_loss: 12.5286 - dense_20_loss: 2.2872\nEpoch 492/500\n451/451 [==============================] - 7s 16ms/step - loss: 43.2558 - dense_19_loss: 10.8023 - dense_20_loss: 2.1061\nEpoch 493/500\n451/451 [==============================] - 7s 15ms/step - loss: 41.9010 - dense_19_loss: 9.8476 - dense_20_loss: 1.9948\nEpoch 494/500\n451/451 [==============================] - 7s 15ms/step - loss: 47.5742 - dense_19_loss: 14.7938 - dense_20_loss: 2.5250\nEpoch 495/500\n451/451 [==============================] - 7s 15ms/step - loss: 43.6644 - dense_19_loss: 11.2282 - dense_20_loss: 2.1214\nEpoch 496/500\n451/451 [==============================] - 7s 16ms/step - loss: 42.0072 - dense_19_loss: 9.9638 - dense_20_loss: 2.0141\nEpoch 497/500\n451/451 [==============================] - 7s 15ms/step - loss: 41.5500 - dense_19_loss: 9.8075 - dense_20_loss: 1.9379\nEpoch 498/500\n451/451 [==============================] - 7s 15ms/step - loss: 42.0261 - dense_19_loss: 10.2989 - dense_20_loss: 1.9697\nEpoch 499/500\n451/451 [==============================] - 7s 15ms/step - loss: 42.0350 - dense_19_loss: 10.2247 - dense_20_loss: 2.0825\nEpoch 500/500\n451/451 [==============================] - 7s 15ms/step - loss: 42.4307 - dense_19_loss: 10.7384 - dense_20_loss: 2.0751\nmsynergy_mean_squared_error 201.63497947577227\nmsenstivity_mean_squared_error 15.401981740994339\nmsynergy_mean_absolute_error 9.321047880754183\nmsenstivity_mean_absolute_error 2.7081140303502798\nmsynergy_r2_score 0.5772721730278905\nmsenstivity_r2_score 0.8960077173272443\n109/109 [==============================] - 2s 8ms/step - loss: 246.4960 - dense_19_loss: 201.6350 - dense_20_loss: 15.4020\n[246.49603271484375, 201.63497924804688, 15.40198040008545]\nmsynergy_pear (array([0.7608291804361014], dtype=object), 0.0)\nmsynergy_spear SpearmanrResult(correlation=0.7284177352586305, pvalue=0.0)\nmsenstivity_pear (array([0.9473105281306563], dtype=object), 0.0)\nmsenstivity_spear SpearmanrResult(correlation=0.9478710635645486, pvalue=0.0)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(array([[-8.784463 ],\n        [ 4.5820017],\n        [ 2.0213995],\n        ...,\n        [ 6.754138 ],\n        [ 4.641059 ],\n        [ 8.13182  ]], dtype=float32),\n array([[30.180082 ],\n        [ 2.1186607],\n        [17.688425 ],\n        ...,\n        [16.93225  ],\n        [38.494675 ],\n        [23.287607 ]], dtype=float32))"},"metadata":{}}]},{"cell_type":"code","source":"\nfrom IPython.display import FileLink\nnp.savetxt('pred_syn1.csv', ap111 ,delimiter=',')\nFileLink(r'pred_syn1.csv')\n\nnp.savetxt('pred_sen1.csv', ap221 ,delimiter=',')\nFileLink(r'pred_sen1.csv')\n\nnp.savetxt('test_syn1.csv', test_synergy ,delimiter=',')\nFileLink(r'test_syn1.csv')\n\nnp.savetxt('test_sen1.csv', test_senstivity ,delimiter=',')\nFileLink(r'test_sen1.csv')","metadata":{"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/test_sen1.csv","text/html":"<a href='test_sen1.csv' target='_blank'>test_sen1.csv</a><br>"},"metadata":{}}]}]}