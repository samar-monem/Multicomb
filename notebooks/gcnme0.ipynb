{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install deepchem\n!pip install pysmiles\n!pip install openpyxl\n# !pip install rdkit\n\n# !pip install MolGraphConvFeaturizer\n!pip install PubChemPy\n!pip install PyDrive\n!pip install xlrd==2.0.1\n!pip install Pandas==1.3.5\n# !pip install Pandas==1.1.5\n!pip install  gdown\nimport deepchem as dc\nfrom rdkit import Chem\nprint(\"hjjbjh\")\nfrom pysmiles import read_smiles\nimport networkx as nx\nfrom deepchem.feat import MolGraphConvFeaturizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n#   gauth = GoogleAuth()\n#   gauth.credentials = GoogleCredentials.get_application_default()\n#   drivea = GoogleDrive(gauth)\n#   drive.mount('/content/drive')\ndef normalize1(X, means1=None, std1=None, means2=None, std2=None, feat_filt=None, norm='tanh_norm'):\n    if std1 is None:\n        std1 = np.nanstd(X, axis=0)\n    if feat_filt is None:\n        feat_filt = std1!=0\n    X = X[:,feat_filt]\n    X = np.ascontiguousarray(X)\n    if means1 is None:\n        means1 = np.mean(X, axis=0)\n    X = (X-means1)/std1[feat_filt]\n    if norm == 'norm':\n        return(X, means1, std1, feat_filt)\n    elif norm == 'tanh':\n        return(np.tanh(X), means1, std1, feat_filt)\n    elif norm == 'tanh_norm':\n        X = np.tanh(X)\n        if means2 is None:\n            means2 = np.mean(X, axis=0)\n        if std2 is None:\n            std2 = np.std(X, axis=0)\n        X = (X-means2)/std2\n        X[:,std2==0]=0\n        return(X, means1, std1, means2, std2, feat_filt) \n    \n    \ndef get_data():\n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  smiles=pd.read_excel('pubchem.xls', header=None)\n  smiles=np.array(smiles)\n\n#   !gdown https://drive.google.com/uc?id=15bNKK7tacCJIFzvt5y4WfU6uKbPdYtA6\n  !gdown --id 1z1sWG4E9BqipP0iczLrbR4qxSa4KG95_\n  data_to_repeat=pd.read_excel('pcbi.1006752.s004.xls', header=None)\n  data_to_repeat=np.array(data_to_repeat)\n  \n#   !gdown --id 1TThHsLyORlcHuEBgOad20SQUYH88O9mm\n#   data_to_repeat=pd.read_excel('labels1.xlsx', header=None)\n#   data_to_repeat=np.array(data_to_repeat)\n    \n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  unique_drugs=pd.read_excel('pubchem.xls', header=None)\n  unique_drugs=np.array(unique_drugs)         \n\n#   !gdown https://drive.google.com/uc?id=1IMr5zMLRAXC5iE2MAHJbKmHf2-0DCZNd\n# #   !gdown --id 1bBJUFBA4Tm9YdE5OxA1mbUfBI8B7wqcr\n#   feature_cell=pd.read_excel('final_feature_cell.xlsx', header=None)\n#   feature_cell=np.array(feature_cell)\n\n  !gdown --id 1NoUKCRZ0CkNVFP1-578x_L0otPUMkz6T\n  feature_cell=pd.read_excel('unique934_cell.xlsx',header=None)\n  feature_cell=np.array(feature_cell)\n  a=np.zeros((1,934))\n  feature_cell[22,1:]=a\n  feature_cell[36,1:]=a\n#   !gdown --id 109nyFVOO_P9DdyrhWzbNg0FBB_Y8gD58\n#   feature_cell=pd.read_excel('final_deep_cell.xlsx',header=None)\n#   feature_cell=np.array(feature_cell)\n\n#   !gdown https://drive.google.com/uc?id=1YTe0v5PzwjlgPqo3OTU4FMghOFAHgeeC\n  !gdown --id 1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\n  deleted_index=pd.read_excel('deleted_index.xls', header=None)\n  deleted_index=np.array(deleted_index)  \n\n  return smiles[1:,1],data_to_repeat[1:,:],unique_drugs[1:,0],feature_cell,deleted_index\n\n\ndef atom_features(atom):\n    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n                                          ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As',\n                                           'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se',\n                                           'Ti', 'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n                                           'Pt', 'Hg', 'Pb', 'Unknown']) +\n                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    [atom.GetIsAromatic()])\n\n\ndef one_of_k_encoding(x, allowable_set):\n    if x not in allowable_set:\n        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))\n\n\ndef one_of_k_encoding_unk(x, allowable_set):\n    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\n\n\ndef smile_to_graph(smile):\n    mol = Chem.MolFromSmiles(smile)\n\n    c_size = mol.GetNumAtoms()\n\n    features = []\n    for atom in mol.GetAtoms():\n        feature = atom_features(atom)\n        features.append(feature / sum(feature))\n\n    edges = []\n    for bond in mol.GetBonds():\n        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n    g = nx.Graph(edges).to_directed()\n    edge_index = []\n    for e1, e2 in g.edges:\n        edge_index.append([e1, e2])\n\n    return c_size, features, edge_index\n\n\ndef graph_node(smiles):\n  node=[]\n  adj=[]\n  max=0\n  final_feature=[]\n  final_adjacency=[]\n  for i  in range(len(smiles)):\n     m=smiles[i]\n    #  aa=m#.item((0))\n     featurizer = MolGraphConvFeaturizer(use_edges=True,use_chirality=True)\n     out = featurizer.featurize(m)\n     feature=out[0].node_features\n        \n#      featurizer = DMPNNFeaturizer()\n#      out = featurizer.featurize(m)\n#      feature=out[0].node_features\n    \n     ma=feature.shape[0]\n     n=feature.shape[1]\n     if(ma>max):\n       max=ma\n    # out[0].edge_featuwres.shape\n     molecules=read_smiles(m)\n     adjacency1=nx.to_numpy_array(molecules)\n     node.append(feature)\n     adj.append(adjacency1)\n#   f_n=np.zeros((max,n))\n#   f_a=np.zeros((max,max))\n  feature=[]\n  adjacency=[]\n  for i  in range(len(smiles)):\n    n1=node[i]\n    a=adj[i]\n    f_n=np.zeros((max,n))\n    f_a=np.zeros((max,max))\n    f_n[0:n1.shape[0],]=n1\n    f_a[0:n1.shape[0],0:n1.shape[0]]=a\n    final_feature.append(f_n)\n    final_adjacency.append(f_a)\n\n\n  return final_feature,final_adjacency \n\n\ndef graph_node_edge(smiles):\n  node=[]\n  adj=[]\n  max=0\n  max1=0\n  final_feature=[]\n  final_adjacency=[]\n  for i  in range(len(smiles)):\n     m=smiles[i]\n    #  aa=m#.item((0))\n    #  featurizer = MolGraphConvFeaturizer(use_edges=True,use_chirality=True)\n    #  out = featurizer.featurize(m)\n    #  feature=out[0].node_features\n     num_atom,feature,edge_index=smile_to_graph(m)  \n#      featurizer = DMPNNFeaturizer()\n#      out = featurizer.featurize(m)\n#      feature=out[0].node_features\n     feature=np.stack( feature, axis=0)\n     edge_index=np.stack( edge_index, axis=0)\n    \n     ma=num_atom\n     n=feature.shape[1]\n     if(ma>max):\n       max=ma\n    # out[0].edge_featuwres.shape\n     node.append(feature)\n\n     \n     ne=edge_index.shape[0]\n     if(ne>max1):\n       max1=ne\n    # out[0].edge_featuwres.shape\n     adj.append(edge_index)\n#   f_n=np.zeros((max,n))\n#   f_a=np.zeros((max,max))\n  feature=[]\n  adjacency=[]\n  for i  in range(len(smiles)):\n    n1=node[i]\n    a=adj[i]\n    f_n=np.zeros((max,n))\n    f_n[0:n1.shape[0],]=n1\n    f_a=np.zeros((max1,2))\n    f_a[0:a.shape[0],]=a\n    final_feature.append(f_n)\n    final_adjacency.append(f_a)\n\n  return final_feature,final_adjacency \n\n\ndef repeat_smiles(data_to_repeat,unique_drugs,feature,adjacency,deleted_index,synergy,senstivity):\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  synergy1=[]\n  senstivity1=[]\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    if(i not in deleted_index):\n      f_drug1.append(feature[k1[0]])\n      f_drug2.append(feature[k2[0]])\n      a_drug1.append(adjacency[k1[0]])\n      a_drug2.append(adjacency[k2[0]])\n      synergy1.append(synergy[i,])\n      senstivity1.append(senstivity[i,])\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,synergy1,senstivity1\n\ndef repeat_smiles1(data_to_repeat,unique_drugs,feature,adjacency,unique_cell):\n  unique_feature=unique_cell[:,1:]\n  unique_name=unique_cell[:,0]\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  c=data_to_repeat[:,2]\n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  feature_cell=[]\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    cc=c[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    cc1=[m for m, v in enumerate(unique_name) if cc in v]\n    f_drug1.append(feature[k1[0]])\n    f_drug2.append(feature[k2[0]])\n    a_drug1.append(adjacency[k1[0]])\n    a_drug2.append(adjacency[k2[0]])\n    feature_cell.append(unique_feature[cc1[0]])\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,feature_cell \n\ndef repeat_smiles_deep(data_to_repeat,unique_drugs,feature,adjacency,unique_cell):\n  unique_feature=unique_cell[:,1:]\n  unique_name=unique_cell[:,0]\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  c=data_to_repeat[:,2]\n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  feature_cell=[]\n  f=np.zeros((feature[0].shape[0],feature[0].shape[1]))\n  a=np.zeros((adjacency[0].shape[0],adjacency[0].shape[1]))\n  cell=np.zeros((unique_cell[0].shape[0]))\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    cc=c[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    cc1=[m for m, v in enumerate(unique_name) if cc in v]\n    if(cc1):\n        f_drug1.append(feature[k1[0]])\n        f_drug2.append(feature[k2[0]])\n        a_drug1.append(adjacency[k1[0]])\n        a_drug2.append(adjacency[k2[0]])\n        feature_cell.append(unique_feature[cc1[0]])\n    else:\n        f_drug1.append(f)\n        f_drug2.append(f)\n        a_drug1.append(a)\n        a_drug2.append(a)\n        feature_cell.append(cell)\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,feature_cell\n\ndef split_data(feature_cell,synergy,senstivity):\n  row=feature_cell.shape[0]\n  col=feature_cell.shape[1]\n  input1=np.zeros((row,col+1))\n  # a=np.array('i')\n  for i in range(len(feature_cell)):\n    input1[i,0:col]=feature_cell[i,:]\n    input1[i,col]=i\n\n\n  index_train=[]\n  index_test=[]\n  output = np.c_[synergy,senstivity ]\n#   dataset1=np.c_[input11,output1]\n#   dataset=np.random.shuffle(dataset1)\n#   input1=dataset[:,0:len(dataset[0])-2]\n#   output=dataset[:,-2:]\n  x_train, x_test, y_train, y_test = train_test_split(input1, output,test_size=.20)\n  train_synergy=y_train[:,0]\n  train_senstivity=y_train[:,1]\n  test_synergy=y_test[:,0]\n  test_senstivity=y_test[:,1]\n  for i in range(len(x_train)):#.shape[0]):\n    for j in range(len(input1)):#.shape[0]):\n      if(all(x_train[i]==input1[j])):\n        index_train.append(j)\n        \n\n  for ii in range(len(x_test)):#.shape[0]):\n    for jj in range(len(input1)):#.shape[0]):\n      if(all(x_test[ii]==input1[jj])):\n        index_test.append(jj)\n        \n\n  return  train_synergy, train_senstivity,test_synergy, test_senstivity,index_train,index_test\n\n\ndef split_data1(feature_cell,synergy,senstivity):\n  row=feature_cell.shape[0]\n  col=feature_cell.shape[1]\n  input1=np.zeros((row,col+1))\n  # a=np.array('i')\n  k=0\n  for i in range(len(feature_cell)):\n    if((senstivity[i])):\n        input1[k,0:col]=feature_cell[i,:]\n        input1[k,col]=i\n        k=k+1\n\n\n  index_train=[]\n  index_test=[]\n  output=[]\n  for i in range(len(feature_cell)):\n    if((senstivity[i])):\n      output.append (np.c_[synergy[i],senstivity[i] ])\n#   dataset1=np.c_[input11,output1]\n#   dataset=np.random.shuffle(dataset1)\n#   input1=dataset[:,0:len(dataset[0])-2]\n#   output=dataset[:,-2:]\n  output= np.array(output)\n  x_train, x_test, y_train, y_test = train_test_split(input1, output,test_size=.20)\n  train_synergy=y_train[:,0]\n  train_senstivity=y_train[:,1]\n  test_synergy=y_test[:,0]\n  test_senstivity=y_test[:,1]\n  for i in range(len(x_train)):#.shape[0]):\n    for j in range(len(input1)):#.shape[0]):\n      if(all(x_train[i]==input1[j])):\n        index_train.append(j)\n        \n\n  for ii in range(len(x_test)):#.shape[0]):\n    for jj in range(len(input1)):#.shape[0]):\n      if(all(x_test[ii]==input1[jj])):\n        index_test.append(jj)\n        \n\n  return  index_train,index_test\n\n\ndef index(m):\n    if m==1:\n      !gdown --id 1b82ry7sSPqPIcJSr-i4rmC9XYp7Z1CpH\n      test_ind=pd.read_excel('ind1_test.xls', header=None)\n      test_ind=np.array(test_ind)\n    \n      !gdown --id 1aAaYK1EG5AVlWtwHlsajVJo4Qx-u2ig3\n      train_ind=pd.read_excel('ind1_train.xls', header=None)\n      train_ind=np.array(train_ind)\n\n    \n    if m==2:\n        !gdown --id 1EXKurPZ5ScjiZM0OG-bSv7IPXyKKW6B5\n        test_ind=pd.read_excel('ind2_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1-rayr-tZdwX4DDtSHChkoSdgEK1P3jWN\n        train_ind=pd.read_excel('ind2_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    if m==3:\n        !gdown --id 19O4yKBUCAyPrvdUne2IhwOY73A4y7pzT\n        test_ind=pd.read_excel('ind3_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1YNZBGdL--Ww9ZSGWUKmeC26DdJDywrR4\n        train_ind=pd.read_excel('ind3_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==4:\n        !gdown --id 1sJ3ksFbMOQoBTqNw5Ddv7bGgD9YESRGn\n        test_ind=pd.read_excel('ind4_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1-P14BxLrpnYS-9TbsvyATKtFTFG7QsLO\n        train_ind=pd.read_excel('ind4_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==5:\n        !gdown --id 1iZo5wJgiUOGBAVRy0Hw6vXngFHe9ciFH\n        test_ind=pd.read_excel('ind5_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1xlFp8g1gDGPa_Sf0zgrPWm3hnRCSkAxp\n        train_ind=pd.read_excel('ind5_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    return test_ind,train_ind\n\n\ndef index1(m):\n    if m==1:\n      !gdown --id 10a4cV1tOqomBo7RzWqnbs6j5-RGcBNjS\n      test_ind=pd.read_excel('another_ind1_test.xls', header=None)\n      test_ind=np.array(test_ind)\n    \n      !gdown --id 1Kn8Y2iGCH7DLwq72s_QmouA0Db1WBfVR\n      train_ind=pd.read_excel('another_ind1_train.xls', header=None)\n      train_ind=np.array(train_ind)\n\n    \n    if m==2:\n        !gdown --id 1WR4kDyNO6Xo-P-z3O2q6M_tHr6Soj6NZ\n        test_ind=pd.read_excel('another_ind2_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1aegtCzmVg9znXPTUvpzf2zLkmcjEeQ22\n        train_ind=pd.read_excel('another_ind2_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    if m==3:\n        !gdown --id 1W29vAF6mOoKmacbibxzTNQCfp42VVYYN\n        test_ind=pd.read_excel('another_ind3_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1B8h0ewSo4XJCvSWBdS5oiQiwtAL3H72T\n        train_ind=pd.read_excel('another_ind3_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==4:\n        !gdown --id 1tUXsGGT_-JdJTpT5sYEBB3zNFnh60tJR\n        test_ind=pd.read_excel('another_ind4_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1T9H0O6bzr2d01cPnSyE-vYk_9doKKBHZ\n        train_ind=pd.read_excel('another_ind4_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n        \n    if m==5:\n        !gdown --id 1p1kOBJQRhsavnOwgeb_3z-gBqLSS08sE\n        test_ind=pd.read_excel('another_ind5_test.xls', header=None)\n        test_ind=np.array(test_ind) \n        \n        !gdown --id 1hhw-ITuxUnKwTlLveS1MJER_KM580SXa\n        train_ind=pd.read_excel('another_ind5_train.xls', header=None)\n        train_ind=np.array(train_ind)\n\n    return test_ind,train_ind\n\n\ndef train_test_input(f_drug1,a_drug1,f_drug2,a_drug2,cell_line,index_train,index_test,synery,senstivity):\n  train_f_drug1=[]\n  train_a_drug1=[]\n  train_f_drug2=[]\n  train_a_drug2=[]\n  train_cell_line=[]\n  train_synergy=[]\n  train_senstivity=[]\n  test_f_drug1=[]\n  test_a_drug1=[]\n  test_f_drug2=[]\n  test_a_drug2=[]\n  test_cell_line=[]\n  test_synergy=[]\n  test_senstivity=[]\n  for i in range(len(index_train)):\n      train_a_drug1.append(a_drug1[index_train[i]])\n      train_a_drug2.append(a_drug2[index_train[i]])\n      train_f_drug1.append(f_drug1[index_train[i]])\n      train_f_drug2.append(f_drug2[index_train[i]])\n      train_cell_line.append(cell_line[index_train[i]])\n      train_synergy.append(synergy[index_train[i]])\n      train_senstivity.append(senstivity[index_train[i]])\n  for ii in range(len(index_test)):\n      test_a_drug1.append(a_drug1[index_test[ii]])\n      test_a_drug2.append(a_drug2[index_test[ii]])\n      test_f_drug1.append(f_drug1[index_test[ii]])\n      test_f_drug2.append(f_drug2[index_test[ii]])\n      test_cell_line.append(cell_line[index_test[ii]])\n      test_synergy.append(synergy[index_test[ii]])\n      test_senstivity.append(senstivity[index_test[ii]])\n\n  return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity\n\ndef train_test_input1(f_drug1,a_drug1,f_drug2,a_drug2,cell_line,index_test,synergy,senstivity):\n  train_f_drug1=[]\n  train_a_drug1=[]\n  train_f_drug2=[]\n  train_a_drug2=[]\n  train_cell_line=[]\n  train_synergy=[]\n  train_senstivity=[]\n  test_f_drug1=[]\n  test_a_drug1=[]\n  test_f_drug2=[]\n  test_a_drug2=[]\n  test_cell_line=[]\n  test_synergy=[]\n  test_senstivity=[]\n    \n  for i in range(len(f_drug1)):\n     ind=i+1\n     if(ind not in index_test):\n          train_a_drug1.append(a_drug1[i])\n          train_a_drug2.append(a_drug2[i])\n          train_f_drug1.append(f_drug1[i])\n          train_f_drug2.append(f_drug2[i])\n          train_cell_line.append(cell_line[i])\n          train_synergy.append(synergy[i])\n          train_senstivity.append(senstivity[i])\n  for ii in range(len(index_test)):\n#       ind1=ii-1\n      x=index_test[ii]-1\n      n=x[0]\n      test_a_drug1.append(a_drug1[n])\n      test_a_drug2.append(a_drug2[n])\n      test_f_drug1.append(f_drug1[n])\n      test_f_drug2.append(f_drug2[n])\n      test_cell_line.append(cell_line[n])\n      test_synergy.append(synergy[n])\n      test_senstivity.append(senstivity[n])\n    \n  return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity\n\n\ndef preprocess(index_train,index_test):\n    index_train1=[]\n    index_test1=[]\n    for i in range(len(index_train)):\n        index_train1.append((index_train[i])[0])\n        \n    for ii in range(len(index_test)):\n        index_test1.append((index_test[ii])[0])\n        \n    return index_train1,index_test1\n\n\n\ndef train_test_input2(f_drug1,a_drug1,f_drug2,a_drug2,cell_line,index_test,synergy,senstivity):\n  \n \n  train_a_drug1=a_drug1[3581:]\n  train_a_drug2=a_drug2[3581:]\n  train_f_drug1=f_drug1[3581:]\n  train_f_drug2=f_drug2[3581:]\n  train_cell_line=cell_line[3581:]\n  train_synergy=synergy[3581:]\n  train_senstivity=senstivity[3581:]\n  test_a_drug1=a_drug1[0:3581]\n  test_a_drug2=a_drug2[0:3581]\n  test_f_drug1=f_drug1[0:3581]\n  test_f_drug2=f_drug2[0:3581]\n  test_cell_line=cell_line[0:3581]\n  test_synergy=synergy[0:3581]\n  test_senstivity=senstivity[0:3581]\n    \n#   train_synergy=np.array(train_synergy)\n#   train_senstivity=np.array(train_senstivity)\n#   test_synergy=np.array(test_synergy)\n#   test_senstivity=np.array(test_senstivity)\n\n  return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity\n\n\ndef get_data_me1(s):\n    \n\n    !gdown --id 1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\n    dele=pd.read_excel('deleted_index.xls',header=None)\n    dele=np.array(dele)\n    dele=dele-1\n    \n    \n#     !gdown http://www.bioinf.jku.at/software/DeepSynergy/data_test_fold0_tanh.p.gz\n#     file = gzip.open(data_file, 'rb')\n#     X_tr, X_val, X_train, X_test, y_tr, y_val, y_train, y_test = pickle.load(file)\n#     file.close()\n    \n\n    !gdown 1SvLGjU---dvwsuCTqRMehtV-Gmf2Hk52\n    labels = pd.read_csv('pcbi.1006752.s004.csv', index_col=0) \n    \n    h=len(np.array(labels))\n    #labels are duplicated for the two different ways of ordering in the data\n#     labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['Fold']!=test_fold)\n    \n#     h=len(idx_train[0])\n    xx=np.where(idx_train == dele)[1]\n    idx_train=np.delete(idx_train,xx)\n    xx1=np.where(idx_train == (dele+h))[1]\n    idx_train=np.delete(idx_train,xx1)\n#     idx_train=idx_train[!dele]\n    #indices of test data for model testing: fold 0\n    idx_test = np.where(labels['Fold']==test_fold)\n#     h1=len(idx_test[0])\n    yy=np.where(idx_test == dele)[1]\n    idx_test=np.delete(idx_test,yy)\n    yy1=np.where(idx_test == (dele+h))[1]\n    idx_test=np.delete(idx_test,yy1)\n    \n#     X_train = X[idx_train]\n#     X_test = X[idx_test]\n    idx_train1=idx_train + (idx_train.shape)\n    idx_test1=idx_test + (idx_test.shape)\n    \n    idx_train1=idx_train + (h)\n    idx_test1=idx_test + (h)\n    idx_train1=np.r_[idx_train,idx_train1]\n    idx_test1=np.r_[idx_test,idx_test1]\n    #choose idx_train1 and idx_test1 if you want to duplicate data\n    return idx_train1,idx_test1\n\ndef get_data_me(s):\n   \n    !gdown --id 1R_E1txnkHrwMQlBKpG7a4BHcZM4kRSkj\n    dele=pd.read_excel('deleted_deep.xlsx',header=None)\n\n    dele=np.array(dele)\n    dele=dele-1\n \n#     !gdown http://www.bioinf.jku.at/software/DeepSynergy/data_test_fold0_tanh.p.gz\n#     file = gzip.open(data_file, 'rb')\n#     X_tr, X_val, X_train, X_test, y_tr, y_val, y_train, y_test = pickle.load(file)\n#     file.close()\n    \n    !gdown 1HNEch5czfqjThnZpFpP-Dcy0Qi-DQdc1\n    labels = pd.read_csv('labels1.csv', index_col=0)\n\n\n    h=len(np.array(labels))\n    #labels are duplicated for the two different ways of ordering in the data\n#     labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['fold']!=test_fold)\n    \n#     h=len(idx_train[0])\n    xx=np.where(idx_train == dele)[1]\n    idx_train=np.delete(idx_train,xx)\n    xx1=np.where(idx_train == (dele+h))[1]\n    idx_train=np.delete(idx_train,xx1)\n#     idx_train=idx_train[!dele]\n    #indices of test data for model testing: fold 0\n    idx_test = np.where(labels['fold']==test_fold)\n#     h1=len(idx_test[0])\n    yy=np.where(idx_test == dele)[1]\n    idx_test=np.delete(idx_test,yy)\n    yy1=np.where(idx_test == (dele+h))[1]\n    idx_test=np.delete(idx_test,yy1)\n    \n#     X_train = X[idx_train]\n#     X_test = X[idx_test]\n\n    y_train = labels.iloc[idx_train]['synergy'].values\n    y_test = labels.iloc[idx_test]['synergy'].values\n    y1_train = labels.iloc[idx_train]['senstivity'].values\n    y1_test = labels.iloc[idx_test]['senstivity'].values\n \n    return y_train,y_test,idx_train,idx_test,y1_train,y1_test\n\n\n\nsmiles,data_to_repeat,unique_drugs,unique_cell,deleted_index=get_data()\n# smiles,data_to_repeat,unique_drugs,unique_cell,deleted_index=get_data()\ndeleted_index=deleted_index-1\n\nfeature,adjacency=graph_node(smiles)\n# feature,edge_index=graph_node_edge(smiles)\ndata_to_repeat=np.r_[data_to_repeat,data_to_repeat]\nl=int((data_to_repeat.shape[0])/2)\ndata_to_repeat[l:,0]=data_to_repeat[0:l,1]\ndata_to_repeat[l:,1]=data_to_repeat[0:l,0]\n# np.random.shuffle(data_to_repeat)\nsynergy=data_to_repeat[:,13]\nsenstivity=data_to_repeat[:,5]\n# synergy=data_to_repeat[:,3]\n# senstivity=data_to_repeat[:,5]\n\n# f_drug1,f_drug2,a_drug1,a_drug2,synergy,senstivity=repeat_smiles(data_to_repeat,unique_drugs,feature,adjacency,deleted_index,synergy,senstivity)\n\nf_drug1,f_drug2,a_drug1,a_drug2,feature_cell=repeat_smiles1(data_to_repeat,unique_drugs,feature,adjacency,unique_cell)\n# f_drug1,f_drug2,a_drug1,a_drug2,feature_cell=repeat_smiles_deep(data_to_repeat,unique_drugs,feature,adjacency,unique_cell)\n# y_train,y_test,index_train,index_test,y1_train,y1_test=get_data_me(3)\nindex_train,index_test=get_data_me1(1)\n# train_synergy1, train_senstivity1,test_synergy1, test_senstivity1,index_train,index_test=split_data(feature_cell,synergy,senstivity)\n# feature_cell=np.array(feature_cell).astype(float)\n# index_train,index_test=split_data1(feature_cell,synergy,senstivity)\n# index_test,index_train=index1(1)\n# index_train,index_test=preprocess(index_train,index_test)\n# train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy, train_senstivity,test_synergy, test_senstivity=train_test_input1(f_drug1,a_drug1,f_drug2,a_drug2,feature_cell,index_test,synergy,senstivity)\ntrain_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_senstivity,test_synergy,test_senstivity=train_test_input(f_drug1,a_drug1,f_drug2,a_drug2,feature_cell,index_train,index_test,synergy,senstivity)\ntrain_cell_line=np.array(train_cell_line).astype(float)\ntest_cell_line=np.array(test_cell_line).astype(float)  \nnorm = \"tanh_norm\"\nif norm == \"tanh_norm\":\n    train_cell_line, mean, std, mean2, std2, feat_filt = normalize1(train_cell_line, norm=norm)\n    test_cell_line, mean, std, mean2, std2, feat_filt = normalize1(test_cell_line, mean, std, mean2, std2, \n                                                          feat_filt=feat_filt, norm=norm)\nelse:\n    train_cell_line, mean, std, feat_filt = normalize1(train_cell_line, norm=norm)\n    test_cell_line, mean, std, feat_filt = normalize1(test_cell_line, mean, std, feat_filt=feat_filt, norm=norm)\n  \nprint(synergy.shape)\n# print(np.random.shuffle(dataset1))\n# cc\n# cc1=[m for m, v in enumerate(unique_name) if cc in v]\n# # int((l))ik2\n# unique_cell[0].shape[1]\n# unique_name=unique_cell[:,0]\nprint(train_cell_line.shape)  \nprint(test_cell_line.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T13:16:26.261121Z","iopub.execute_input":"2022-12-12T13:16:26.261677Z","iopub.status.idle":"2022-12-12T13:19:17.745404Z","shell.execute_reply.started":"2022-12-12T13:16:26.261558Z","shell.execute_reply":"2022-12-12T13:19:17.743961Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting deepchem\n  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m845.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy<1.9 in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.7.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.3.5)\nRequirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.21.6)\nCollecting rdkit\n  Downloading rdkit-2022.9.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/29.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.0.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from deepchem) (1.0.2)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->deepchem) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->deepchem) (2022.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from rdkit->deepchem) (9.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->deepchem) (3.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\nInstalling collected packages: rdkit, deepchem\nSuccessfully installed deepchem-2.7.1 rdkit-2022.9.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pysmiles\n  Downloading pysmiles-1.0.2-py2.py3-none-any.whl (22 kB)\nRequirement already satisfied: networkx~=2.0 in /opt/conda/lib/python3.7/site-packages (from pysmiles) (2.5)\nRequirement already satisfied: pbr in /opt/conda/lib/python3.7/site-packages (from pysmiles) (5.10.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx~=2.0->pysmiles) (5.1.1)\nInstalling collected packages: pysmiles\nSuccessfully installed pysmiles-1.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: openpyxl in /opt/conda/lib/python3.7/site-packages (3.0.10)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.7/site-packages (from openpyxl) (1.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting PubChemPy\n  Downloading PubChemPy-1.0.4.tar.gz (29 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: PubChemPy\n  Building wheel for PubChemPy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PubChemPy: filename=PubChemPy-1.0.4-py3-none-any.whl size=13834 sha256=ba3cd1d55548d869fcbd960a04c9249d213f371588725bacf31865de9129f343\n  Stored in directory: /root/.cache/pip/wheels/7c/3d/8c/8192697412e9899dc55bbbb08bbc1197bef333caaa2a71c448\nSuccessfully built PubChemPy\nInstalling collected packages: PubChemPy\nSuccessfully installed PubChemPy-1.0.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting PyDrive\n  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (1.12.11)\nRequirement already satisfied: oauth2client>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (4.1.3)\nRequirement already satisfied: PyYAML>=3.0 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (6.0)\nRequirement already satisfied: google-api-core<3dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.33.2)\nRequirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (0.1.0)\nRequirement already satisfied: google-auth<3dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.35.0)\nRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\nRequirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (0.20.4)\nRequirement already satisfied: six<2dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.15.0)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (4.8)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (0.2.7)\nCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5\n  Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.28.1)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.56.3)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.2.4)\nRequirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (59.8.0)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.2->PyDrive) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2022.9.24)\nBuilding wheels for collected packages: PyDrive\n  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PyDrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27453 sha256=c3b25297161df1889a4503efabf85c0f021c1bcf0f23350db7b84d729d49c089\n  Stored in directory: /root/.cache/pip/wheels/57/cc/07/6aac75f5395a224650905accd38c868c2276782a56f1046b7b\nSuccessfully built PyDrive\nInstalling collected packages: protobuf, PyDrive\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Uninstalling protobuf-3.19.4:\n      Successfully uninstalled protobuf-3.19.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\ntensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\ntensorflow 2.6.4 requires tensorboard<2.7,>=2.6.0, but you have tensorboard 2.10.1 which is incompatible.\ntensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\ntensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\ntensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.12.0 requires protobuf<=3.20.1,>=3.12.2, but you have protobuf 3.20.3 which is incompatible.\nnnabla 1.31.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.47.0 requires grpcio>=1.47.0, but you have grpcio 1.43.0 which is incompatible.\ngcsfs 2022.5.0 requires fsspec==2022.5.0, but you have fsspec 2022.8.2 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed PyDrive-1.3.1 protobuf-3.20.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting xlrd==2.0.1\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m333.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: xlrd\nSuccessfully installed xlrd-2.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: Pandas==1.3.5 in /opt/conda/lib/python3.7/site-packages (1.3.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from Pandas==1.3.5) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from Pandas==1.3.5) (2022.1)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from Pandas==1.3.5) (1.21.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->Pandas==1.3.5) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting gdown\n  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.28.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.15.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.7.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.64.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.3)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","output_type":"stream"},{"name":"stdout","text":"hjjbjh\nDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 31.3MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1z1sWG4E9BqipP0iczLrbR4qxSa4KG95_\nTo: /kaggle/working/pcbi.1006752.s004.xls\n100%|██████████████████████████████████████| 5.24M/5.24M [00:00<00:00, 6.85MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 31.2MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1NoUKCRZ0CkNVFP1-578x_L0otPUMkz6T\nTo: /kaggle/working/unique934_cell.xlsx\n100%|█████████████████████████████████████████| 255k/255k [00:00<00:00, 774kB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\nTo: /kaggle/working/deleted_index.xls\n100%|██████████████████████████████████████| 58.9k/58.9k [00:00<00:00, 2.31MB/s]\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\nTo: /kaggle/working/deleted_index.xls\n100%|██████████████████████████████████████| 58.9k/58.9k [00:00<00:00, 2.65MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1SvLGjU---dvwsuCTqRMehtV-Gmf2Hk52\nTo: /kaggle/working/pcbi.1006752.s004.csv\n100%|██████████████████████████████████████| 2.57M/2.57M [00:00<00:00, 3.82MB/s]\n(37810,)\n(28618, 934)\n(7184, 934)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow\nfrom networkx.readwrite.graph6 import data_to_n\nfrom tensorflow.python.training.tracking import data_structures\n!pip install spektral\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import add,PReLU, Flatten,Reshape,Dropout, Input,Dense,Add,concatenate,BatchNormalization, Activation#,MultiHeadAttention,AdditiveAttention\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n# from tensorflow.random import set_seed\nfrom spektral.data.loaders import SingleLoader\nfrom spektral.datasets.citation import Citation\nfrom spektral.layers import GATConv, GlobalAvgPool,GlobalMaxPool, GCNConv,GlobalAttentionPool\nfrom spektral.transforms import LayerPreprocess\n# import tensorflow.compat.v1.keras.backend as K\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score#,AUC\nfrom scipy.stats import pearsonr,spearmanr\nfrom sklearn.metrics import roc_curve,auc\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras import regularizers\ndef generate_network1(x_in1,a_in1,x_in2,a_in2,cell, inDrop, drop):\n    # fill the architecture params from dict\n    gcn_layers = [78,156,312]\n    \n    gan_layers = [128,128]\n    cell_layers = [512,265,128]#for gcn\n#     cell_layers = [2048,512,128*2] #for gan\n    snp_layers = [512,128]#for gcn\n#     snp_layers = [2048,512,256]# for gan\n    dsn1_layers = [1024,2048,1024]\n    dsn2_layers = [1024,2048,1024]\n    N = x_in1[0].shape[0]\n    F = x_in1[0].shape[1]\n    \n    channels = 8  # Number of channels in each head of the first GAT layer\n    n_attn_heads = 10  # Number of attention heads in first GAT layer\n    dropout = 0.2 # Dropout rate for the features and adjacency matrix\n    drop=0.2\n    l2_reg = 1e-3  # L2 regularization rate\n    # contruct two parallel networks\n    x_in1 = Input(shape=(N,F,),name='x_in1')\n    a_in1 = Input((N,N,),name='a_in1')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d1 = Dropout(dropout)(x_in1)\n             middle_layer_d1  = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activty_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d1, a_in1])\n#              middle_layer_d1=BatchNormalization()(middle_layer_d1)\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n             gcn_output1=GlobalMaxPool(name='a1')(middle_layer_d1)\n             \n#              gcn_output1=Flatten()(x_in1)\n#              gcn_output1=GlobalAttentionPool(1000)(x_in1)#(middle_layer_d1)\n             \n             \n        else:\n             middle_layer_d1 = Dropout(dropout)(middle_layer_d1)\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n#              middle_layer_d1=BatchNormalization()(middle_layer_d1)\n#     print(x_in1.shape)\n    # print(a_in1.shape)\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n#             middle_layer1  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in1, a_in1])\n            \n#         elif l == (len(gan_layers)-1):\n#             middle_layer1 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n            \n#             gan_output1=GlobalMaxPool()(middle_layer1)\n# #             gan_output1 = Dense(int(gan_layers[l]), activation='tanh')(gan_output1)\n#         else:\n#             middle_layer1 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n            \n            \n    \n#   156  concatModel1 = concatenate([gcn_output1, gan_output1])\n    gcn_output1 = Dense(256, activation='relu')(gcn_output1)\n    gcn_output1 = Dropout(dropout)(gcn_output1)\n    gcn_output1 = Dense(128, activation='linear')(gcn_output1)\n    \n    gan_output1 = Dense(128, activation='relu')(gan_output1)\n   \n    \n    concatModel1 =  gan_output1\n    # # addtModel = Add([gcn_output, gan_output])\n    x_in2 = Input(shape=(N,F,),name='x_in2')\n    a_in2 = Input((N,N,),name='a_in2')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d2 = Dropout(dropout)(x_in2)\n             middle_layer_d2 =  GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d2, a_in2])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n             gcn_output2=GlobalMaxPool(name='a2')(middle_layer_d2)\n#             gcn_output2=Flatten()(x_in2)\n#             gcn_output2=GlobalAttentionPool(1000)(x_in2)#(middle_layer_d2)\n             \n        else:\n             middle_layer_d2 = Dropout(dropout)(middle_layer_d2)\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n#             middle_layer2  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in2, a_in2])\n           \n#         elif l == (len(gan_layers)-1):\n#             middle_layer2 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n#             gan_output2=GlobalMaxPool()(middle_layer2)\n# #             gan_output2 = Dense(int(gan_layers[l]), activation='tanh')(gan_output2)\n#         else:\n#             middle_layer2 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n    # print(gcn_output2.shape)\n    # print(gan_output2.shape)        \n#   156  concatModel2 = concatenate([gcn_output2, gan_output2])\n    gcn_output2 = Dense(256, activation='relu')(gcn_output2)\n    gcn_output2 = Dropout(dropout)(gcn_output2)\n    gcn_output2 = Dense(128, activation='linear')(gcn_output2)\n    \n    gan_output2 = Dense(128, activation='relu')(gan_output2)\n    \n    concatModel2 = gcn_output2\n    # concatModel2 = Add()([gcn_output2, gan_output2])\n    for cell_layer in range(len(cell_layers)):\n      if cell_layer == 0:\n        input_cell    = Input(shape=(cell[0].shape[0],))\n#         input_cell1 = BatchNormalization()(input_cell)\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,)(input_cell)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      elif cell_layer == (len(cell_layers)-1):\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,)(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      #  snp_output = Dense(1, activation='linear')(snpFC)\n      else:\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,)(cellFC)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n    # print(concatModel1.shape)\n    # print(cellFC.shape)\n    \n    concatModel=concatenate([concatModel2,concatModel1,cellFC])\n    concatModel=BatchNormalization()(concatModel)\n#     input2=concatenate([concatModel2,cellFC])\n    # input1=concatenate([gan_output1,cellFC])\n    # input2=concatenate([gan_output2,cellFC])\n    # # addtModel = Add([concatModel1, concatModel2])\n    # dsn1_layers = [2048,4096,2048]\n    # dsn2_layers = [2048,4096,2048]\n    # snp_layers = [2048,1024]\n    # # contruct two parallel networks\n    \n       \n#     for snp_layer in range(len(snp_layers)):\n#        if snp_layer == 0:\n#          snpFC1 = Dense(int(snp_layers[snp_layer]), activation='linear',use_bias=True,)(concatModel)\n#          snpFC1=PReLU()(snpFC1)\n#          snpFC = Dropout(float(drop))(snpFC1)\n# #          snpFC=BatchNormalization()(snpFC)\n#        elif snp_layer == (len(snp_layers)-1):\n#          snpFC = Dense(int(snp_layers[snp_layer]), activation='linear',use_bias=True,)(snpFC)\n#          snpFC=PReLU()(snpFC)\n# #          layer = MultiHeadAttention(num_heads=2, key_dim=2)\n# #          rr_task1=Reshape([1,snpFC.shape[1]])(snpFC)\n# #          a_task1=layer(rr_task1,rr_task1)\n#          snp_output1 = Dense(1, activation='linear')(snpFC)\n#        else:\n#           snpFC = Dense(int(snp_layers[snp_layer]), activation='linear',use_bias=True,)(snpFC)\n#           snpFC=PReLU()(snpFC)\n#           snpFC = Dropout(float(drop))(snpFC)\n\n\n    for snp_layer in range(len(snp_layers)):\n       if snp_layer == 0:\n         snpFC1 = Dense(int(snp_layers[snp_layer]), activation='relu',use_bias=True,)(concatModel)\n#          snpFC1=PReLU()(snpFC1)\n         snpFC = Dropout(float(drop))(snpFC1)\n#          snpFC=BatchNormalization()(snpFC)\n       elif snp_layer == (len(snp_layers)-1):\n         snpFC = Dense(int(snp_layers[snp_layer]), activation='relu',use_bias=True,)(snpFC)\n#          snpFC=PReLU()(snpFC)\n#          layer = MultiHeadAttention(num_heads=2, key_dim=2)\n#          rr_task1=Reshape([1,snpFC.shape[1]])(snpFC)\n#          a_task1=layer(rr_task1,rr_task1)\n         snp_output1 = Dense(1, activation='linear')(snpFC)\n       else:\n          snpFC = Dense(int(snp_layers[snp_layer]), activation='relu',use_bias=True,)(snpFC)\n#           snpFC=PReLU()(snpFC)\n          snpFC = Dropout(float(drop))(snpFC)\n#     model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [f_task1,f_task2])\n    model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [snp_output1])\n    # tf.keras.utils.plot_model(model.build_graph(), to_file=\"model.png\",\n    #        expand_nested=True, show_shapes=True)\n    print(model.summary())\n    return model\n\ndef trainer1(model, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity, epo, batch_size, earlyStop):\n#     model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=float(l_rate), momentum=0.9,nesterov=True, clipvalue=0.3))\n    model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=float(l_rate)))#, beta_1=0.9, beta_2=0.999, amsgrad=True))\n    model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line], [train_synergy], shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 )#,restore_best_weights=True,validation_split=0.1) \n                  #  validation_data=([val_input1,val_input2], [val_synergy,val_senstivity]))\n    # model.fit({\"input1\":train_drug1, \"input2\":train_drug2}, {\"synergy\":train_synergy,\"senstivity\":train_senstivity}, epochs=epo, shuffle=True, batch_size=batch_size,verbose=1) \n    #                validation_data=({\"input1\":val_input1,\"input2\":val_input2}, {\"synergy\":val_synergy,\"senstivity\":val_senstivity}))\n\n    return model\ntrain_f_drug1=np.array(train_f_drug1)\ntrain_a_drug1=np.array(train_a_drug1)\ntrain_f_drug2=np.array(train_f_drug2)\ntrain_a_drug2=np.array(train_a_drug2)\ntrain_cell_line=np.array(train_cell_line)\ntest_f_drug1=np.array(test_f_drug1)\ntest_a_drug1=np.array(test_a_drug1)\ntest_f_drug2=np.array(test_f_drug2)\ntest_a_drug2=np.array(test_a_drug2)\ntest_cell_line=np.array(test_cell_line)\ntrain_synergy=np.array(train_synergy)\ntrain_senstivity=np.array(train_senstivity)\ntest_synergy=np.array(test_synergy)\ntest_senstivity=np.array(test_senstivity)\n\n\n# train_synergy\n# test_synergy\n# train_synergy.shape\n# test_synergy.shape\n# train_senstivity","metadata":{"execution":{"iopub.status.busy":"2022-12-12T13:19:17.748474Z","iopub.execute_input":"2022-12-12T13:19:17.748917Z","iopub.status.idle":"2022-12-12T13:19:52.016100Z","shell.execute_reply.started":"2022-12-12T13:19:17.748874Z","shell.execute_reply":"2022-12-12T13:19:52.014733Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.6.4)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nCollecting numpy~=1.19.2\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.15.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.43.0)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (5.0)\nCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tensorboard<2.7,>=2.6.0\n  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.35.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (59.8.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.8.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.2.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.4.6)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.6.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.28.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (3.3.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (4.13.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2.1.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (3.2.0)\nInstalling collected packages: typing-extensions, numpy, h5py, tensorboard\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.1.1\n    Uninstalling typing_extensions-4.1.1:\n      Successfully uninstalled typing_extensions-4.1.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.7.0\n    Uninstalling h5py-3.7.0:\n      Successfully uninstalled h5py-3.7.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.10.1\n    Uninstalling tensorboard-2.10.1:\n      Successfully uninstalled tensorboard-2.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nxarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\ntensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\nrich 12.6.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.7.7 requires tensorboard>=2.9.1, but you have tensorboard 2.6.0 which is incompatible.\npytorch-lightning 1.7.7 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.12 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nonnx 1.12.0 requires protobuf<=3.20.1,>=3.12.2, but you have protobuf 3.20.3 which is incompatible.\nnnabla 1.31.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\nnnabla 1.31.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\njaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\njax 0.3.23 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\nflax 0.6.1 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\nfeaturetools 1.11.1 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndeepchem 2.7.1 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\ncmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\nallennlp 2.10.1 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\nallennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\naioitertools 0.11.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.93 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed h5py-3.1.0 numpy-1.19.5 tensorboard-2.6.0 typing-extensions-3.10.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting spektral\n  Downloading spektral-1.2.0-py3-none-any.whl (140 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m311.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from spektral) (1.0.2)\nRequirement already satisfied: tensorflow>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from spektral) (2.6.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from spektral) (1.19.5)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from spektral) (1.3.5)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from spektral) (2.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from spektral) (2.28.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from spektral) (4.64.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from spektral) (1.7.3)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from spektral) (4.9.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from spektral) (1.0.1)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.12.1)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.43.0)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (5.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.1.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.12)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (2.6.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (2.6.0)\nRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (2.6.0)\nRequirement already satisfied: typing-extensions<3.11,>=3.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.10.0.2)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.4.0)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.15.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.1.2)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\nRequirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.1.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.37.1)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (3.20.3)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (1.15.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->spektral) (5.1.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->spektral) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->spektral) (2022.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->spektral) (2022.9.24)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->spektral) (3.1.0)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=2.2.0->spektral) (1.5.2)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (1.35.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (1.8.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.6.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (2.2.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.4.6)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (59.8.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (3.3.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (4.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (4.13.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.2.0->spektral) (3.2.0)\nInstalling collected packages: spektral\nSuccessfully installed spektral-1.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"### from sklearn.metrics import roc_curve,auc\nl_rate = 0.0001\ninDrop = 0.2\ndrop = 0.2\nmax_epoch = 500\n# batch_size = 128 #gcn\nbatch_size = 64 #gan\nearlyStop_patience = 20#np.ceil(train_f_drug1.shape[0]/batch_size)#1000\n\n# model1= generate_network1(train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line, inDrop, drop)\n# plot_model(model1, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n# model1=trainer1(model1, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity,max_epoch, batch_size,\n#                                 earlyStop_patience)\n# # p1,p2= predict(model, [test_input1,test_input2])\n# p1= model1.predict( [test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line])\n# # s1=np.zeros(len(p1))\n# # s2=np.zeros(len(p1))\n# # for i in range(len(p1)):\n# #     n1=p1[0]\n# #     h1=n1[0]\n# #     x1=h1[0]\n# #     s1[i]=x1\n# #     n2=p2[i]\n# #     h2=n2[0]\n# #     x2=h2[0]\n\n# synergy_error=mean_squared_error(test_synergy, p1)\n# # senstivity_error=mean_squared_error(test_senstivity, p2)\n# # p1,p2\n\n# # senstivity_error=mean_squared_error(test_senstivity, p2)\n# synergy_error1=mean_absolute_error(test_synergy, p1)\n# # senstivity_error1=mean_absolute_error(test_senstivity, p2)\n# p12=[]\n# for i in range(len(p1)):\n#   x=p1[i]\n#   p12.append(x[0])\n# p12=np.array(p12)\n# synergy_error2=r2_score(test_synergy, p12)\n# # senstivity_error2=r2_score(test_senstivity, p2)\n# print(\"synergy_mean_squared_error\",synergy_error)\n# # print(\"senstivity_mean_squared_error\",senstivity_error)\n# print(\"synergy_mean_absolute_error\",synergy_error1)\n# # print(\"senstivity_mean_absolute_error\",senstivity_error1)\n# print(\"synergy_r2_score\",synergy_error2)\n# # print(\"senstivity_r2_score\",senstivity_error2)\n# losses = model1.evaluate([test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line],[test_synergy],batch_size =64)\n# print(losses)\n\n\n# synergy_pear= pearsonr(test_synergy, p12)\n# synergy_spear= spearmanr(test_synergy, p12)\n# print(\"synergy_pear\",synergy_pear)\n# print(\"synergy_spear\",synergy_spear)\n# p1\n# # senstivity_pear= pearsonr(test_senstivity, p2)\n# # senstivity_spear= spearmanr(test_senstivity, p2)\n# # print(\"senstivity_pear\",senstivity_pear)\n# # print(\"senstivity_spear\",senstivity_spear)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T13:19:52.018463Z","iopub.execute_input":"2022-12-12T13:19:52.019348Z","iopub.status.idle":"2022-12-12T13:19:52.032026Z","shell.execute_reply.started":"2022-12-12T13:19:52.019300Z","shell.execute_reply":"2022-12-12T13:19:52.030952Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# import tensorflow_datasets as tfds\nimport tensorflow as tf\ndef scaled_dot_product_attention(q, k, v, mask=None):\n  \"\"\"Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead)\n  but it must be broadcastable for addition.\n\n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable\n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n  Returns:\n    output, attention_weights\n  \"\"\"\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)\n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits)#, axis=-1)#tf.nn.linear()#, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output,attention_weights \n\n#multi-head\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self,*, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n\n    assert d_model % self.num_heads == 0\n\n    self.depth = d_model // self.num_heads\n\n    self.wq = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wk = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wv = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n\n    self.dense = tf.keras.layers.Dense(d_model)#,activation='relu',use_bias='true')\n\n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n\n  def call(self, v, k, q, mask=None):\n    batch_size = tf.shape(q)[0]\n\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n#     q=PReLU()(q)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n#     k=PReLU()(k)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n#     v=PReLU()(v)\n\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention,\n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n    return output#, attention_weights\n\n\n\nclass CrossStitch(tf.keras.layers.Layer):\n\n    \"\"\"Cross-Stitch implementation according to arXiv:1604.03539\n    Implementation adapted from https://github.com/helloyide/Cross-stitch-Networks-for-Multi-task-Learning\"\"\"\n\n    def __init__(self, num_tasks, *args, **kwargs):\n        \"\"\"initialize class variables\"\"\"\n        self.num_tasks = num_tasks\n        super(CrossStitch, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        \"\"\"initialize the kernel and set the instance to 'built'\"\"\"\n        self.kernel = self.add_weight(name=\"kernel\",\n                                      shape=(self.num_tasks,\n                                             self.num_tasks),\n                                      initializer='identity',\n                                      trainable=True)\n        super(CrossStitch, self).build(input_shape)\n\n    def call(self, xl):\n        \"\"\"\n        called by TensorFlow when the model gets build. \n        Returns a stacked tensor with num_tasks channels in the 0 dimension, \n        which need to be unstacked.\n        \"\"\"\n        if (len(xl) != self.num_tasks):\n            # should not happen\n            raise ValueError()\n\n        out_values = []\n        for this_task in range(self.num_tasks):\n            this_weight = self.kernel[this_task, this_task]\n            out = tf.math.scalar_mul(this_weight, xl[this_task])\n            for other_task in range(self.num_tasks):\n                if this_task == other_task:\n                    continue  # already weighted!\n                other_weight = self.kernel[this_task, other_task]\n                out += tf.math.scalar_mul(other_weight, xl[other_task])\n            out_values.append(out)\n        # HACK!\n        # unless we stack, and then unstack the tensors, TF (2.0.0) can't follow\n        # the graph, so it aborts during model initialization.\n        # return tf.stack(out_values, axis=0)\n        return out_values[0],out_values[1]\n\n    def compute_output_shape(self, input_shape):\n        return [self.num_tasks] + input_shape\n\n    def get_config(self):\n        \"\"\"implemented so keras can save the model to json/yml\"\"\"\n        config = {\n            \"num_tasks\": self.num_tasks\n        }\n        base_config = super(CrossStitch, self).get_config()\n        return dict(list(config.items()) + list(base_config.items()))\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T13:19:52.035868Z","iopub.execute_input":"2022-12-12T13:19:52.036732Z","iopub.status.idle":"2022-12-12T13:19:52.185879Z","shell.execute_reply.started":"2022-12-12T13:19:52.036672Z","shell.execute_reply":"2022-12-12T13:19:52.184326Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\ndef generate_network_att1(x_in1,a_in1,x_in2,a_in2,cell, inDrop, drop):\n    # fill the architecture params from dict\n#     gcn_layers = [1024,512,156]\n#     gan_layers = [1024,512]\n#     cell_layers = [2048,512]#,2048]\n#     snp_layers = [1024,512,128]#,2048]\n#     dsn1_layers = [1024,2048,1024]\n#     dsn2_layers = [1024,2048,1024]\n    # fill the architecture params from dict\n    gcn_layers = [78,156,312]\n#     gan_layers = [128,128]\n#     gcn_layers = [32,64,128]\n    cell_layers = [512,265,128]#for gcn\n#     cell_layers = [2048,512,256] #for gan\n    snp_layers = [512,128]#for gcn\n#     snp_layers = [2048,512,256]# for gan\n    dsn1_layers = [1024,2048,1024]\n    dsn2_layers = [1024,2048,1024]\n    N = x_in1[0].shape[0]\n    F = x_in1[0].shape[1]\n    \n    channels = 8  # Number of channels in each head of the first GAT layer\n    n_attn_heads = 10  # Number of attention heads in first GAT layer\n    dropout = 0.2 # Dropout rate for the features and adjacency matrix\n    drop=0.2\n    l2_reg = 1e-3\n    N = x_in1[0].shape[0]\n    F = x_in1[0].shape[1]\n    \n    channels = 8  # Number of channels in each head of the first GAT layer\n    n_attn_heads = 8  # Number of attention heads in first GAT layer\n    dropout = 0.2 # Dropout rate for the features and adjacency matrix\n    drop=0.2\n    l2_reg = 1e-3  # L2 regularization rate\n    # contruct two parallel networks\n    x_in1 = Input(shape=(N,F,),name='x_in1')\n    a_in1 = Input((N,N,),name='a_in1')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d1 = Dropout(dropout)(x_in1)\n             middle_layer_d1  = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activty_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d1, a_in1])\n#              middle_layer_d1=concatenate([middle_layer_d1,do_1_d1])\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n             gcn_output1=GlobalMaxPool()(middle_layer_d1)\n#              gcn_output1=GlobalAttentionPool(gcn_layers[l],name='a1')(middle_layer_d1)\n             \n#              gcn_output1=Flatten()(x_in1)\n#              gcn_output1=GlobalAttentionPool(1000)(x_in1)#(middle_layer_d1)\n             \n             \n        else:\n             middle_layer_d1 = Dropout(dropout)(middle_layer_d1)\n             middle_layer_d1 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d1, a_in1])\n#              middle_layer_d1=concatenate([middle_layer_d1,middle_layer_d11])\n#              middle_layer_d1=BatchNormalization()(middle_layer_d1)\n#     print(x_in1.shape)\n    # print(a_in1.shape)\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n#             middle_layer1  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in1, a_in1])\n#             middle_layer1=concatenate([middle_layer1,x_in1]) \n#         elif l == (len(gan_layers)-1):\n#             middle_layer1 = GATConv(\n#             gan_layers[l],\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n#             gan_output1=GlobalAttentionPool(512)(middle_layer1)\n# #             gan_output1 = Dense(int(gan_layers[l]), activation='elu')(gan_output1)\n#         else:\n#             middle_layer11 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer1, a_in1])\n#             middle_layer1=concatenate([middle_layer1,middle_layer11])\n    \n#  156   concatModel1 = concatenate([gcn_output1, gan_output1])\n    gcn_output1 = Dense(256, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output1)\n    gcn_output1 = Dropout(dropout)(gcn_output1)\n    gcn_output1 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output1)\n    \n#     gan_output1 = Dense(128, activation='relu')(gan_output1)\n    \n    concatModel1 =  gcn_output1\n    # # addtModel = Add([gcn_output, gan_output])\n    x_in2 = Input(shape=(N,F,),name='x_in2')\n    a_in2 = Input((N,N,),name='a_in2')\n    for l in range(len(gcn_layers)):\n        if l == 0:\n             \n             do_1_d2 = Dropout(dropout)(x_in2)\n             middle_layer_d2 =  GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([do_1_d2, a_in2])\n#              middle_layer_d2=concatenate([middle_layer_d2,do_1_d2])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n        elif l == (len(gcn_layers)-1):\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n#              gcn_output2=GlobalAttentionPool(gcn_layers[l],name='a2')(middle_layer_d2)\n             gcn_output2=GlobalMaxPool()(middle_layer_d2)\n#             gcn_output2=Flatten()(x_in2)\n#             gcn_output2=GlobalAttentionPool(1000)(x_in2)#(middle_layer_d2)\n             \n        else:\n             middle_layer_d2 = Dropout(dropout)(middle_layer_d2)\n             middle_layer_d2 = GCNConv(gcn_layers[l], activation='relu', use_bias=True, \n             kernel_initializer='glorot_uniform', \n             bias_initializer='zeros',kernel_regularizer=l2(l2_reg),\n             bias_regularizer=l2(l2_reg), activity_regularizer=None,\n             kernel_constraint=None, bias_constraint=None)([middle_layer_d2, a_in2])\n#              middle_layer_d2=concatenate([middle_layer_d2,middle_layer_d21])\n#              middle_layer_d2=BatchNormalization()(middle_layer_d2)\n\n#     for l in range(len(gan_layers)):\n#         if l == 0:\n            \n#             middle_layer2  = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([x_in2, a_in2])\n#             middle_layer2=concatenate([middle_layer2,x_in2])\n#         elif l == (len(gan_layers)-1):\n#             middle_layer2 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n#             gan_output2=GlobalAttentionPool(512)(middle_layer2)\n# #             gan_output2 = Dense(int(gan_layers[l]), activation='tanh')(gan_output2)\n#         else:\n#             middle_layer21 = GATConv(\n#             gan_layers[l],\n#             attn_heads=n_attn_heads,\n#             concat_heads=True,\n#             dropout_rate=dropout,\n#             activation='elu',\n#             kernel_regularizer=l2(l2_reg),\n#             attn_kernel_regularizer=l2(l2_reg),\n#             bias_regularizer=l2(l2_reg),\n#              )([middle_layer2, a_in2])\n#             middle_layer2=concatenate([middle_layer2,middle_layer_d21])\n    # print(gcn_output2.shape)\n    # print(gan_output2.shape)        \n#  156   concatModel2 = concatenate([gcn_output2, gan_output2])\n    gcn_output2 = Dense(256, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output2)\n    gcn_output2 = Dropout(dropout)(gcn_output2)\n    gcn_output2 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(gcn_output2)\n    \n#     gan_output2 = Dense(128, activation='relu')(gan_output2)\n        \n    concatModel2 = gcn_output2\n    # concatModel2 = Add()([gcn_output2, gan_output2])\n    for cell_layer in range(len(cell_layers)):\n      if cell_layer == 0:\n        input_cell    = Input(shape=(cell[0].shape[0],))\n#         input_cell1 = BatchNormalization()(input_cell)\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(input_cell)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      elif cell_layer == (len(cell_layers)-1):\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      #  snp_output = Dense(1, activation='linear')(snpFC)\n      else:\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n    # print(concatModel1.shape)\n    # print(cellFC.shape)\n    \n    concatModel=concatenate([concatModel2,concatModel1,cellFC])\n    concatModel=BatchNormalization()(concatModel)\n#     input2=concatenate([concatModel2,cellFC])\n    # input1=concatenate([gan_output1,cellFC])\n    # input2=concatenate([gan_output2,cellFC])\n    # # addtModel = Add([concatModel1, concatModel2])\n    # dsn1_layers = [2048,4096,2048]\n    # dsn2_layers = [2048,4096,2048]\n    # snp_layers = [2048,1024]\n    # # contruct two parallel networks\n  \n    \n#     task1=Dense(512, activation='relu',use_bias=True)(concatModel)\n# #     task1=PReLU()(task1)\n#     task2=Dense(512, activation='relu',use_bias=True)(concatModel)\n    \n    layer1 =MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task1= layer1(concatModel,concatModel,concatModel)\n    layer2 = MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task2= layer2(concatModel,concatModel,concatModel)\n    task11 = Reshape([a_task1.shape[2]])(a_task1)\n    task22 = Reshape([a_task2.shape[2]])(a_task2)\n    task1=concatenate([task11,concatModel])\n    task2=concatenate([task22,concatModel])\n#     task2=PReLU()(task2)\n#     task1=BatchNormalization()(task1)\n#     task2=BatchNormalization()(task2)\n    r_task1,r_task2 = CrossStitch(2)([task1,task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = apply_cross_stitch([task1,task2])\n#     r_task1=PReLU()(r_task1)\n#     r_task2=PReLU()(r_task2)\n\n#     r_task1=concatenate([r_task1,task1])\n#     r_task2=concatenate([r_task2,task2])\n#     r_task1=BatchNormalization()(r_task1)\n#     r_task2=BatchNormalization()(r_task2)\n#     r_task1=concatenate([r_task1,task1])\n#     r_task2=concatenate([r_task2,task2])\n    \n#     r_task1=Dense(256, activation='relu',use_bias=True)(r_task1)\n    \n#     r_task2=Dense(256, activation='relu',use_bias=True)(r_task2)\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n    \n    \n    r_task1=Dense(1024,activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    \n    r_task2=Dense(1024, activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n    r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n    \n#     layer1 =MultiHeadAttention(d_model=r_task1.shape[1], num_heads=4)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=4)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     task11 = Reshape([a_task1.shape[2]])(a_task1)\n#     task22 = Reshape([a_task2.shape[2]])(a_task2)\n#     r_task1=concatenate([task11,r_task1])\n#     r_task2=concatenate([task22,r_task2])\n    \n    r_task1=concatenate([r_task1,task1])\n    r_task2=concatenate([r_task2,task2])\n    \n#     r_task1=PReLU()(r_task1)\n#     r_task2=PReLU()(r_task2)\n\n \n   \n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = apply_cross_stitch([r_task1,r_task2])\n\n    \n#     r_task1=BatchNormalization()(r_task1)\n#     r_task2=BatchNormalization()(r_task2)\n    \n#     r_task1=Dense(128, activation='linear',use_bias=True)(r_task1)\n    \n#     r_task2=Dense(128, activation='linear',use_bias=True)(r_task2)\n#     r_task1=PReLU()(r_task1)\n#     r_task2=PReLU()(r_task2)\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = apply_cross_stitch(r_task1,r_task2)\n\n    \n#     r_task1=BatchNormalization()(r_task1)\n#     r_task2=BatchNormalization()(r_task2)\n    \n     \n#     layer1 =MultiHeadAttention(d_model=r_task1.shape[1], num_heads=4)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=4)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task1=BatchNormalization()(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n#     a_task2=BatchNormalization()(a_task2)\n    \n    \n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n#     r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n# #  \n#     rr_task2=Reshape([1,r_task2.shape[1]])(r_task2)\n#     a_task1=layer(rr_task1,rr_task1)\n#     a_task2=layer(rr_task2,rr_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n    \n#     layer1 = MultiHeadAttention(d_model=r_task1.shape[1], num_heads=8)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=8)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n    \n#     r_task1 = Dense(64, activation='linear',name=\"synergy\")(a_task1)\n#     r_task2 = Dense(64, activation='linear',name=\"senstivity\")(a_task2)\n    \n#     layer1 = MultiHeadAttention(d_model=r_task1.shape[1], num_heads=8)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=8)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     a_task1 = Reshape([a_task1.shape[2]])(a_task1)\n#     a_task1=BatchNormalization()(a_task1)\n#     a_task2 = Reshape([a_task2.shape[2]])(a_task2)\n#     a_task2=BatchNormalization()(a_task2)\n   \n    r_task1 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(128, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n    r_task2=PReLU()(r_task2)\n    r_task1 = Dense(64, activation='linear',name=\"synergy\",kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(64, activation='relu',name=\"senstivity\",kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n# \n#     r_task1 = Dense(512, activation='relu')(r_task1)\n# #   \n#     r_task2 = Dense(512, activation='relu')(r_task2)\n\n#     r_task1 = Dense(256, activation='relu',name=\"synergy\")(r_task1)\n#     r_task2 = Dense(256, activation='relu',name=\"senstivity\")(r_task2)\n    \n    snp_output1 = Dense(1, activation='linear')(r_task1)\n#     snp_output1=PReLU()(snp_output1)\n    snp_output2 = Dense(1, activation='relu')(r_task2)\n#     model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [f_task1,f_task2])\n    model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [snp_output1,snp_output2])\n    # tf.keras.utils.plot_model(model.build_graph(), to_file=\"model.png\",\n    #        expand_nested=True, show_shapes=True)\n    print(model.summary())\n    return model\n\ndef trainer_att1(model, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity, epo, batch_size, earlyStop):\n    model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=float(l_rate)))#, beta_1=0.9, beta_2=0.999, amsgrad=True))\n#     model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=float(l_rate), momentum=0.9, nesterov=True, clipnorm=1.))\n    model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line], [train_synergy,train_senstivity], shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 )#,restore_best_weights=True,validation_split=0.1) \n                  #  validation_data=([val_input1,val_input2], [val_synergy,val_senstivity]))\n    # model.fit({\"input1\":train_drug1, \"input2\":train_drug2}, {\"synergy\":train_synergy,\"senstivity\":train_senstivity}, epochs=epo, shuffle=True, batch_size=batch_size,verbose=1) \n    #                validation_data=({\"input1\":val_input1,\"input2\":val_input2}, {\"synergy\":val_synergy,\"senstivity\":val_senstivity}))\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-12-12T13:19:52.192274Z","iopub.execute_input":"2022-12-12T13:19:52.194699Z","iopub.status.idle":"2022-12-12T13:19:52.261422Z","shell.execute_reply.started":"2022-12-12T13:19:52.194660Z","shell.execute_reply":"2022-12-12T13:19:52.260084Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# l_rate = 0.0001\n# inDrop = 0.2\n# drop = 0.2\n# max_epoch = 500\n# batch_size =16\n\n\n# earlyStop_patience = np.ceil(train_f_drug1.shape[0]/batch_size)#1000\nmodel_att1= generate_network_att1(train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line, inDrop, drop)\nmodel_att1=trainer_att1(model_att1, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_senstivity,max_epoch, batch_size,\n                                earlyStop_patience)\n\n# p1,p2= predict(model, [test_input1,test_input2])\nap111,ap221= model_att1.predict( [test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line])\nasynergy_error1=mean_squared_error(test_synergy, ap111)\nasenstivity_error1=mean_squared_error(test_senstivity, ap221)\nasynergy_error11=mean_absolute_error(test_synergy, ap111)\nasenstivity_error11=mean_absolute_error(test_senstivity, ap221)\nasynergy_error21=r2_score(test_synergy, ap111)\nasenstivity_error21=r2_score(test_senstivity, ap221)\nprint(\"msynergy_mean_squared_error\",asynergy_error1)\nprint(\"msenstivity_mean_squared_error\",asenstivity_error1)\nprint(\"msynergy_mean_absolute_error\",asynergy_error11)\nprint(\"msenstivity_mean_absolute_error\",asenstivity_error11)\nprint(\"msynergy_r2_score\",asynergy_error21)\nprint(\"msenstivity_r2_score\",asenstivity_error21)\ncross_att1 = model_att1.evaluate([test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line],[test_synergy,test_senstivity],batch_size =64)\nprint(cross_att1)\nasynergy_pear1= pearsonr(test_synergy, ap111)\nasynergy_spear1= spearmanr(test_synergy, ap111)\nprint(\"msynergy_pear\",asynergy_pear1)\nprint(\"msynergy_spear\",asynergy_spear1)\nasenstivity_pear1= pearsonr(test_senstivity, ap221)\nasenstivity_spear1= spearmanr(test_senstivity, ap221)\nprint(\"msenstivity_pear\",asenstivity_pear1)\nprint(\"msenstivity_spear\",asenstivity_spear1)\nap111,ap221\n# yourTerminal:prompt> jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10 \n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T13:19:52.266440Z","iopub.execute_input":"2022-12-12T13:19:52.268741Z","iopub.status.idle":"2022-12-12T14:17:48.723667Z","shell.execute_reply.started":"2022-12-12T13:19:52.268695Z","shell.execute_reply":"2022-12-12T14:17:48.722680Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nx_in2 (InputLayer)              [(None, 69, 32)]     0                                            \n__________________________________________________________________________________________________\nx_in1 (InputLayer)              [(None, 69, 32)]     0                                            \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 69, 32)       0           x_in2[0][0]                      \n__________________________________________________________________________________________________\na_in2 (InputLayer)              [(None, 69, 69)]     0                                            \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 69, 32)       0           x_in1[0][0]                      \n__________________________________________________________________________________________________\na_in1 (InputLayer)              [(None, 69, 69)]     0                                            \n__________________________________________________________________________________________________\ngcn_conv_3 (GCNConv)            (None, 69, 78)       2574        dropout_3[0][0]                  \n                                                                 a_in2[0][0]                      \n__________________________________________________________________________________________________\ngcn_conv (GCNConv)              (None, 69, 78)       2574        dropout[0][0]                    \n                                                                 a_in1[0][0]                      \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 69, 78)       0           gcn_conv_3[0][0]                 \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 69, 78)       0           gcn_conv[0][0]                   \n__________________________________________________________________________________________________\ngcn_conv_4 (GCNConv)            (None, 69, 156)      12324       dropout_4[0][0]                  \n                                                                 a_in2[0][0]                      \n__________________________________________________________________________________________________\ngcn_conv_1 (GCNConv)            (None, 69, 156)      12324       dropout_1[0][0]                  \n                                                                 a_in1[0][0]                      \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            [(None, 934)]        0                                            \n__________________________________________________________________________________________________\ngcn_conv_5 (GCNConv)            (None, 69, 312)      48984       gcn_conv_4[0][0]                 \n                                                                 a_in2[0][0]                      \n__________________________________________________________________________________________________\ngcn_conv_2 (GCNConv)            (None, 69, 312)      48984       gcn_conv_1[0][0]                 \n                                                                 a_in1[0][0]                      \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 512)          478720      input_1[0][0]                    \n__________________________________________________________________________________________________\nglobal_max_pool_1 (GlobalMaxPoo (None, 312)          0           gcn_conv_5[0][0]                 \n__________________________________________________________________________________________________\nglobal_max_pool (GlobalMaxPool) (None, 312)          0           gcn_conv_2[0][0]                 \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 256)          80128       global_max_pool_1[0][0]          \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 256)          80128       global_max_pool[0][0]            \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 265)          135945      dropout_6[0][0]                  \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 256)          0           dense_2[0][0]                    \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 256)          0           dense[0][0]                      \n__________________________________________________________________________________________________\ndropout_7 (Dropout)             (None, 265)          0           dense_5[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 128)          32896       dropout_5[0][0]                  \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 128)          32896       dropout_2[0][0]                  \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 128)          34048       dropout_7[0][0]                  \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 384)          0           dense_3[0][0]                    \n                                                                 dense_1[0][0]                    \n                                                                 dense_6[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 384)          1536        concatenate[0][0]                \n__________________________________________________________________________________________________\nmulti_head_attention (MultiHead (None, None, 384)    591360      batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\nmulti_head_attention_1 (MultiHe (None, None, 384)    591360      batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 384)          0           multi_head_attention[0][0]       \n__________________________________________________________________________________________________\nreshape_1 (Reshape)             (None, 384)          0           multi_head_attention_1[0][0]     \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 768)          0           reshape[0][0]                    \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 768)          0           reshape_1[0][0]                  \n                                                                 batch_normalization[0][0]        \n__________________________________________________________________________________________________\ncross_stitch (CrossStitch)      ((None, 768), (None, 4           concatenate_1[0][0]              \n                                                                 concatenate_2[0][0]              \n__________________________________________________________________________________________________\ndense_15 (Dense)                (None, 1024)         787456      cross_stitch[0][0]               \n__________________________________________________________________________________________________\ndense_16 (Dense)                (None, 1024)         787456      cross_stitch[0][1]               \n__________________________________________________________________________________________________\ncross_stitch_1 (CrossStitch)    ((None, 1024), (None 4           dense_15[0][0]                   \n                                                                 dense_16[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 1792)         0           cross_stitch_1[0][0]             \n                                                                 concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndense_17 (Dense)                (None, 128)          229504      concatenate_3[0][0]              \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 1792)         0           cross_stitch_1[0][1]             \n                                                                 concatenate_2[0][0]              \n__________________________________________________________________________________________________\np_re_lu (PReLU)                 (None, 128)          128         dense_17[0][0]                   \n__________________________________________________________________________________________________\ndense_18 (Dense)                (None, 128)          229504      concatenate_4[0][0]              \n__________________________________________________________________________________________________\nsynergy (Dense)                 (None, 64)           8256        p_re_lu[0][0]                    \n__________________________________________________________________________________________________\np_re_lu_1 (PReLU)               (None, 128)          128         dense_18[0][0]                   \n__________________________________________________________________________________________________\np_re_lu_2 (PReLU)               (None, 64)           64          synergy[0][0]                    \n__________________________________________________________________________________________________\nsenstivity (Dense)              (None, 64)           8256        p_re_lu_1[0][0]                  \n__________________________________________________________________________________________________\ndense_19 (Dense)                (None, 1)            65          p_re_lu_2[0][0]                  \n__________________________________________________________________________________________________\ndense_20 (Dense)                (None, 1)            65          senstivity[0][0]                 \n==================================================================================================\nTotal params: 4,237,671\nTrainable params: 4,236,903\nNon-trainable params: 768\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n2022-12-12 13:19:59.263268: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 252754176 exceeds 10% of free system memory.\n2022-12-12 13:19:59.756675: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 545001192 exceeds 10% of free system memory.\n2022-12-12 13:20:00.902413: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 252754176 exceeds 10% of free system memory.\n2022-12-12 13:20:01.827570: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 545001192 exceeds 10% of free system memory.\n2022-12-12 13:20:02.645364: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 252754176 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n448/448 [==============================] - 12s 16ms/step - loss: 775.1669 - dense_19_loss: 519.5069 - dense_20_loss: 127.6657\nEpoch 2/500\n448/448 [==============================] - 7s 16ms/step - loss: 702.0804 - dense_19_loss: 483.0885 - dense_20_loss: 94.0051\nEpoch 3/500\n448/448 [==============================] - 7s 15ms/step - loss: 675.9263 - dense_19_loss: 468.0392 - dense_20_loss: 84.7671\nEpoch 4/500\n448/448 [==============================] - 7s 17ms/step - loss: 658.8951 - dense_19_loss: 458.2232 - dense_20_loss: 79.0590\nEpoch 5/500\n448/448 [==============================] - 7s 15ms/step - loss: 641.9553 - dense_19_loss: 445.2128 - dense_20_loss: 76.4039\nEpoch 6/500\n448/448 [==============================] - 7s 15ms/step - loss: 627.4459 - dense_19_loss: 435.2522 - dense_20_loss: 72.9787\nEpoch 7/500\n448/448 [==============================] - 7s 16ms/step - loss: 612.0610 - dense_19_loss: 423.9848 - dense_20_loss: 69.8989\nEpoch 8/500\n448/448 [==============================] - 7s 16ms/step - loss: 596.1522 - dense_19_loss: 411.1863 - dense_20_loss: 67.7305\nEpoch 9/500\n448/448 [==============================] - 7s 16ms/step - loss: 586.8987 - dense_19_loss: 404.9778 - dense_20_loss: 65.6221\nEpoch 10/500\n448/448 [==============================] - 7s 16ms/step - loss: 572.2744 - dense_19_loss: 394.4183 - dense_20_loss: 62.4145\nEpoch 11/500\n448/448 [==============================] - 7s 15ms/step - loss: 561.8362 - dense_19_loss: 386.4870 - dense_20_loss: 60.8072\nEpoch 12/500\n448/448 [==============================] - 7s 16ms/step - loss: 549.9691 - dense_19_loss: 377.5065 - dense_20_loss: 58.7685\nEpoch 13/500\n448/448 [==============================] - 7s 16ms/step - loss: 537.2362 - dense_19_loss: 367.6805 - dense_20_loss: 56.6475\nEpoch 14/500\n448/448 [==============================] - 7s 15ms/step - loss: 531.9700 - dense_19_loss: 364.9986 - dense_20_loss: 54.6934\nEpoch 15/500\n448/448 [==============================] - 7s 16ms/step - loss: 518.3190 - dense_19_loss: 354.0702 - dense_20_loss: 52.6714\nEpoch 16/500\n448/448 [==============================] - 7s 16ms/step - loss: 508.4387 - dense_19_loss: 345.8362 - dense_20_loss: 51.5892\nEpoch 17/500\n448/448 [==============================] - 7s 16ms/step - loss: 501.0829 - dense_19_loss: 340.8366 - dense_20_loss: 49.8840\nEpoch 18/500\n448/448 [==============================] - 7s 16ms/step - loss: 497.2290 - dense_19_loss: 339.2995 - dense_20_loss: 48.1401\nEpoch 19/500\n448/448 [==============================] - 7s 16ms/step - loss: 480.5569 - dense_19_loss: 325.0027 - dense_20_loss: 46.2825\nEpoch 20/500\n448/448 [==============================] - 7s 16ms/step - loss: 472.8370 - dense_19_loss: 318.7392 - dense_20_loss: 45.2572\nEpoch 21/500\n448/448 [==============================] - 7s 16ms/step - loss: 468.5153 - dense_19_loss: 315.8560 - dense_20_loss: 44.3175\nEpoch 22/500\n448/448 [==============================] - 7s 16ms/step - loss: 459.9411 - dense_19_loss: 309.7124 - dense_20_loss: 42.4305\nEpoch 23/500\n448/448 [==============================] - 7s 16ms/step - loss: 450.8645 - dense_19_loss: 302.2772 - dense_20_loss: 41.1819\nEpoch 24/500\n448/448 [==============================] - 7s 16ms/step - loss: 449.6996 - dense_19_loss: 302.7147 - dense_20_loss: 39.9848\nEpoch 25/500\n448/448 [==============================] - 7s 15ms/step - loss: 436.6483 - dense_19_loss: 291.4069 - dense_20_loss: 38.7494\nEpoch 26/500\n448/448 [==============================] - 8s 17ms/step - loss: 426.5566 - dense_19_loss: 282.9872 - dense_20_loss: 37.4696\nEpoch 27/500\n448/448 [==============================] - 7s 16ms/step - loss: 421.0299 - dense_19_loss: 278.6223 - dense_20_loss: 36.6812\nEpoch 28/500\n448/448 [==============================] - 7s 15ms/step - loss: 418.2896 - dense_19_loss: 277.2228 - dense_20_loss: 35.6914\nEpoch 29/500\n448/448 [==============================] - 7s 16ms/step - loss: 405.3706 - dense_19_loss: 265.8262 - dense_20_loss: 34.6415\nEpoch 30/500\n448/448 [==============================] - 7s 16ms/step - loss: 397.7783 - dense_19_loss: 259.6696 - dense_20_loss: 33.4335\nEpoch 31/500\n448/448 [==============================] - 7s 16ms/step - loss: 390.1473 - dense_19_loss: 253.3669 - dense_20_loss: 32.5531\nEpoch 32/500\n448/448 [==============================] - 7s 16ms/step - loss: 384.6950 - dense_19_loss: 248.6508 - dense_20_loss: 32.1182\nEpoch 33/500\n448/448 [==============================] - 7s 15ms/step - loss: 375.7695 - dense_19_loss: 241.0827 - dense_20_loss: 31.1647\nEpoch 34/500\n448/448 [==============================] - 7s 15ms/step - loss: 366.3680 - dense_19_loss: 232.8633 - dense_20_loss: 30.1872\nEpoch 35/500\n448/448 [==============================] - 7s 17ms/step - loss: 359.1283 - dense_19_loss: 226.5856 - dense_20_loss: 29.6141\nEpoch 36/500\n448/448 [==============================] - 7s 15ms/step - loss: 366.6479 - dense_19_loss: 234.8977 - dense_20_loss: 29.1833\nEpoch 37/500\n448/448 [==============================] - 7s 16ms/step - loss: 353.6396 - dense_19_loss: 222.6472 - dense_20_loss: 28.5974\nEpoch 38/500\n448/448 [==============================] - 7s 15ms/step - loss: 345.2277 - dense_19_loss: 215.5305 - dense_20_loss: 27.5456\nEpoch 39/500\n448/448 [==============================] - 7s 15ms/step - loss: 342.9319 - dense_19_loss: 213.7744 - dense_20_loss: 27.2997\nEpoch 40/500\n448/448 [==============================] - 7s 17ms/step - loss: 338.0446 - dense_19_loss: 209.7449 - dense_20_loss: 26.8159\nEpoch 41/500\n448/448 [==============================] - 7s 16ms/step - loss: 332.7595 - dense_19_loss: 205.5517 - dense_20_loss: 26.0814\nEpoch 42/500\n448/448 [==============================] - 7s 15ms/step - loss: 328.2692 - dense_19_loss: 201.6923 - dense_20_loss: 25.6997\nEpoch 43/500\n448/448 [==============================] - 7s 16ms/step - loss: 325.5498 - dense_19_loss: 199.8606 - dense_20_loss: 25.0685\nEpoch 44/500\n448/448 [==============================] - 7s 16ms/step - loss: 319.1071 - dense_19_loss: 194.4282 - dense_20_loss: 24.3029\nEpoch 45/500\n448/448 [==============================] - 7s 15ms/step - loss: 311.6349 - dense_19_loss: 187.8348 - dense_20_loss: 23.8002\nEpoch 46/500\n448/448 [==============================] - 7s 16ms/step - loss: 314.5480 - dense_19_loss: 191.5361 - dense_20_loss: 23.2075\nEpoch 47/500\n448/448 [==============================] - 7s 15ms/step - loss: 305.0732 - dense_19_loss: 183.2078 - dense_20_loss: 22.3374\nEpoch 48/500\n448/448 [==============================] - 7s 15ms/step - loss: 307.0563 - dense_19_loss: 185.3399 - dense_20_loss: 22.4215\nEpoch 49/500\n448/448 [==============================] - 8s 17ms/step - loss: 298.4042 - dense_19_loss: 177.2134 - dense_20_loss: 22.1945\nEpoch 50/500\n448/448 [==============================] - 7s 15ms/step - loss: 297.2691 - dense_19_loss: 176.5969 - dense_20_loss: 21.9548\nEpoch 51/500\n448/448 [==============================] - 7s 16ms/step - loss: 292.9564 - dense_19_loss: 173.0561 - dense_20_loss: 21.2964\nEpoch 52/500\n448/448 [==============================] - 7s 16ms/step - loss: 283.4458 - dense_19_loss: 164.7399 - dense_20_loss: 20.4313\nEpoch 53/500\n448/448 [==============================] - 7s 16ms/step - loss: 279.1453 - dense_19_loss: 161.4313 - dense_20_loss: 19.6940\nEpoch 54/500\n448/448 [==============================] - 7s 16ms/step - loss: 282.8063 - dense_19_loss: 164.8673 - dense_20_loss: 20.1030\nEpoch 55/500\n448/448 [==============================] - 7s 15ms/step - loss: 280.0747 - dense_19_loss: 163.0170 - dense_20_loss: 19.4263\nEpoch 56/500\n448/448 [==============================] - 7s 15ms/step - loss: 267.1268 - dense_19_loss: 150.8289 - dense_20_loss: 18.9034\nEpoch 57/500\n448/448 [==============================] - 7s 16ms/step - loss: 271.6076 - dense_19_loss: 155.9873 - dense_20_loss: 18.4478\nEpoch 58/500\n448/448 [==============================] - 7s 16ms/step - loss: 271.0596 - dense_19_loss: 155.6058 - dense_20_loss: 18.3791\nEpoch 59/500\n448/448 [==============================] - 7s 16ms/step - loss: 261.9522 - dense_19_loss: 147.1532 - dense_20_loss: 17.9816\nEpoch 60/500\n448/448 [==============================] - 7s 16ms/step - loss: 262.9281 - dense_19_loss: 148.5765 - dense_20_loss: 17.7990\nEpoch 61/500\n448/448 [==============================] - 7s 16ms/step - loss: 254.3713 - dense_19_loss: 140.9907 - dense_20_loss: 17.0229\nEpoch 62/500\n448/448 [==============================] - 7s 17ms/step - loss: 251.6945 - dense_19_loss: 138.5441 - dense_20_loss: 17.0886\nEpoch 63/500\n448/448 [==============================] - 7s 16ms/step - loss: 250.0967 - dense_19_loss: 137.1844 - dense_20_loss: 16.9621\nEpoch 64/500\n448/448 [==============================] - 7s 15ms/step - loss: 248.8651 - dense_19_loss: 136.6791 - dense_20_loss: 16.4229\nEpoch 65/500\n448/448 [==============================] - 7s 16ms/step - loss: 244.7538 - dense_19_loss: 133.1299 - dense_20_loss: 16.1181\nEpoch 66/500\n448/448 [==============================] - 7s 16ms/step - loss: 243.6111 - dense_19_loss: 132.3479 - dense_20_loss: 15.9324\nEpoch 67/500\n448/448 [==============================] - 7s 16ms/step - loss: 241.4157 - dense_19_loss: 130.8156 - dense_20_loss: 15.4245\nEpoch 68/500\n448/448 [==============================] - 7s 16ms/step - loss: 231.0382 - dense_19_loss: 121.0614 - dense_20_loss: 14.9394\nEpoch 69/500\n448/448 [==============================] - 7s 16ms/step - loss: 233.1684 - dense_19_loss: 123.0498 - dense_20_loss: 15.2719\nEpoch 70/500\n448/448 [==============================] - 7s 16ms/step - loss: 228.2144 - dense_19_loss: 119.0046 - dense_20_loss: 14.6075\nEpoch 71/500\n448/448 [==============================] - 8s 17ms/step - loss: 229.3597 - dense_19_loss: 120.4036 - dense_20_loss: 14.5447\nEpoch 72/500\n448/448 [==============================] - 7s 16ms/step - loss: 230.5396 - dense_19_loss: 122.0004 - dense_20_loss: 14.3151\nEpoch 73/500\n448/448 [==============================] - 7s 15ms/step - loss: 221.9200 - dense_19_loss: 113.6655 - dense_20_loss: 14.0744\nEpoch 74/500\n448/448 [==============================] - 7s 16ms/step - loss: 219.7594 - dense_19_loss: 111.8687 - dense_20_loss: 13.9690\nEpoch 75/500\n448/448 [==============================] - 7s 15ms/step - loss: 216.8256 - dense_19_loss: 109.4496 - dense_20_loss: 13.6313\nEpoch 76/500\n448/448 [==============================] - 8s 17ms/step - loss: 211.3635 - dense_19_loss: 104.5207 - dense_20_loss: 13.2978\nEpoch 77/500\n448/448 [==============================] - 7s 16ms/step - loss: 208.5621 - dense_19_loss: 102.2389 - dense_20_loss: 12.9773\nEpoch 78/500\n448/448 [==============================] - 7s 15ms/step - loss: 207.9956 - dense_19_loss: 101.8237 - dense_20_loss: 12.9243\nEpoch 79/500\n448/448 [==============================] - 7s 16ms/step - loss: 205.5365 - dense_19_loss: 99.7752 - dense_20_loss: 12.7556\nEpoch 80/500\n448/448 [==============================] - 7s 17ms/step - loss: 215.0571 - dense_19_loss: 109.0898 - dense_20_loss: 13.0586\nEpoch 81/500\n448/448 [==============================] - 7s 15ms/step - loss: 200.8040 - dense_19_loss: 95.8564 - dense_20_loss: 12.2831\nEpoch 82/500\n448/448 [==============================] - 7s 16ms/step - loss: 202.8587 - dense_19_loss: 97.8914 - dense_20_loss: 12.3658\nEpoch 83/500\n448/448 [==============================] - 7s 16ms/step - loss: 202.3160 - dense_19_loss: 97.5249 - dense_20_loss: 12.3406\nEpoch 84/500\n448/448 [==============================] - 7s 16ms/step - loss: 201.1764 - dense_19_loss: 96.5680 - dense_20_loss: 12.3609\nEpoch 85/500\n448/448 [==============================] - 7s 16ms/step - loss: 194.1546 - dense_19_loss: 89.9957 - dense_20_loss: 11.9804\nEpoch 86/500\n448/448 [==============================] - 7s 16ms/step - loss: 189.8105 - dense_19_loss: 86.5267 - dense_20_loss: 11.3542\nEpoch 87/500\n448/448 [==============================] - 7s 15ms/step - loss: 192.8979 - dense_19_loss: 89.7912 - dense_20_loss: 11.4003\nEpoch 88/500\n448/448 [==============================] - 7s 16ms/step - loss: 190.6196 - dense_19_loss: 87.5336 - dense_20_loss: 11.4618\nEpoch 89/500\n448/448 [==============================] - 7s 16ms/step - loss: 188.1229 - dense_19_loss: 85.4592 - dense_20_loss: 11.2205\nEpoch 90/500\n448/448 [==============================] - 7s 16ms/step - loss: 183.0628 - dense_19_loss: 80.9808 - dense_20_loss: 10.9006\nEpoch 91/500\n448/448 [==============================] - 7s 16ms/step - loss: 184.6591 - dense_19_loss: 82.4667 - dense_20_loss: 11.1310\nEpoch 92/500\n448/448 [==============================] - 7s 15ms/step - loss: 180.8786 - dense_19_loss: 79.4947 - dense_20_loss: 10.5514\nEpoch 93/500\n448/448 [==============================] - 7s 16ms/step - loss: 183.2811 - dense_19_loss: 81.6694 - dense_20_loss: 10.9367\nEpoch 94/500\n448/448 [==============================] - 7s 16ms/step - loss: 181.5867 - dense_19_loss: 80.3771 - dense_20_loss: 10.6780\nEpoch 95/500\n448/448 [==============================] - 7s 15ms/step - loss: 181.3630 - dense_19_loss: 80.1980 - dense_20_loss: 10.6477\nEpoch 96/500\n448/448 [==============================] - 7s 16ms/step - loss: 172.6889 - dense_19_loss: 72.5741 - dense_20_loss: 9.9696\nEpoch 97/500\n448/448 [==============================] - 7s 16ms/step - loss: 170.5011 - dense_19_loss: 70.6148 - dense_20_loss: 9.9180\nEpoch 98/500\n448/448 [==============================] - 7s 16ms/step - loss: 175.6678 - dense_19_loss: 75.7171 - dense_20_loss: 10.1979\nEpoch 99/500\n448/448 [==============================] - 7s 16ms/step - loss: 170.6182 - dense_19_loss: 70.9784 - dense_20_loss: 9.9982\nEpoch 100/500\n448/448 [==============================] - 7s 16ms/step - loss: 169.6233 - dense_19_loss: 70.1796 - dense_20_loss: 9.9860\nEpoch 101/500\n448/448 [==============================] - 7s 15ms/step - loss: 168.1270 - dense_19_loss: 69.1750 - dense_20_loss: 9.7121\nEpoch 102/500\n448/448 [==============================] - 7s 16ms/step - loss: 168.1350 - dense_19_loss: 69.3649 - dense_20_loss: 9.6494\nEpoch 103/500\n448/448 [==============================] - 7s 16ms/step - loss: 160.7367 - dense_19_loss: 62.7884 - dense_20_loss: 9.1247\nEpoch 104/500\n448/448 [==============================] - 7s 16ms/step - loss: 171.1919 - dense_19_loss: 73.0578 - dense_20_loss: 9.5079\nEpoch 105/500\n448/448 [==============================] - 7s 16ms/step - loss: 164.1910 - dense_19_loss: 66.0812 - dense_20_loss: 9.5591\nEpoch 106/500\n448/448 [==============================] - 7s 15ms/step - loss: 158.5757 - dense_19_loss: 61.2311 - dense_20_loss: 9.0239\nEpoch 107/500\n448/448 [==============================] - 7s 17ms/step - loss: 166.9663 - dense_19_loss: 69.4509 - dense_20_loss: 9.3109\nEpoch 108/500\n448/448 [==============================] - 7s 16ms/step - loss: 157.9026 - dense_19_loss: 60.8042 - dense_20_loss: 9.1229\nEpoch 109/500\n448/448 [==============================] - 7s 15ms/step - loss: 152.9357 - dense_19_loss: 56.5261 - dense_20_loss: 8.7423\nEpoch 110/500\n448/448 [==============================] - 7s 15ms/step - loss: 152.6229 - dense_19_loss: 56.6306 - dense_20_loss: 8.5701\nEpoch 111/500\n448/448 [==============================] - 7s 16ms/step - loss: 152.3722 - dense_19_loss: 56.7052 - dense_20_loss: 8.4799\nEpoch 112/500\n448/448 [==============================] - 7s 15ms/step - loss: 155.0333 - dense_19_loss: 59.2517 - dense_20_loss: 8.7160\nEpoch 113/500\n448/448 [==============================] - 7s 16ms/step - loss: 153.9750 - dense_19_loss: 58.4875 - dense_20_loss: 8.6133\nEpoch 114/500\n448/448 [==============================] - 7s 15ms/step - loss: 151.1285 - dense_19_loss: 56.0852 - dense_20_loss: 8.4502\nEpoch 115/500\n448/448 [==============================] - 7s 16ms/step - loss: 151.1331 - dense_19_loss: 56.3721 - dense_20_loss: 8.3851\nEpoch 116/500\n448/448 [==============================] - 7s 16ms/step - loss: 148.9873 - dense_19_loss: 54.5185 - dense_20_loss: 8.3237\nEpoch 117/500\n448/448 [==============================] - 7s 16ms/step - loss: 147.7133 - dense_19_loss: 53.8229 - dense_20_loss: 7.9930\nEpoch 118/500\n448/448 [==============================] - 7s 16ms/step - loss: 152.8758 - dense_19_loss: 58.7151 - dense_20_loss: 8.3433\nEpoch 119/500\n448/448 [==============================] - 7s 16ms/step - loss: 147.3385 - dense_19_loss: 53.6787 - dense_20_loss: 8.0354\nEpoch 120/500\n448/448 [==============================] - 7s 15ms/step - loss: 141.9002 - dense_19_loss: 48.9166 - dense_20_loss: 7.7253\nEpoch 121/500\n448/448 [==============================] - 7s 16ms/step - loss: 142.6346 - dense_19_loss: 49.7285 - dense_20_loss: 7.8734\nEpoch 122/500\n448/448 [==============================] - 7s 15ms/step - loss: 144.0736 - dense_19_loss: 51.3539 - dense_20_loss: 7.8746\nEpoch 123/500\n448/448 [==============================] - 7s 15ms/step - loss: 141.9705 - dense_19_loss: 49.4081 - dense_20_loss: 7.9450\nEpoch 124/500\n448/448 [==============================] - 7s 15ms/step - loss: 145.4992 - dense_19_loss: 53.2008 - dense_20_loss: 7.8493\nEpoch 125/500\n448/448 [==============================] - 7s 15ms/step - loss: 142.1774 - dense_19_loss: 50.1109 - dense_20_loss: 7.8031\nEpoch 126/500\n448/448 [==============================] - 7s 16ms/step - loss: 137.0973 - dense_19_loss: 45.5947 - dense_20_loss: 7.5231\nEpoch 127/500\n448/448 [==============================] - 7s 16ms/step - loss: 139.4147 - dense_19_loss: 48.1899 - dense_20_loss: 7.5247\nEpoch 128/500\n448/448 [==============================] - 7s 15ms/step - loss: 137.6878 - dense_19_loss: 46.9679 - dense_20_loss: 7.2326\nEpoch 129/500\n448/448 [==============================] - 7s 16ms/step - loss: 132.1858 - dense_19_loss: 41.8916 - dense_20_loss: 7.1855\nEpoch 130/500\n448/448 [==============================] - 7s 16ms/step - loss: 135.0134 - dense_19_loss: 44.8300 - dense_20_loss: 7.2614\nEpoch 131/500\n448/448 [==============================] - 7s 15ms/step - loss: 136.4802 - dense_19_loss: 46.2301 - dense_20_loss: 7.5206\nEpoch 132/500\n448/448 [==============================] - 7s 16ms/step - loss: 137.6236 - dense_19_loss: 47.5904 - dense_20_loss: 7.4234\nEpoch 133/500\n448/448 [==============================] - 7s 15ms/step - loss: 131.1707 - dense_19_loss: 41.9575 - dense_20_loss: 6.9789\nEpoch 134/500\n448/448 [==============================] - 7s 16ms/step - loss: 132.0501 - dense_19_loss: 43.1369 - dense_20_loss: 6.9124\nEpoch 135/500\n448/448 [==============================] - 7s 16ms/step - loss: 132.5136 - dense_19_loss: 43.5997 - dense_20_loss: 7.0636\nEpoch 136/500\n448/448 [==============================] - 7s 15ms/step - loss: 130.2476 - dense_19_loss: 41.7876 - dense_20_loss: 6.9092\nEpoch 137/500\n448/448 [==============================] - 7s 16ms/step - loss: 133.1151 - dense_19_loss: 44.7257 - dense_20_loss: 7.1321\nEpoch 138/500\n448/448 [==============================] - 7s 16ms/step - loss: 134.9467 - dense_19_loss: 46.6265 - dense_20_loss: 7.1587\nEpoch 139/500\n448/448 [==============================] - 7s 15ms/step - loss: 127.7866 - dense_19_loss: 40.2141 - dense_20_loss: 6.7380\nEpoch 140/500\n448/448 [==============================] - 7s 16ms/step - loss: 124.7852 - dense_19_loss: 37.8141 - dense_20_loss: 6.4349\nEpoch 141/500\n448/448 [==============================] - 7s 15ms/step - loss: 129.1699 - dense_19_loss: 42.1131 - dense_20_loss: 6.7376\nEpoch 142/500\n448/448 [==============================] - 7s 15ms/step - loss: 133.1217 - dense_19_loss: 45.7890 - dense_20_loss: 7.0296\nEpoch 143/500\n448/448 [==============================] - 7s 15ms/step - loss: 126.3882 - dense_19_loss: 39.7657 - dense_20_loss: 6.6561\nEpoch 144/500\n448/448 [==============================] - 7s 15ms/step - loss: 125.9403 - dense_19_loss: 39.4129 - dense_20_loss: 6.6834\nEpoch 145/500\n448/448 [==============================] - 7s 16ms/step - loss: 123.7726 - dense_19_loss: 37.8351 - dense_20_loss: 6.4853\nEpoch 146/500\n448/448 [==============================] - 7s 15ms/step - loss: 121.8058 - dense_19_loss: 36.2035 - dense_20_loss: 6.4342\nEpoch 147/500\n448/448 [==============================] - 7s 15ms/step - loss: 121.2817 - dense_19_loss: 36.2663 - dense_20_loss: 6.1812\nEpoch 148/500\n448/448 [==============================] - 7s 16ms/step - loss: 121.8416 - dense_19_loss: 36.9775 - dense_20_loss: 6.2567\nEpoch 149/500\n448/448 [==============================] - 7s 15ms/step - loss: 126.8674 - dense_19_loss: 41.7968 - dense_20_loss: 6.5829\nEpoch 150/500\n448/448 [==============================] - 7s 15ms/step - loss: 124.7803 - dense_19_loss: 40.0445 - dense_20_loss: 6.4048\nEpoch 151/500\n448/448 [==============================] - 7s 16ms/step - loss: 118.1807 - dense_19_loss: 34.2046 - dense_20_loss: 6.0547\nEpoch 152/500\n448/448 [==============================] - 7s 16ms/step - loss: 119.9993 - dense_19_loss: 36.0561 - dense_20_loss: 6.2369\nEpoch 153/500\n448/448 [==============================] - 7s 15ms/step - loss: 120.0354 - dense_19_loss: 36.4611 - dense_20_loss: 6.1113\nEpoch 154/500\n448/448 [==============================] - 7s 16ms/step - loss: 122.7284 - dense_19_loss: 39.2181 - dense_20_loss: 6.2796\nEpoch 155/500\n448/448 [==============================] - 7s 15ms/step - loss: 118.7732 - dense_19_loss: 35.4829 - dense_20_loss: 6.2665\nEpoch 156/500\n448/448 [==============================] - 7s 16ms/step - loss: 116.7160 - dense_19_loss: 34.0057 - dense_20_loss: 6.0900\nEpoch 157/500\n448/448 [==============================] - 7s 16ms/step - loss: 114.1884 - dense_19_loss: 32.0146 - dense_20_loss: 5.8293\nEpoch 158/500\n448/448 [==============================] - 7s 15ms/step - loss: 116.9832 - dense_19_loss: 35.1060 - dense_20_loss: 5.8176\nEpoch 159/500\n448/448 [==============================] - 7s 16ms/step - loss: 118.9325 - dense_19_loss: 36.9379 - dense_20_loss: 6.1047\nEpoch 160/500\n448/448 [==============================] - 7s 15ms/step - loss: 116.3324 - dense_19_loss: 34.8363 - dense_20_loss: 5.8843\nEpoch 161/500\n448/448 [==============================] - 7s 17ms/step - loss: 116.6737 - dense_19_loss: 35.2046 - dense_20_loss: 6.0021\nEpoch 162/500\n448/448 [==============================] - 7s 16ms/step - loss: 116.6175 - dense_19_loss: 35.6387 - dense_20_loss: 5.7845\nEpoch 163/500\n448/448 [==============================] - 7s 15ms/step - loss: 113.2883 - dense_19_loss: 32.4255 - dense_20_loss: 5.7767\nEpoch 164/500\n448/448 [==============================] - 7s 15ms/step - loss: 110.0972 - dense_19_loss: 29.9508 - dense_20_loss: 5.5542\nEpoch 165/500\n448/448 [==============================] - 7s 15ms/step - loss: 111.4799 - dense_19_loss: 31.4671 - dense_20_loss: 5.6297\nEpoch 166/500\n448/448 [==============================] - 7s 15ms/step - loss: 110.9734 - dense_19_loss: 31.2923 - dense_20_loss: 5.5933\nEpoch 167/500\n448/448 [==============================] - 7s 16ms/step - loss: 109.0025 - dense_19_loss: 29.9488 - dense_20_loss: 5.3825\nEpoch 168/500\n448/448 [==============================] - 7s 15ms/step - loss: 108.0410 - dense_19_loss: 29.2417 - dense_20_loss: 5.3636\nEpoch 169/500\n448/448 [==============================] - 8s 17ms/step - loss: 109.3584 - dense_19_loss: 30.7131 - dense_20_loss: 5.5089\nEpoch 170/500\n448/448 [==============================] - 7s 15ms/step - loss: 108.6087 - dense_19_loss: 30.1898 - dense_20_loss: 5.5019\nEpoch 171/500\n448/448 [==============================] - 7s 15ms/step - loss: 112.9503 - dense_19_loss: 34.5686 - dense_20_loss: 5.6606\nEpoch 172/500\n448/448 [==============================] - 7s 16ms/step - loss: 114.4807 - dense_19_loss: 35.8687 - dense_20_loss: 5.9053\nEpoch 173/500\n448/448 [==============================] - 7s 16ms/step - loss: 107.6861 - dense_19_loss: 29.9103 - dense_20_loss: 5.4147\nEpoch 174/500\n448/448 [==============================] - 7s 15ms/step - loss: 104.9300 - dense_19_loss: 27.7348 - dense_20_loss: 5.2354\nEpoch 175/500\n448/448 [==============================] - 7s 16ms/step - loss: 102.1592 - dense_19_loss: 25.4364 - dense_20_loss: 5.1383\nEpoch 176/500\n448/448 [==============================] - 7s 15ms/step - loss: 103.9892 - dense_19_loss: 27.5438 - dense_20_loss: 5.1225\nEpoch 177/500\n448/448 [==============================] - 7s 15ms/step - loss: 103.9752 - dense_19_loss: 27.8021 - dense_20_loss: 5.2000\nEpoch 178/500\n448/448 [==============================] - 7s 16ms/step - loss: 103.4752 - dense_19_loss: 27.5748 - dense_20_loss: 5.1682\nEpoch 179/500\n448/448 [==============================] - 7s 15ms/step - loss: 106.2927 - dense_19_loss: 30.3179 - dense_20_loss: 5.3722\nEpoch 180/500\n448/448 [==============================] - 7s 15ms/step - loss: 106.5750 - dense_19_loss: 30.7373 - dense_20_loss: 5.4415\nEpoch 181/500\n448/448 [==============================] - 7s 15ms/step - loss: 102.3851 - dense_19_loss: 27.1730 - dense_20_loss: 5.1577\nEpoch 182/500\n448/448 [==============================] - 7s 15ms/step - loss: 103.3687 - dense_19_loss: 28.5217 - dense_20_loss: 5.0701\nEpoch 183/500\n448/448 [==============================] - 7s 16ms/step - loss: 102.3908 - dense_19_loss: 27.6798 - dense_20_loss: 5.0948\nEpoch 184/500\n448/448 [==============================] - 7s 16ms/step - loss: 98.8898 - dense_19_loss: 24.7496 - dense_20_loss: 4.8968\nEpoch 185/500\n448/448 [==============================] - 7s 15ms/step - loss: 101.6447 - dense_19_loss: 27.7513 - dense_20_loss: 4.9651\nEpoch 186/500\n448/448 [==============================] - 7s 16ms/step - loss: 100.9039 - dense_19_loss: 26.9744 - dense_20_loss: 5.1239\nEpoch 187/500\n448/448 [==============================] - 7s 15ms/step - loss: 102.5300 - dense_19_loss: 28.8498 - dense_20_loss: 5.1544\nEpoch 188/500\n448/448 [==============================] - 7s 15ms/step - loss: 99.7640 - dense_19_loss: 26.4042 - dense_20_loss: 5.0723\nEpoch 189/500\n448/448 [==============================] - 7s 16ms/step - loss: 107.0592 - dense_19_loss: 33.3005 - dense_20_loss: 5.4172\nEpoch 190/500\n448/448 [==============================] - 7s 15ms/step - loss: 99.8941 - dense_19_loss: 26.7487 - dense_20_loss: 4.9967\nEpoch 191/500\n448/448 [==============================] - 7s 15ms/step - loss: 97.8529 - dense_19_loss: 25.4497 - dense_20_loss: 4.7218\nEpoch 192/500\n448/448 [==============================] - 7s 15ms/step - loss: 98.1593 - dense_19_loss: 25.9336 - dense_20_loss: 4.8175\nEpoch 193/500\n448/448 [==============================] - 7s 15ms/step - loss: 97.8662 - dense_19_loss: 25.6981 - dense_20_loss: 4.8952\nEpoch 194/500\n448/448 [==============================] - 7s 15ms/step - loss: 95.4653 - dense_19_loss: 23.9607 - dense_20_loss: 4.6137\nEpoch 195/500\n448/448 [==============================] - 7s 15ms/step - loss: 97.3631 - dense_19_loss: 25.9126 - dense_20_loss: 4.8111\nEpoch 196/500\n448/448 [==============================] - 7s 15ms/step - loss: 96.6096 - dense_19_loss: 25.4969 - dense_20_loss: 4.7267\nEpoch 197/500\n448/448 [==============================] - 7s 15ms/step - loss: 95.2645 - dense_19_loss: 24.4728 - dense_20_loss: 4.7037\nEpoch 198/500\n448/448 [==============================] - 7s 15ms/step - loss: 94.6660 - dense_19_loss: 24.2147 - dense_20_loss: 4.6169\nEpoch 199/500\n448/448 [==============================] - 7s 15ms/step - loss: 94.6711 - dense_19_loss: 24.3528 - dense_20_loss: 4.7865\nEpoch 200/500\n448/448 [==============================] - 7s 15ms/step - loss: 93.7250 - dense_19_loss: 23.9439 - dense_20_loss: 4.5363\nEpoch 201/500\n448/448 [==============================] - 7s 15ms/step - loss: 93.4107 - dense_19_loss: 23.7929 - dense_20_loss: 4.6403\nEpoch 202/500\n448/448 [==============================] - 7s 15ms/step - loss: 92.6601 - dense_19_loss: 23.4372 - dense_20_loss: 4.5497\nEpoch 203/500\n448/448 [==============================] - 7s 15ms/step - loss: 91.3041 - dense_19_loss: 22.5266 - dense_20_loss: 4.4333\nEpoch 204/500\n448/448 [==============================] - 7s 16ms/step - loss: 93.2894 - dense_19_loss: 24.5430 - dense_20_loss: 4.5834\nEpoch 205/500\n448/448 [==============================] - 7s 15ms/step - loss: 97.7061 - dense_19_loss: 28.5497 - dense_20_loss: 4.9871\nEpoch 206/500\n448/448 [==============================] - 7s 15ms/step - loss: 96.0082 - dense_19_loss: 26.9284 - dense_20_loss: 4.9876\nEpoch 207/500\n448/448 [==============================] - 7s 15ms/step - loss: 90.8947 - dense_19_loss: 22.8106 - dense_20_loss: 4.4254\nEpoch 208/500\n448/448 [==============================] - 7s 15ms/step - loss: 89.8747 - dense_19_loss: 22.2919 - dense_20_loss: 4.2746\nEpoch 209/500\n448/448 [==============================] - 7s 15ms/step - loss: 98.8422 - dense_19_loss: 30.7342 - dense_20_loss: 4.9004\nEpoch 210/500\n448/448 [==============================] - 7s 15ms/step - loss: 96.8799 - dense_19_loss: 27.9060 - dense_20_loss: 5.2429\nEpoch 211/500\n448/448 [==============================] - 7s 15ms/step - loss: 90.9861 - dense_19_loss: 23.3925 - dense_20_loss: 4.4304\nEpoch 212/500\n448/448 [==============================] - 7s 15ms/step - loss: 89.6280 - dense_19_loss: 22.5878 - dense_20_loss: 4.2547\nEpoch 213/500\n448/448 [==============================] - 7s 15ms/step - loss: 87.6999 - dense_19_loss: 21.0343 - dense_20_loss: 4.2076\nEpoch 214/500\n448/448 [==============================] - 7s 15ms/step - loss: 87.4456 - dense_19_loss: 21.1207 - dense_20_loss: 4.1986\nEpoch 215/500\n448/448 [==============================] - 7s 15ms/step - loss: 90.3206 - dense_19_loss: 23.8842 - dense_20_loss: 4.4204\nEpoch 216/500\n448/448 [==============================] - 7s 15ms/step - loss: 87.6960 - dense_19_loss: 21.5954 - dense_20_loss: 4.3607\nEpoch 217/500\n448/448 [==============================] - 7s 15ms/step - loss: 86.1012 - dense_19_loss: 20.5341 - dense_20_loss: 4.1393\nEpoch 218/500\n448/448 [==============================] - 7s 15ms/step - loss: 90.6654 - dense_19_loss: 24.8010 - dense_20_loss: 4.4962\nEpoch 219/500\n448/448 [==============================] - 7s 15ms/step - loss: 88.6314 - dense_19_loss: 22.8914 - dense_20_loss: 4.4216\nEpoch 220/500\n448/448 [==============================] - 7s 15ms/step - loss: 87.3140 - dense_19_loss: 22.3127 - dense_20_loss: 4.1367\nEpoch 221/500\n448/448 [==============================] - 7s 15ms/step - loss: 88.1299 - dense_19_loss: 23.0808 - dense_20_loss: 4.3002\nEpoch 222/500\n448/448 [==============================] - 7s 16ms/step - loss: 85.5357 - dense_19_loss: 20.8863 - dense_20_loss: 4.2016\nEpoch 223/500\n448/448 [==============================] - 7s 15ms/step - loss: 84.6934 - dense_19_loss: 20.4250 - dense_20_loss: 4.0677\nEpoch 224/500\n448/448 [==============================] - 7s 15ms/step - loss: 85.4273 - dense_19_loss: 21.3101 - dense_20_loss: 4.1697\nEpoch 225/500\n448/448 [==============================] - 7s 15ms/step - loss: 92.4892 - dense_19_loss: 27.6742 - dense_20_loss: 4.7718\nEpoch 226/500\n448/448 [==============================] - 7s 15ms/step - loss: 89.6115 - dense_19_loss: 24.8552 - dense_20_loss: 4.5203\nEpoch 227/500\n448/448 [==============================] - 7s 15ms/step - loss: 85.3842 - dense_19_loss: 21.5195 - dense_20_loss: 4.1446\nEpoch 228/500\n448/448 [==============================] - 7s 15ms/step - loss: 84.0824 - dense_19_loss: 20.7194 - dense_20_loss: 3.9722\nEpoch 229/500\n448/448 [==============================] - 7s 15ms/step - loss: 81.5310 - dense_19_loss: 18.6694 - dense_20_loss: 3.8543\nEpoch 230/500\n448/448 [==============================] - 7s 15ms/step - loss: 81.8867 - dense_19_loss: 19.3724 - dense_20_loss: 3.8247\nEpoch 231/500\n448/448 [==============================] - 7s 15ms/step - loss: 83.9918 - dense_19_loss: 21.3074 - dense_20_loss: 4.0854\nEpoch 232/500\n448/448 [==============================] - 7s 15ms/step - loss: 82.5893 - dense_19_loss: 20.2909 - dense_20_loss: 3.9711\nEpoch 233/500\n448/448 [==============================] - 7s 15ms/step - loss: 82.7941 - dense_19_loss: 20.6553 - dense_20_loss: 4.0344\nEpoch 234/500\n448/448 [==============================] - 7s 15ms/step - loss: 81.5352 - dense_19_loss: 19.7283 - dense_20_loss: 3.9372\nEpoch 235/500\n448/448 [==============================] - 7s 15ms/step - loss: 87.8671 - dense_19_loss: 25.0202 - dense_20_loss: 4.7048\nEpoch 236/500\n448/448 [==============================] - 7s 15ms/step - loss: 82.0665 - dense_19_loss: 20.3975 - dense_20_loss: 3.9666\nEpoch 237/500\n448/448 [==============================] - 7s 15ms/step - loss: 80.6173 - dense_19_loss: 19.4320 - dense_20_loss: 3.8011\nEpoch 238/500\n448/448 [==============================] - 7s 15ms/step - loss: 79.2645 - dense_19_loss: 18.4066 - dense_20_loss: 3.7692\nEpoch 239/500\n448/448 [==============================] - 7s 15ms/step - loss: 79.8188 - dense_19_loss: 19.1214 - dense_20_loss: 3.8776\nEpoch 240/500\n448/448 [==============================] - 7s 16ms/step - loss: 81.5007 - dense_19_loss: 20.7408 - dense_20_loss: 3.9876\nEpoch 241/500\n448/448 [==============================] - 7s 15ms/step - loss: 83.8626 - dense_19_loss: 22.8707 - dense_20_loss: 4.2062\nEpoch 242/500\n448/448 [==============================] - 7s 15ms/step - loss: 78.4799 - dense_19_loss: 18.3650 - dense_20_loss: 3.6945\nEpoch 243/500\n448/448 [==============================] - 7s 15ms/step - loss: 78.9264 - dense_19_loss: 19.0469 - dense_20_loss: 3.7709\nEpoch 244/500\n448/448 [==============================] - 7s 15ms/step - loss: 80.0194 - dense_19_loss: 20.3480 - dense_20_loss: 3.8209\nEpoch 245/500\n448/448 [==============================] - 7s 15ms/step - loss: 85.3568 - dense_19_loss: 24.9628 - dense_20_loss: 4.2962\nEpoch 246/500\n448/448 [==============================] - 7s 15ms/step - loss: 81.3332 - dense_19_loss: 21.5336 - dense_20_loss: 3.9686\nEpoch 247/500\n448/448 [==============================] - 7s 15ms/step - loss: 78.9245 - dense_19_loss: 19.4645 - dense_20_loss: 3.8778\nEpoch 248/500\n448/448 [==============================] - 7s 16ms/step - loss: 83.3200 - dense_19_loss: 23.5701 - dense_20_loss: 4.2493\nEpoch 249/500\n448/448 [==============================] - 7s 15ms/step - loss: 86.7669 - dense_19_loss: 26.1129 - dense_20_loss: 4.6421\nEpoch 250/500\n448/448 [==============================] - 7s 15ms/step - loss: 79.8119 - dense_19_loss: 20.2708 - dense_20_loss: 3.8956\nEpoch 251/500\n448/448 [==============================] - 7s 16ms/step - loss: 76.0470 - dense_19_loss: 17.2857 - dense_20_loss: 3.5857\nEpoch 252/500\n448/448 [==============================] - 7s 15ms/step - loss: 74.5508 - dense_19_loss: 16.3488 - dense_20_loss: 3.4204\nEpoch 253/500\n448/448 [==============================] - 7s 16ms/step - loss: 75.2567 - dense_19_loss: 17.2515 - dense_20_loss: 3.5065\nEpoch 254/500\n448/448 [==============================] - 7s 15ms/step - loss: 75.3656 - dense_19_loss: 17.5710 - dense_20_loss: 3.5488\nEpoch 255/500\n448/448 [==============================] - 7s 15ms/step - loss: 76.3566 - dense_19_loss: 18.6098 - dense_20_loss: 3.6926\nEpoch 256/500\n448/448 [==============================] - 7s 15ms/step - loss: 76.3329 - dense_19_loss: 18.7096 - dense_20_loss: 3.7263\nEpoch 257/500\n448/448 [==============================] - 7s 16ms/step - loss: 75.3857 - dense_19_loss: 18.0823 - dense_20_loss: 3.5883\nEpoch 258/500\n448/448 [==============================] - 7s 15ms/step - loss: 76.1104 - dense_19_loss: 18.9560 - dense_20_loss: 3.6729\nEpoch 259/500\n448/448 [==============================] - 7s 15ms/step - loss: 75.9123 - dense_19_loss: 18.8412 - dense_20_loss: 3.6953\nEpoch 260/500\n448/448 [==============================] - 7s 15ms/step - loss: 78.4991 - dense_19_loss: 21.0869 - dense_20_loss: 4.0027\nEpoch 261/500\n448/448 [==============================] - 7s 15ms/step - loss: 74.8001 - dense_19_loss: 17.9953 - dense_20_loss: 3.7145\nEpoch 262/500\n448/448 [==============================] - 7s 15ms/step - loss: 73.4193 - dense_19_loss: 17.2135 - dense_20_loss: 3.4173\nEpoch 263/500\n448/448 [==============================] - 7s 15ms/step - loss: 72.2546 - dense_19_loss: 16.4052 - dense_20_loss: 3.3517\nEpoch 264/500\n448/448 [==============================] - 7s 15ms/step - loss: 76.2080 - dense_19_loss: 19.8319 - dense_20_loss: 3.7967\nEpoch 265/500\n448/448 [==============================] - 7s 15ms/step - loss: 74.7666 - dense_19_loss: 18.7675 - dense_20_loss: 3.6496\nEpoch 266/500\n448/448 [==============================] - 7s 15ms/step - loss: 73.1734 - dense_19_loss: 17.5434 - dense_20_loss: 3.5695\nEpoch 267/500\n448/448 [==============================] - 7s 15ms/step - loss: 72.8713 - dense_19_loss: 17.5589 - dense_20_loss: 3.4702\nEpoch 268/500\n448/448 [==============================] - 7s 15ms/step - loss: 76.6176 - dense_19_loss: 20.9168 - dense_20_loss: 3.7807\nEpoch 269/500\n448/448 [==============================] - 7s 15ms/step - loss: 77.8931 - dense_19_loss: 21.8035 - dense_20_loss: 3.9610\nEpoch 270/500\n448/448 [==============================] - 7s 15ms/step - loss: 72.6529 - dense_19_loss: 17.5989 - dense_20_loss: 3.3578\nEpoch 271/500\n448/448 [==============================] - 7s 15ms/step - loss: 71.1112 - dense_19_loss: 16.3813 - dense_20_loss: 3.3569\nEpoch 272/500\n448/448 [==============================] - 7s 15ms/step - loss: 70.9892 - dense_19_loss: 16.4903 - dense_20_loss: 3.3521\nEpoch 273/500\n448/448 [==============================] - 7s 15ms/step - loss: 73.6238 - dense_19_loss: 18.9539 - dense_20_loss: 3.5425\nEpoch 274/500\n448/448 [==============================] - 7s 15ms/step - loss: 72.4408 - dense_19_loss: 17.9984 - dense_20_loss: 3.5003\nEpoch 275/500\n448/448 [==============================] - 7s 16ms/step - loss: 70.4046 - dense_19_loss: 16.3832 - dense_20_loss: 3.3432\nEpoch 276/500\n448/448 [==============================] - 7s 15ms/step - loss: 72.8461 - dense_19_loss: 18.5558 - dense_20_loss: 3.6589\nEpoch 277/500\n448/448 [==============================] - 7s 15ms/step - loss: 71.4777 - dense_19_loss: 17.5514 - dense_20_loss: 3.4588\nEpoch 278/500\n448/448 [==============================] - 7s 15ms/step - loss: 78.1038 - dense_19_loss: 23.2640 - dense_20_loss: 4.1140\nEpoch 279/500\n448/448 [==============================] - 7s 15ms/step - loss: 75.2435 - dense_19_loss: 20.7164 - dense_20_loss: 3.7845\nEpoch 280/500\n448/448 [==============================] - 7s 15ms/step - loss: 70.4330 - dense_19_loss: 16.7624 - dense_20_loss: 3.3102\nEpoch 281/500\n448/448 [==============================] - 7s 15ms/step - loss: 68.7917 - dense_19_loss: 15.6232 - dense_20_loss: 3.1843\nEpoch 282/500\n448/448 [==============================] - 7s 15ms/step - loss: 70.8613 - dense_19_loss: 17.6269 - dense_20_loss: 3.3975\nEpoch 283/500\n448/448 [==============================] - 7s 15ms/step - loss: 70.2512 - dense_19_loss: 17.0336 - dense_20_loss: 3.4643\nEpoch 284/500\n448/448 [==============================] - 7s 15ms/step - loss: 70.2657 - dense_19_loss: 17.3049 - dense_20_loss: 3.4065\nEpoch 285/500\n448/448 [==============================] - 7s 15ms/step - loss: 71.4920 - dense_19_loss: 18.3824 - dense_20_loss: 3.5510\nEpoch 286/500\n448/448 [==============================] - 7s 15ms/step - loss: 69.0524 - dense_19_loss: 16.4841 - dense_20_loss: 3.3086\nEpoch 287/500\n448/448 [==============================] - 7s 15ms/step - loss: 71.9988 - dense_19_loss: 19.0964 - dense_20_loss: 3.5771\nEpoch 288/500\n448/448 [==============================] - 7s 15ms/step - loss: 68.2430 - dense_19_loss: 16.0479 - dense_20_loss: 3.2083\nEpoch 289/500\n448/448 [==============================] - 7s 15ms/step - loss: 68.8551 - dense_19_loss: 16.7374 - dense_20_loss: 3.2917\nEpoch 290/500\n448/448 [==============================] - 7s 15ms/step - loss: 67.0922 - dense_19_loss: 15.4015 - dense_20_loss: 3.1151\nEpoch 291/500\n448/448 [==============================] - 7s 15ms/step - loss: 67.1032 - dense_19_loss: 15.5632 - dense_20_loss: 3.1701\nEpoch 292/500\n448/448 [==============================] - 7s 15ms/step - loss: 68.6429 - dense_19_loss: 17.2169 - dense_20_loss: 3.2028\nEpoch 293/500\n448/448 [==============================] - 7s 16ms/step - loss: 72.0295 - dense_19_loss: 19.9853 - dense_20_loss: 3.6253\nEpoch 294/500\n448/448 [==============================] - 7s 15ms/step - loss: 81.9164 - dense_19_loss: 28.0800 - dense_20_loss: 4.7678\nEpoch 295/500\n448/448 [==============================] - 7s 15ms/step - loss: 69.2496 - dense_19_loss: 17.2078 - dense_20_loss: 3.3728\nEpoch 296/500\n448/448 [==============================] - 7s 15ms/step - loss: 66.3114 - dense_19_loss: 14.9225 - dense_20_loss: 3.1329\nEpoch 297/500\n448/448 [==============================] - 7s 15ms/step - loss: 69.3227 - dense_19_loss: 17.9398 - dense_20_loss: 3.2959\nEpoch 298/500\n448/448 [==============================] - 7s 15ms/step - loss: 65.7337 - dense_19_loss: 14.7347 - dense_20_loss: 3.1100\nEpoch 299/500\n448/448 [==============================] - 7s 15ms/step - loss: 65.4195 - dense_19_loss: 14.7347 - dense_20_loss: 3.1181\nEpoch 300/500\n448/448 [==============================] - 7s 15ms/step - loss: 66.0376 - dense_19_loss: 15.4794 - dense_20_loss: 3.1389\nEpoch 301/500\n448/448 [==============================] - 7s 16ms/step - loss: 64.9915 - dense_19_loss: 14.7274 - dense_20_loss: 3.0867\nEpoch 302/500\n448/448 [==============================] - 7s 15ms/step - loss: 65.1621 - dense_19_loss: 15.0095 - dense_20_loss: 3.1765\nEpoch 303/500\n448/448 [==============================] - 7s 15ms/step - loss: 70.5246 - dense_19_loss: 19.9909 - dense_20_loss: 3.5362\nEpoch 304/500\n448/448 [==============================] - 7s 15ms/step - loss: 68.5636 - dense_19_loss: 17.9326 - dense_20_loss: 3.4940\nEpoch 305/500\n448/448 [==============================] - 7s 15ms/step - loss: 71.6680 - dense_19_loss: 20.9161 - dense_20_loss: 3.5348\nEpoch 306/500\n448/448 [==============================] - 7s 15ms/step - loss: 65.6116 - dense_19_loss: 15.4547 - dense_20_loss: 3.2113\nEpoch 307/500\n448/448 [==============================] - 7s 15ms/step - loss: 63.3655 - dense_19_loss: 13.9291 - dense_20_loss: 2.8527\nEpoch 308/500\n448/448 [==============================] - 7s 15ms/step - loss: 64.8682 - dense_19_loss: 15.5659 - dense_20_loss: 2.9724\nEpoch 309/500\n448/448 [==============================] - 7s 15ms/step - loss: 63.4707 - dense_19_loss: 14.2657 - dense_20_loss: 3.0543\nEpoch 310/500\n448/448 [==============================] - 7s 15ms/step - loss: 67.6659 - dense_19_loss: 18.0059 - dense_20_loss: 3.3503\nEpoch 311/500\n448/448 [==============================] - 7s 16ms/step - loss: 66.2676 - dense_19_loss: 17.0677 - dense_20_loss: 3.0916\nEpoch 312/500\n448/448 [==============================] - 7s 15ms/step - loss: 63.5175 - dense_19_loss: 14.7244 - dense_20_loss: 2.9944\nEpoch 313/500\n448/448 [==============================] - 7s 15ms/step - loss: 64.7969 - dense_19_loss: 16.0571 - dense_20_loss: 3.0837\nEpoch 314/500\n448/448 [==============================] - 7s 15ms/step - loss: 63.5990 - dense_19_loss: 15.0709 - dense_20_loss: 3.0214\nEpoch 315/500\n448/448 [==============================] - 7s 15ms/step - loss: 62.8162 - dense_19_loss: 14.5536 - dense_20_loss: 3.0071\nEpoch 316/500\n448/448 [==============================] - 7s 15ms/step - loss: 63.2604 - dense_19_loss: 15.1614 - dense_20_loss: 2.9661\nEpoch 317/500\n448/448 [==============================] - 7s 16ms/step - loss: 63.4122 - dense_19_loss: 15.3176 - dense_20_loss: 3.0502\nEpoch 318/500\n448/448 [==============================] - 7s 15ms/step - loss: 63.0597 - dense_19_loss: 15.2130 - dense_20_loss: 3.0260\nEpoch 319/500\n448/448 [==============================] - 7s 15ms/step - loss: 65.1740 - dense_19_loss: 17.0505 - dense_20_loss: 3.2865\nEpoch 320/500\n448/448 [==============================] - 7s 15ms/step - loss: 64.3755 - dense_19_loss: 16.4884 - dense_20_loss: 3.1511\nEpoch 321/500\n448/448 [==============================] - 7s 15ms/step - loss: 64.0248 - dense_19_loss: 16.2507 - dense_20_loss: 3.1390\nEpoch 322/500\n448/448 [==============================] - 7s 15ms/step - loss: 66.8339 - dense_19_loss: 18.6711 - dense_20_loss: 3.3400\nEpoch 323/500\n448/448 [==============================] - 7s 15ms/step - loss: 62.7213 - dense_19_loss: 15.2467 - dense_20_loss: 3.0291\nEpoch 324/500\n448/448 [==============================] - 7s 15ms/step - loss: 60.6237 - dense_19_loss: 13.6032 - dense_20_loss: 2.8641\nEpoch 325/500\n448/448 [==============================] - 7s 15ms/step - loss: 61.1520 - dense_19_loss: 14.3736 - dense_20_loss: 2.8172\nEpoch 326/500\n448/448 [==============================] - 7s 15ms/step - loss: 65.6632 - dense_19_loss: 18.0406 - dense_20_loss: 3.3632\nEpoch 327/500\n448/448 [==============================] - 7s 15ms/step - loss: 61.9656 - dense_19_loss: 15.0829 - dense_20_loss: 2.9075\nEpoch 328/500\n448/448 [==============================] - 7s 15ms/step - loss: 67.8107 - dense_19_loss: 20.4559 - dense_20_loss: 3.3795\nEpoch 329/500\n448/448 [==============================] - 7s 15ms/step - loss: 64.5992 - dense_19_loss: 16.7753 - dense_20_loss: 3.3870\nEpoch 330/500\n448/448 [==============================] - 7s 15ms/step - loss: 59.8307 - dense_19_loss: 13.1832 - dense_20_loss: 2.7724\nEpoch 331/500\n448/448 [==============================] - 7s 15ms/step - loss: 60.2924 - dense_19_loss: 13.9966 - dense_20_loss: 2.7549\nEpoch 332/500\n448/448 [==============================] - 7s 15ms/step - loss: 59.4970 - dense_19_loss: 13.3946 - dense_20_loss: 2.8059\nEpoch 333/500\n448/448 [==============================] - 7s 15ms/step - loss: 59.6002 - dense_19_loss: 13.6916 - dense_20_loss: 2.7883\nEpoch 334/500\n448/448 [==============================] - 7s 16ms/step - loss: 60.1182 - dense_19_loss: 14.2480 - dense_20_loss: 2.8909\nEpoch 335/500\n448/448 [==============================] - 7s 15ms/step - loss: 60.8670 - dense_19_loss: 15.0120 - dense_20_loss: 2.9857\nEpoch 336/500\n448/448 [==============================] - 7s 15ms/step - loss: 60.7531 - dense_19_loss: 15.0069 - dense_20_loss: 2.9640\nEpoch 337/500\n448/448 [==============================] - 7s 15ms/step - loss: 59.6340 - dense_19_loss: 14.2353 - dense_20_loss: 2.7942\nEpoch 338/500\n448/448 [==============================] - 7s 15ms/step - loss: 59.5795 - dense_19_loss: 14.2545 - dense_20_loss: 2.8503\nEpoch 339/500\n448/448 [==============================] - 7s 15ms/step - loss: 60.5502 - dense_19_loss: 15.1433 - dense_20_loss: 2.9492\nEpoch 340/500\n448/448 [==============================] - 7s 15ms/step - loss: 60.2499 - dense_19_loss: 15.0194 - dense_20_loss: 2.9474\nEpoch 341/500\n448/448 [==============================] - 7s 15ms/step - loss: 60.5894 - dense_19_loss: 15.3373 - dense_20_loss: 2.9853\nEpoch 342/500\n448/448 [==============================] - 7s 15ms/step - loss: 59.4258 - dense_19_loss: 14.4351 - dense_20_loss: 2.9222\nEpoch 343/500\n448/448 [==============================] - 7s 15ms/step - loss: 58.4061 - dense_19_loss: 13.7668 - dense_20_loss: 2.7719\nEpoch 344/500\n448/448 [==============================] - 7s 15ms/step - loss: 58.8671 - dense_19_loss: 14.1417 - dense_20_loss: 2.8569\nEpoch 345/500\n448/448 [==============================] - 7s 15ms/step - loss: 59.3999 - dense_19_loss: 14.7148 - dense_20_loss: 2.9068\nEpoch 346/500\n448/448 [==============================] - 7s 16ms/step - loss: 58.8158 - dense_19_loss: 14.4237 - dense_20_loss: 2.7983\nEpoch 347/500\n448/448 [==============================] - 7s 15ms/step - loss: 57.0712 - dense_19_loss: 13.0438 - dense_20_loss: 2.6842\nEpoch 348/500\n448/448 [==============================] - 7s 15ms/step - loss: 57.0935 - dense_19_loss: 13.2845 - dense_20_loss: 2.6666\nEpoch 349/500\n448/448 [==============================] - 7s 15ms/step - loss: 58.6127 - dense_19_loss: 14.6284 - dense_20_loss: 2.9099\nEpoch 350/500\n448/448 [==============================] - 7s 16ms/step - loss: 57.7947 - dense_19_loss: 13.9552 - dense_20_loss: 2.8359\nEpoch 351/500\n448/448 [==============================] - 7s 15ms/step - loss: 59.9427 - dense_19_loss: 15.5510 - dense_20_loss: 3.1115\nEpoch 352/500\n448/448 [==============================] - 7s 15ms/step - loss: 57.2892 - dense_19_loss: 13.6640 - dense_20_loss: 2.7382\nEpoch 353/500\n448/448 [==============================] - 7s 15ms/step - loss: 60.4138 - dense_19_loss: 16.3728 - dense_20_loss: 3.1190\nEpoch 354/500\n448/448 [==============================] - 7s 15ms/step - loss: 58.0646 - dense_19_loss: 14.4824 - dense_20_loss: 2.8213\nEpoch 355/500\n448/448 [==============================] - 7s 16ms/step - loss: 56.3348 - dense_19_loss: 13.2433 - dense_20_loss: 2.6002\nEpoch 356/500\n448/448 [==============================] - 7s 15ms/step - loss: 56.0578 - dense_19_loss: 13.1474 - dense_20_loss: 2.6082\nEpoch 357/500\n448/448 [==============================] - 7s 15ms/step - loss: 59.3054 - dense_19_loss: 16.2354 - dense_20_loss: 2.7912\nEpoch 358/500\n448/448 [==============================] - 7s 15ms/step - loss: 57.7110 - dense_19_loss: 14.5019 - dense_20_loss: 2.8558\nEpoch 359/500\n448/448 [==============================] - 7s 15ms/step - loss: 56.6885 - dense_19_loss: 13.8088 - dense_20_loss: 2.7205\nEpoch 360/500\n448/448 [==============================] - 7s 16ms/step - loss: 55.9642 - dense_19_loss: 13.2980 - dense_20_loss: 2.6871\nEpoch 361/500\n448/448 [==============================] - 7s 15ms/step - loss: 55.8516 - dense_19_loss: 13.3208 - dense_20_loss: 2.6627\nEpoch 362/500\n448/448 [==============================] - 7s 15ms/step - loss: 58.7741 - dense_19_loss: 15.7849 - dense_20_loss: 2.9532\nEpoch 363/500\n448/448 [==============================] - 7s 15ms/step - loss: 60.2640 - dense_19_loss: 17.0761 - dense_20_loss: 3.0999\nEpoch 364/500\n448/448 [==============================] - 7s 16ms/step - loss: 73.0740 - dense_19_loss: 27.6026 - dense_20_loss: 4.4347\nEpoch 365/500\n448/448 [==============================] - 7s 15ms/step - loss: 58.3442 - dense_19_loss: 14.6568 - dense_20_loss: 2.9530\nEpoch 366/500\n448/448 [==============================] - 7s 15ms/step - loss: 55.1896 - dense_19_loss: 12.3409 - dense_20_loss: 2.5890\nEpoch 367/500\n448/448 [==============================] - 7s 15ms/step - loss: 53.8935 - dense_19_loss: 11.5489 - dense_20_loss: 2.4472\nEpoch 368/500\n448/448 [==============================] - 7s 15ms/step - loss: 53.8326 - dense_19_loss: 11.7371 - dense_20_loss: 2.4836\nEpoch 369/500\n448/448 [==============================] - 7s 16ms/step - loss: 60.6503 - dense_19_loss: 17.4623 - dense_20_loss: 3.1594\nEpoch 370/500\n448/448 [==============================] - 7s 15ms/step - loss: 60.4411 - dense_19_loss: 17.3817 - dense_20_loss: 3.0062\nEpoch 371/500\n448/448 [==============================] - 7s 15ms/step - loss: 54.1063 - dense_19_loss: 11.8849 - dense_20_loss: 2.5404\nEpoch 372/500\n448/448 [==============================] - 7s 15ms/step - loss: 55.8156 - dense_19_loss: 13.7350 - dense_20_loss: 2.6406\nEpoch 373/500\n448/448 [==============================] - 7s 15ms/step - loss: 58.8582 - dense_19_loss: 16.1760 - dense_20_loss: 2.9857\nEpoch 374/500\n448/448 [==============================] - 7s 15ms/step - loss: 56.6635 - dense_19_loss: 14.5669 - dense_20_loss: 2.6571\nEpoch 375/500\n448/448 [==============================] - 7s 15ms/step - loss: 55.1864 - dense_19_loss: 13.3056 - dense_20_loss: 2.6338\nEpoch 376/500\n448/448 [==============================] - 7s 15ms/step - loss: 54.9418 - dense_19_loss: 13.2445 - dense_20_loss: 2.6069\nEpoch 377/500\n448/448 [==============================] - 7s 15ms/step - loss: 54.0683 - dense_19_loss: 12.5484 - dense_20_loss: 2.5752\nEpoch 378/500\n448/448 [==============================] - 8s 17ms/step - loss: 53.3559 - dense_19_loss: 12.2070 - dense_20_loss: 2.4515\nEpoch 379/500\n448/448 [==============================] - 7s 15ms/step - loss: 53.8947 - dense_19_loss: 12.8122 - dense_20_loss: 2.5098\nEpoch 380/500\n448/448 [==============================] - 7s 15ms/step - loss: 53.3942 - dense_19_loss: 12.5040 - dense_20_loss: 2.4982\nEpoch 381/500\n448/448 [==============================] - 7s 16ms/step - loss: 53.1605 - dense_19_loss: 12.3687 - dense_20_loss: 2.5329\nEpoch 382/500\n448/448 [==============================] - 7s 16ms/step - loss: 54.1730 - dense_19_loss: 13.4271 - dense_20_loss: 2.5637\nEpoch 383/500\n448/448 [==============================] - 7s 15ms/step - loss: 54.0481 - dense_19_loss: 13.3730 - dense_20_loss: 2.5979\nEpoch 384/500\n448/448 [==============================] - 7s 16ms/step - loss: 57.1607 - dense_19_loss: 15.9435 - dense_20_loss: 2.8920\nEpoch 385/500\n448/448 [==============================] - 7s 16ms/step - loss: 55.7473 - dense_19_loss: 14.5195 - dense_20_loss: 2.8758\nEpoch 386/500\n448/448 [==============================] - 7s 15ms/step - loss: 53.9760 - dense_19_loss: 13.1868 - dense_20_loss: 2.6285\nEpoch 387/500\n448/448 [==============================] - 8s 17ms/step - loss: 52.6142 - dense_19_loss: 12.3038 - dense_20_loss: 2.4573\nEpoch 388/500\n448/448 [==============================] - 7s 16ms/step - loss: 51.6388 - dense_19_loss: 11.6666 - dense_20_loss: 2.3677\nEpoch 389/500\n448/448 [==============================] - 7s 15ms/step - loss: 52.5574 - dense_19_loss: 12.5472 - dense_20_loss: 2.5424\nEpoch 390/500\n448/448 [==============================] - 7s 16ms/step - loss: 52.7608 - dense_19_loss: 12.8534 - dense_20_loss: 2.5047\nEpoch 391/500\n448/448 [==============================] - 7s 16ms/step - loss: 51.4680 - dense_19_loss: 11.7742 - dense_20_loss: 2.4653\nEpoch 392/500\n448/448 [==============================] - 7s 15ms/step - loss: 51.8740 - dense_19_loss: 12.2915 - dense_20_loss: 2.4745\nEpoch 393/500\n448/448 [==============================] - 7s 16ms/step - loss: 52.0733 - dense_19_loss: 12.6184 - dense_20_loss: 2.4879\nEpoch 394/500\n448/448 [==============================] - 7s 15ms/step - loss: 51.9689 - dense_19_loss: 12.5556 - dense_20_loss: 2.5468\nEpoch 395/500\n448/448 [==============================] - 8s 17ms/step - loss: 58.4824 - dense_19_loss: 18.3724 - dense_20_loss: 3.0150\nEpoch 396/500\n448/448 [==============================] - 7s 16ms/step - loss: 60.1848 - dense_19_loss: 19.2099 - dense_20_loss: 3.2578\nEpoch 397/500\n448/448 [==============================] - 7s 15ms/step - loss: 53.0223 - dense_19_loss: 13.1700 - dense_20_loss: 2.5330\nEpoch 398/500\n448/448 [==============================] - 7s 16ms/step - loss: 50.6158 - dense_19_loss: 11.3398 - dense_20_loss: 2.3251\nEpoch 399/500\n448/448 [==============================] - 7s 17ms/step - loss: 51.6563 - dense_19_loss: 12.2418 - dense_20_loss: 2.5245\nEpoch 400/500\n448/448 [==============================] - 7s 16ms/step - loss: 50.7444 - dense_19_loss: 11.7384 - dense_20_loss: 2.3659\nEpoch 401/500\n448/448 [==============================] - 7s 16ms/step - loss: 50.3474 - dense_19_loss: 11.5933 - dense_20_loss: 2.2903\nEpoch 402/500\n448/448 [==============================] - 7s 16ms/step - loss: 50.2695 - dense_19_loss: 11.5731 - dense_20_loss: 2.3979\nEpoch 403/500\n448/448 [==============================] - 7s 16ms/step - loss: 50.3041 - dense_19_loss: 11.7489 - dense_20_loss: 2.3998\nEpoch 404/500\n448/448 [==============================] - 7s 16ms/step - loss: 50.7005 - dense_19_loss: 12.1578 - dense_20_loss: 2.4483\nEpoch 405/500\n448/448 [==============================] - 7s 16ms/step - loss: 50.7112 - dense_19_loss: 12.3217 - dense_20_loss: 2.4505\nEpoch 406/500\n448/448 [==============================] - 7s 15ms/step - loss: 51.5347 - dense_19_loss: 13.1575 - dense_20_loss: 2.4312\nEpoch 407/500\n448/448 [==============================] - 7s 16ms/step - loss: 50.1120 - dense_19_loss: 11.8567 - dense_20_loss: 2.4667\nEpoch 408/500\n448/448 [==============================] - 7s 16ms/step - loss: 52.2320 - dense_19_loss: 13.7483 - dense_20_loss: 2.6569\nEpoch 409/500\n448/448 [==============================] - 7s 15ms/step - loss: 50.8245 - dense_19_loss: 12.5815 - dense_20_loss: 2.4824\nEpoch 410/500\n448/448 [==============================] - 7s 16ms/step - loss: 49.2405 - dense_19_loss: 11.3498 - dense_20_loss: 2.3530\nEpoch 411/500\n448/448 [==============================] - 7s 15ms/step - loss: 48.9536 - dense_19_loss: 11.1864 - dense_20_loss: 2.3996\nEpoch 412/500\n448/448 [==============================] - 7s 16ms/step - loss: 49.9689 - dense_19_loss: 12.2859 - dense_20_loss: 2.4206\nEpoch 413/500\n448/448 [==============================] - 7s 16ms/step - loss: 51.1664 - dense_19_loss: 13.3701 - dense_20_loss: 2.5039\nEpoch 414/500\n448/448 [==============================] - 7s 15ms/step - loss: 50.4871 - dense_19_loss: 12.8164 - dense_20_loss: 2.4732\nEpoch 415/500\n448/448 [==============================] - 7s 16ms/step - loss: 50.7664 - dense_19_loss: 13.1661 - dense_20_loss: 2.4138\nEpoch 416/500\n448/448 [==============================] - 7s 15ms/step - loss: 48.9512 - dense_19_loss: 11.6103 - dense_20_loss: 2.3610\nEpoch 417/500\n448/448 [==============================] - 7s 15ms/step - loss: 49.1863 - dense_19_loss: 11.9497 - dense_20_loss: 2.3711\nEpoch 418/500\n448/448 [==============================] - 7s 15ms/step - loss: 50.7856 - dense_19_loss: 13.3644 - dense_20_loss: 2.5205\nEpoch 419/500\n448/448 [==============================] - 7s 16ms/step - loss: 54.4416 - dense_19_loss: 16.1985 - dense_20_loss: 2.8771\nEpoch 420/500\n448/448 [==============================] - 7s 16ms/step - loss: 48.8230 - dense_19_loss: 11.4792 - dense_20_loss: 2.3885\nEpoch 421/500\n448/448 [==============================] - 7s 15ms/step - loss: 47.7727 - dense_19_loss: 10.8707 - dense_20_loss: 2.2459\nEpoch 422/500\n448/448 [==============================] - 7s 15ms/step - loss: 47.7259 - dense_19_loss: 10.9336 - dense_20_loss: 2.2687\nEpoch 423/500\n448/448 [==============================] - 7s 15ms/step - loss: 47.7303 - dense_19_loss: 11.0805 - dense_20_loss: 2.3066\nEpoch 424/500\n448/448 [==============================] - 8s 17ms/step - loss: 48.6235 - dense_19_loss: 12.0886 - dense_20_loss: 2.2883\nEpoch 425/500\n448/448 [==============================] - 7s 15ms/step - loss: 48.4129 - dense_19_loss: 11.9431 - dense_20_loss: 2.3217\nEpoch 426/500\n448/448 [==============================] - 7s 15ms/step - loss: 48.8368 - dense_19_loss: 12.3250 - dense_20_loss: 2.4005\nEpoch 427/500\n448/448 [==============================] - 7s 16ms/step - loss: 49.6898 - dense_19_loss: 13.0515 - dense_20_loss: 2.5301\nEpoch 428/500\n448/448 [==============================] - 7s 15ms/step - loss: 48.6912 - dense_19_loss: 12.2091 - dense_20_loss: 2.4105\nEpoch 429/500\n448/448 [==============================] - 7s 16ms/step - loss: 49.6793 - dense_19_loss: 12.8445 - dense_20_loss: 2.5241\nEpoch 430/500\n448/448 [==============================] - 7s 15ms/step - loss: 65.8461 - dense_19_loss: 27.4798 - dense_20_loss: 3.5772\nEpoch 431/500\n448/448 [==============================] - 7s 15ms/step - loss: 53.6654 - dense_19_loss: 15.9067 - dense_20_loss: 2.7730\nEpoch 432/500\n448/448 [==============================] - 7s 15ms/step - loss: 49.6556 - dense_19_loss: 12.6229 - dense_20_loss: 2.3675\nEpoch 433/500\n448/448 [==============================] - 7s 16ms/step - loss: 46.8185 - dense_19_loss: 10.4313 - dense_20_loss: 2.1190\nEpoch 434/500\n448/448 [==============================] - 7s 16ms/step - loss: 46.5547 - dense_19_loss: 10.3091 - dense_20_loss: 2.1967\nEpoch 435/500\n448/448 [==============================] - 7s 15ms/step - loss: 46.2418 - dense_19_loss: 10.2780 - dense_20_loss: 2.1588\nEpoch 436/500\n448/448 [==============================] - 7s 15ms/step - loss: 46.2879 - dense_19_loss: 10.4400 - dense_20_loss: 2.1998\nEpoch 437/500\n448/448 [==============================] - 7s 15ms/step - loss: 45.6683 - dense_19_loss: 10.0991 - dense_20_loss: 2.1587\nEpoch 438/500\n448/448 [==============================] - 7s 16ms/step - loss: 47.4065 - dense_19_loss: 11.7217 - dense_20_loss: 2.2887\nEpoch 439/500\n448/448 [==============================] - 7s 15ms/step - loss: 47.8013 - dense_19_loss: 12.0134 - dense_20_loss: 2.3726\nEpoch 440/500\n448/448 [==============================] - 7s 15ms/step - loss: 47.9274 - dense_19_loss: 12.3001 - dense_20_loss: 2.3112\nEpoch 441/500\n448/448 [==============================] - 7s 15ms/step - loss: 51.0406 - dense_19_loss: 14.9532 - dense_20_loss: 2.6892\nEpoch 442/500\n448/448 [==============================] - 7s 16ms/step - loss: 73.0924 - dense_19_loss: 33.1206 - dense_20_loss: 4.7558\nEpoch 443/500\n448/448 [==============================] - 7s 16ms/step - loss: 49.4167 - dense_19_loss: 12.3132 - dense_20_loss: 2.4093\nEpoch 444/500\n448/448 [==============================] - 7s 15ms/step - loss: 47.0954 - dense_19_loss: 10.7204 - dense_20_loss: 2.1274\nEpoch 445/500\n448/448 [==============================] - 7s 15ms/step - loss: 45.9767 - dense_19_loss: 10.0302 - dense_20_loss: 2.0539\nEpoch 446/500\n448/448 [==============================] - 7s 16ms/step - loss: 44.8551 - dense_19_loss: 9.3153 - dense_20_loss: 1.9881\nEpoch 447/500\n448/448 [==============================] - 7s 16ms/step - loss: 45.3549 - dense_19_loss: 9.9990 - dense_20_loss: 2.0767\nEpoch 448/500\n448/448 [==============================] - 7s 15ms/step - loss: 45.9784 - dense_19_loss: 10.7576 - dense_20_loss: 2.0838\nEpoch 449/500\n448/448 [==============================] - 7s 15ms/step - loss: 46.9644 - dense_19_loss: 11.6323 - dense_20_loss: 2.2466\nEpoch 450/500\n448/448 [==============================] - 7s 16ms/step - loss: 52.7860 - dense_19_loss: 16.6118 - dense_20_loss: 2.7691\nEpoch 451/500\n448/448 [==============================] - 8s 17ms/step - loss: 49.7886 - dense_19_loss: 13.8360 - dense_20_loss: 2.4735\nEpoch 452/500\n448/448 [==============================] - 7s 15ms/step - loss: 47.2482 - dense_19_loss: 11.7610 - dense_20_loss: 2.2013\nEpoch 453/500\n448/448 [==============================] - 7s 16ms/step - loss: 47.6822 - dense_19_loss: 12.2586 - dense_20_loss: 2.2795\nEpoch 454/500\n448/448 [==============================] - 7s 16ms/step - loss: 46.2969 - dense_19_loss: 11.2152 - dense_20_loss: 2.1360\nEpoch 455/500\n448/448 [==============================] - 7s 15ms/step - loss: 45.9107 - dense_19_loss: 10.9939 - dense_20_loss: 2.1628\nEpoch 456/500\n448/448 [==============================] - 7s 16ms/step - loss: 45.7744 - dense_19_loss: 11.0076 - dense_20_loss: 2.1440\nEpoch 457/500\n448/448 [==============================] - 7s 16ms/step - loss: 47.0431 - dense_19_loss: 12.0640 - dense_20_loss: 2.3645\nEpoch 458/500\n448/448 [==============================] - 7s 15ms/step - loss: 50.4006 - dense_19_loss: 15.0274 - dense_20_loss: 2.5290\nEpoch 459/500\n448/448 [==============================] - 7s 15ms/step - loss: 45.4032 - dense_19_loss: 10.7430 - dense_20_loss: 2.1283\nEpoch 460/500\n448/448 [==============================] - 7s 15ms/step - loss: 45.1865 - dense_19_loss: 10.7684 - dense_20_loss: 2.0967\nEpoch 461/500\n448/448 [==============================] - 7s 16ms/step - loss: 44.6495 - dense_19_loss: 10.3602 - dense_20_loss: 2.1069\nEpoch 462/500\n448/448 [==============================] - 7s 15ms/step - loss: 45.5651 - dense_19_loss: 11.3331 - dense_20_loss: 2.1593\nEpoch 463/500\n448/448 [==============================] - 7s 15ms/step - loss: 44.4412 - dense_19_loss: 10.4260 - dense_20_loss: 2.1043\nEpoch 464/500\n448/448 [==============================] - 7s 15ms/step - loss: 45.4275 - dense_19_loss: 11.4190 - dense_20_loss: 2.1746\nEpoch 465/500\n448/448 [==============================] - 7s 16ms/step - loss: 45.3397 - dense_19_loss: 11.3954 - dense_20_loss: 2.1604\nEpoch 466/500\n448/448 [==============================] - 7s 15ms/step - loss: 45.6668 - dense_19_loss: 11.7481 - dense_20_loss: 2.2073\nEpoch 467/500\n448/448 [==============================] - 7s 15ms/step - loss: 44.6626 - dense_19_loss: 10.9153 - dense_20_loss: 2.1280\nEpoch 468/500\n448/448 [==============================] - 7s 15ms/step - loss: 44.2626 - dense_19_loss: 10.6888 - dense_20_loss: 2.0997\nEpoch 469/500\n448/448 [==============================] - 7s 15ms/step - loss: 44.1670 - dense_19_loss: 10.7476 - dense_20_loss: 2.1066\nEpoch 470/500\n448/448 [==============================] - 7s 16ms/step - loss: 46.6355 - dense_19_loss: 12.7856 - dense_20_loss: 2.4209\nEpoch 471/500\n448/448 [==============================] - 7s 15ms/step - loss: 45.4001 - dense_19_loss: 11.7873 - dense_20_loss: 2.2037\nEpoch 472/500\n448/448 [==============================] - 7s 15ms/step - loss: 44.0479 - dense_19_loss: 10.7439 - dense_20_loss: 2.0996\nEpoch 473/500\n448/448 [==============================] - 7s 15ms/step - loss: 43.9108 - dense_19_loss: 10.6178 - dense_20_loss: 2.1542\nEpoch 474/500\n448/448 [==============================] - 7s 16ms/step - loss: 43.3443 - dense_19_loss: 10.3133 - dense_20_loss: 2.0839\nEpoch 475/500\n448/448 [==============================] - 7s 15ms/step - loss: 43.5171 - dense_19_loss: 10.5737 - dense_20_loss: 2.0640\nEpoch 476/500\n448/448 [==============================] - 7s 15ms/step - loss: 45.6794 - dense_19_loss: 12.4500 - dense_20_loss: 2.3095\nEpoch 477/500\n448/448 [==============================] - 7s 15ms/step - loss: 45.3310 - dense_19_loss: 12.0747 - dense_20_loss: 2.2607\nEpoch 478/500\n448/448 [==============================] - 7s 16ms/step - loss: 45.8933 - dense_19_loss: 12.5889 - dense_20_loss: 2.2828\nEpoch 479/500\n448/448 [==============================] - 7s 16ms/step - loss: 47.6844 - dense_19_loss: 14.0404 - dense_20_loss: 2.4936\nEpoch 480/500\n448/448 [==============================] - 7s 16ms/step - loss: 47.4202 - dense_19_loss: 13.5306 - dense_20_loss: 2.4493\nEpoch 481/500\n448/448 [==============================] - 7s 16ms/step - loss: 43.7266 - dense_19_loss: 10.5770 - dense_20_loss: 2.0605\nEpoch 482/500\n448/448 [==============================] - 7s 15ms/step - loss: 43.5553 - dense_19_loss: 10.6044 - dense_20_loss: 2.0736\nEpoch 483/500\n448/448 [==============================] - 7s 16ms/step - loss: 42.5335 - dense_19_loss: 9.9099 - dense_20_loss: 1.9595\nEpoch 484/500\n448/448 [==============================] - 7s 15ms/step - loss: 43.0204 - dense_19_loss: 10.3855 - dense_20_loss: 2.0870\nEpoch 485/500\n448/448 [==============================] - 7s 15ms/step - loss: 43.4969 - dense_19_loss: 10.8565 - dense_20_loss: 2.1256\nEpoch 486/500\n448/448 [==============================] - 7s 15ms/step - loss: 59.9385 - dense_19_loss: 24.4344 - dense_20_loss: 3.8585\nEpoch 487/500\n448/448 [==============================] - 7s 15ms/step - loss: 54.8008 - dense_19_loss: 19.9774 - dense_20_loss: 2.9567\nEpoch 488/500\n448/448 [==============================] - 7s 16ms/step - loss: 45.3914 - dense_19_loss: 11.6872 - dense_20_loss: 2.1782\nEpoch 489/500\n448/448 [==============================] - 7s 15ms/step - loss: 43.9863 - dense_19_loss: 10.6582 - dense_20_loss: 2.0445\nEpoch 490/500\n448/448 [==============================] - 7s 15ms/step - loss: 43.5018 - dense_19_loss: 10.3846 - dense_20_loss: 2.0536\nEpoch 491/500\n448/448 [==============================] - 7s 15ms/step - loss: 43.0262 - dense_19_loss: 10.1865 - dense_20_loss: 2.0201\nEpoch 492/500\n448/448 [==============================] - 7s 15ms/step - loss: 42.0245 - dense_19_loss: 9.3652 - dense_20_loss: 1.9957\nEpoch 493/500\n448/448 [==============================] - 7s 16ms/step - loss: 42.0619 - dense_19_loss: 9.6008 - dense_20_loss: 2.0032\nEpoch 494/500\n448/448 [==============================] - 7s 15ms/step - loss: 41.9923 - dense_19_loss: 9.8088 - dense_20_loss: 1.9070\nEpoch 495/500\n448/448 [==============================] - 7s 15ms/step - loss: 42.2999 - dense_19_loss: 10.1242 - dense_20_loss: 1.9771\nEpoch 496/500\n448/448 [==============================] - 7s 15ms/step - loss: 42.9514 - dense_19_loss: 10.8055 - dense_20_loss: 2.0233\nEpoch 497/500\n448/448 [==============================] - 7s 16ms/step - loss: 45.0976 - dense_19_loss: 12.4735 - dense_20_loss: 2.2532\nEpoch 498/500\n448/448 [==============================] - 7s 15ms/step - loss: 44.0834 - dense_19_loss: 11.6750 - dense_20_loss: 2.1703\nEpoch 499/500\n448/448 [==============================] - 7s 15ms/step - loss: 42.7928 - dense_19_loss: 10.6676 - dense_20_loss: 2.0272\nEpoch 500/500\n448/448 [==============================] - 7s 15ms/step - loss: 43.1295 - dense_19_loss: 10.9899 - dense_20_loss: 2.0975\nmsynergy_mean_squared_error 197.68564731570618\nmsenstivity_mean_squared_error 14.340185207700683\nmsynergy_mean_absolute_error 9.131097033065235\nmsenstivity_mean_absolute_error 2.6717370116584216\nmsynergy_r2_score 0.5564934133251616\nmsenstivity_r2_score 0.909343619363046\n113/113 [==============================] - 2s 8ms/step - loss: 241.8347 - dense_19_loss: 197.6857 - dense_20_loss: 14.3402\n[241.83474731445312, 197.6857147216797, 14.34018325805664]\nmsynergy_pear (array([0.7531707223567362], dtype=object), 0.0)\nmsynergy_spear SpearmanrResult(correlation=0.7169339275646148, pvalue=0.0)\nmsenstivity_pear (array([0.9538331086775622], dtype=object), 0.0)\nmsenstivity_spear SpearmanrResult(correlation=0.954340092175873, pvalue=0.0)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(array([[-19.422928 ],\n        [ -1.4559734],\n        [  2.4356046],\n        ...,\n        [  0.934448 ],\n        [ -3.8760757],\n        [ -2.8579607]], dtype=float32),\n array([[24.626043],\n        [19.307753],\n        [34.254074],\n        ...,\n        [31.495901],\n        [42.120785],\n        [26.851675]], dtype=float32))"},"metadata":{}}]},{"cell_type":"code","source":"\nfrom IPython.display import FileLink\nnp.savetxt('pred_syn0.csv', ap111 ,delimiter=',')\nFileLink(r'pred_syn0.csv')\n\nnp.savetxt('pred_sen0.csv', ap221 ,delimiter=',')\nFileLink(r'pred_sen0.csv')\n\nnp.savetxt('test_syn0.csv', test_synergy ,delimiter=',')\nFileLink(r'test_syn0.csv')\n\nnp.savetxt('test_sen0.csv', test_senstivity ,delimiter=',')\nFileLink(r'test_sen0.csv')","metadata":{"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/test_sen0.csv","text/html":"<a href='test_sen0.csv' target='_blank'>test_sen0.csv</a><br>"},"metadata":{}}]}]}